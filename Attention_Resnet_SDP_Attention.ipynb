{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeSotoG/U-Net-ResNetBlocks/blob/main/Attention_Resnet_SDP_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcH2AvTh97gF"
      },
      "source": [
        "##Descarga datos\n",
        "Los datos se encuentran en el drive, por lo que usara gdown para sacarlos directamente y no tener que hacer la coneccion, ya que estamos descargando un zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GLXIiagwH3",
        "outputId": "3a6c93db-e3df-4017-e478-be2566c396ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt\n",
            "To: /content/input.zip\n",
            "100% 597M/597M [00:05<00:00, 105MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uq8I6BfRhogZ"
      },
      "outputs": [],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H9Y2Q3Bce47L"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_jqAPlaMKtpg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hmJoArAEd41C"
      },
      "outputs": [],
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  X[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)\n",
        "mas=\"/masks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  Y[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)/255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Borrar directorio /input en caso de error"
      ],
      "metadata": {
        "id": "3NcX5DC3VvAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/input"
      ],
      "metadata": {
        "id": "-QywVJIaZjHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6IAoTKTuO5B"
      },
      "source": [
        "##Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ub5XKwbdAQ32"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CambWxdEwn"
      },
      "source": [
        "##Resnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "FlAI4R5nZPxD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DiceMetric(y_true, y_pred):\n",
        "  smooth=1e-6 \n",
        "  gama=2\n",
        "  y_true, y_pred = tf.cast(\n",
        "      y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
        "  nominator = 2 * \\\n",
        "      tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
        "  denominator = tf.reduce_sum(\n",
        "      y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
        "  result = tf.divide(nominator, denominator)\n",
        "  return result\n",
        "def DiceLoss(y_true, y_pred):\n",
        "      result= 1- DiceMetric(y_true, y_pred)\n",
        "      return result"
      ],
      "metadata": {
        "id": "Urgfb0zMWlz-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (2, 2), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (2, 2), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (2, 2), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,s"
      ],
      "metadata": {
        "id": "MDQHBprOXZXG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Intentos fallidos de attention\n",
        "\n",
        "El problema es que hacerles muchas capas de neuronas, o hacerles muchos calculos de atencion, hace que se demore mas y se pierde precicion."
      ],
      "metadata": {
        "id": "MV5Ug-N_usjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attentionblock(Q,V):\n",
        "  v=tf.keras.layers.Dense(Q.shape[-1])(Q)\n",
        "  k=tf.keras.layers.Dense(Q.shape[-1])(Q)\n",
        "  q=tf.keras.layers.Dense(Q.shape[-1])(Q) \n",
        "  Q=tf.keras.layers.Attention()([q,v,k])\n",
        "  v=tf.keras.layers.Dense(V.shape[-1])(V)\n",
        "  k=tf.keras.layers.Dense(V.shape[-1])(V)\n",
        "  q=tf.keras.layers.Dense(V.shape[-1])(V) \n",
        "  V=tf.keras.layers.Attention()([q,v,k])\n",
        "  V = tf.keras.layers.Conv2DTranspose(Q.shape[-1], (2, 2), strides=(2, 2), padding='same')(V)\n",
        "  v=tf.keras.layers.Dense(Q.shape[-1])(V)\n",
        "  k=tf.keras.layers.Dense(Q.shape[-1])(Q)\n",
        "  q=tf.keras.layers.Dense(Q.shape[-1])(Q) \n",
        "  out=tf.keras.layers.Attention()([q,v,k])\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "JIcOYohZOezi"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attentionblock(Q,V): \n",
        "  Q=tf.keras.layers.Attention()([Q,Q,Q])\n",
        "  V=tf.keras.layers.Attention()([V,V,V])\n",
        "  V = tf.keras.layers.Conv2DTranspose(Q.shape[-1], (2, 2), strides=(2, 2), padding='same')(V) \n",
        "  out=tf.keras.layers.Attention()([Q,V,V])\n",
        "  return out"
      ],
      "metadata": {
        "id": "j3G9_fgSdlsd"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bloque de atencion que funciono mejor"
      ],
      "metadata": {
        "id": "wQRG9euUu47T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attentionblock(Q,V): \n",
        "  V = tf.keras.layers.Conv2DTranspose(Q.shape[-1], (2, 2), strides=(2, 2), padding='same')(V) \n",
        "  out=tf.keras.layers.Attention()([Q,V,V])\n",
        "  return out"
      ],
      "metadata": {
        "id": "XW4JysTrfOv9"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJgrBPxjiV-c",
        "outputId": "7f72dc2b-6f91-4a2f-c718-13aa88d8340b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_27 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_26 (Lambda)             (None, 128, 128, 1)  0           ['input_27[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_507 (Conv2D)            (None, 128, 128, 16  80          ['lambda_26[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_498 (Batch  (None, 128, 128, 16  64         ['conv2d_507[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_166 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_498[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_508 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_166[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_509 (Conv2D)            (None, 128, 128, 16  80          ['lambda_26[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_499 (Batch  (None, 128, 128, 16  64         ['conv2d_508[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_500 (Batch  (None, 128, 128, 16  64         ['conv2d_509[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_166 (Add)                  (None, 128, 128, 16  0           ['batch_normalization_499[0][0]',\n",
            "                                )                                 'batch_normalization_500[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_166 (ReLU)               (None, 128, 128, 16  0           ['add_166[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_104 (MaxPooling2  (None, 64, 64, 16)  0           ['re_lu_166[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_510 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_104[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_501 (Batch  (None, 64, 64, 32)  128         ['conv2d_510[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_167 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_501[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_511 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_167[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_512 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_104[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_502 (Batch  (None, 64, 64, 32)  128         ['conv2d_511[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_503 (Batch  (None, 64, 64, 32)  128         ['conv2d_512[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_167 (Add)                  (None, 64, 64, 32)   0           ['batch_normalization_502[0][0]',\n",
            "                                                                  'batch_normalization_503[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_167 (ReLU)               (None, 64, 64, 32)   0           ['add_167[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_105 (MaxPooling2  (None, 32, 32, 32)  0           ['re_lu_167[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_513 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_105[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_504 (Batch  (None, 32, 32, 64)  256         ['conv2d_513[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_168 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_504[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_514 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_168[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_515 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_105[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_505 (Batch  (None, 32, 32, 64)  256         ['conv2d_514[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_506 (Batch  (None, 32, 32, 64)  256         ['conv2d_515[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_168 (Add)                  (None, 32, 32, 64)   0           ['batch_normalization_505[0][0]',\n",
            "                                                                  'batch_normalization_506[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_168 (ReLU)               (None, 32, 32, 64)   0           ['add_168[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_106 (MaxPooling2  (None, 16, 16, 64)  0           ['re_lu_168[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_516 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_106[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_507 (Batch  (None, 16, 16, 128)  512        ['conv2d_516[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_169 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_507[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_517 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_169[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_518 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_106[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_508 (Batch  (None, 16, 16, 128)  512        ['conv2d_517[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_509 (Batch  (None, 16, 16, 128)  512        ['conv2d_518[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_169 (Add)                  (None, 16, 16, 128)  0           ['batch_normalization_508[0][0]',\n",
            "                                                                  'batch_normalization_509[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_169 (ReLU)               (None, 16, 16, 128)  0           ['add_169[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_107 (MaxPooling2  (None, 8, 8, 128)   0           ['re_lu_169[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_519 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_107[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_510 (Batch  (None, 8, 8, 256)   1024        ['conv2d_519[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_170 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_510[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_520 (Conv2D)            (None, 8, 8, 256)    262400      ['dropout_170[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_521 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_107[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_511 (Batch  (None, 8, 8, 256)   1024        ['conv2d_520[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_512 (Batch  (None, 8, 8, 256)   1024        ['conv2d_521[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_170 (Add)                  (None, 8, 8, 256)    0           ['batch_normalization_511[0][0]',\n",
            "                                                                  'batch_normalization_512[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_170 (ReLU)               (None, 8, 8, 256)    0           ['add_170[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_transpose_60 (Conv2DTra  (None, 16, 16, 128)  131200     ['batch_normalization_512[0][0]']\n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " conv2d_transpose_61 (Conv2DTra  (None, 16, 16, 128)  131200     ['re_lu_170[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " attention_70 (Attention)       (None, 16, 16, 128)  0           ['batch_normalization_509[0][0]',\n",
            "                                                                  'conv2d_transpose_60[0][0]',    \n",
            "                                                                  'conv2d_transpose_60[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_61[0][0]',    \n",
            "                                                                  'attention_70[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_522 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_513 (Batch  (None, 16, 16, 128)  512        ['conv2d_522[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_171 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_513[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_523 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_171[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_524 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_514 (Batch  (None, 16, 16, 128)  512        ['conv2d_523[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_515 (Batch  (None, 16, 16, 128)  512        ['conv2d_524[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_171 (Add)                  (None, 16, 16, 128)  0           ['batch_normalization_514[0][0]',\n",
            "                                                                  'batch_normalization_515[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_171 (ReLU)               (None, 16, 16, 128)  0           ['add_171[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_transpose_62 (Conv2DTra  (None, 32, 32, 64)  32832       ['batch_normalization_515[0][0]']\n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " conv2d_transpose_63 (Conv2DTra  (None, 32, 32, 64)  32832       ['re_lu_171[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " attention_71 (Attention)       (None, 32, 32, 64)   0           ['batch_normalization_506[0][0]',\n",
            "                                                                  'conv2d_transpose_62[0][0]',    \n",
            "                                                                  'conv2d_transpose_62[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_63[0][0]',    \n",
            "                                                                  'attention_71[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_525 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_39[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_516 (Batch  (None, 32, 32, 64)  256         ['conv2d_525[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_172 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_516[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_526 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_172[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_527 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_39[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_517 (Batch  (None, 32, 32, 64)  256         ['conv2d_526[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_518 (Batch  (None, 32, 32, 64)  256         ['conv2d_527[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_172 (Add)                  (None, 32, 32, 64)   0           ['batch_normalization_517[0][0]',\n",
            "                                                                  'batch_normalization_518[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_172 (ReLU)               (None, 32, 32, 64)   0           ['add_172[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_transpose_64 (Conv2DTra  (None, 64, 64, 32)  8224        ['batch_normalization_518[0][0]']\n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " conv2d_transpose_65 (Conv2DTra  (None, 64, 64, 32)  8224        ['re_lu_172[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " attention_72 (Attention)       (None, 64, 64, 32)   0           ['batch_normalization_503[0][0]',\n",
            "                                                                  'conv2d_transpose_64[0][0]',    \n",
            "                                                                  'conv2d_transpose_64[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_65[0][0]',    \n",
            "                                                                  'attention_72[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_528 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_519 (Batch  (None, 64, 64, 32)  128         ['conv2d_528[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_173 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_519[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_529 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_173[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_530 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_520 (Batch  (None, 64, 64, 32)  128         ['conv2d_529[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_521 (Batch  (None, 64, 64, 32)  128         ['conv2d_530[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_173 (Add)                  (None, 64, 64, 32)   0           ['batch_normalization_520[0][0]',\n",
            "                                                                  'batch_normalization_521[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_173 (ReLU)               (None, 64, 64, 32)   0           ['add_173[0][0]']                \n",
            "                                                                                                  \n",
            " conv2d_transpose_66 (Conv2DTra  (None, 128, 128, 16  2064       ['batch_normalization_521[0][0]']\n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_transpose_67 (Conv2DTra  (None, 128, 128, 16  2064       ['re_lu_173[0][0]']              \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " attention_73 (Attention)       (None, 128, 128, 16  0           ['batch_normalization_500[0][0]',\n",
            "                                )                                 'conv2d_transpose_66[0][0]',    \n",
            "                                                                  'conv2d_transpose_66[0][0]']    \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_67[0][0]',    \n",
            "                                )                                 'attention_73[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_531 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_41[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_522 (Batch  (None, 128, 128, 16  64         ['conv2d_531[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_174 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_522[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_532 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_174[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_533 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_41[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_523 (Batch  (None, 128, 128, 16  64         ['conv2d_532[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_524 (Batch  (None, 128, 128, 16  64         ['conv2d_533[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_174 (Add)                  (None, 128, 128, 16  0           ['batch_normalization_523[0][0]',\n",
            "                                )                                 'batch_normalization_524[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_174 (ReLU)               (None, 128, 128, 16  0           ['add_174[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_534 (Conv2D)            (None, 128, 128, 1)  17          ['re_lu_174[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,492,369\n",
            "Trainable params: 1,487,953\n",
            "Non-trainable params: 4,416\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "nheads=16\n",
        "\n",
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "#s= inputs\n",
        "#Contraction path\n",
        "c1,z1 = conv_block(s,[16,16])\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "c2,z2 = conv_block(p1,[32,32])\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "#Expansive path \n",
        "m1=attentionblock(z4,z5)\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "m2=attentionblock(z3,z6)\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "m3=attentionblock(z2,z7)\n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "m4=attentionblock(z1,z8) \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "c9,_ = conv_block(u9,[16,16])\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFyLMRezeWO"
      },
      "source": [
        "##Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCB8atJ1zda4",
        "outputId": "93f1a4d0-21f4-4673-fdfc-3bbcc2c2cd1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "99/99 [==============================] - 12s 86ms/step - loss: 0.2117 - accuracy: 0.9600 - DiceMetric: 0.1085 - val_loss: 0.1421 - val_accuracy: 0.9855 - val_DiceMetric: 0.0977\n",
            "Epoch 2/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0697 - accuracy: 0.9871 - DiceMetric: 0.2143 - val_loss: 0.0735 - val_accuracy: 0.9855 - val_DiceMetric: 0.0782\n",
            "Epoch 3/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0411 - accuracy: 0.9878 - DiceMetric: 0.4494 - val_loss: 0.0578 - val_accuracy: 0.9855 - val_DiceMetric: 0.0462\n",
            "Epoch 4/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0283 - accuracy: 0.9882 - DiceMetric: 0.6081 - val_loss: 0.0486 - val_accuracy: 0.9855 - val_DiceMetric: 0.0638\n",
            "Epoch 5/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0216 - accuracy: 0.9884 - DiceMetric: 0.7063 - val_loss: 0.0380 - val_accuracy: 0.9857 - val_DiceMetric: 0.2433\n",
            "Epoch 6/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0176 - accuracy: 0.9886 - DiceMetric: 0.7681 - val_loss: 0.0238 - val_accuracy: 0.9869 - val_DiceMetric: 0.6286\n",
            "Epoch 7/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0150 - accuracy: 0.9888 - DiceMetric: 0.8014 - val_loss: 0.0211 - val_accuracy: 0.9867 - val_DiceMetric: 0.7034\n",
            "Epoch 8/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0137 - accuracy: 0.9888 - DiceMetric: 0.8151 - val_loss: 0.0152 - val_accuracy: 0.9871 - val_DiceMetric: 0.8149\n",
            "Epoch 9/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0124 - accuracy: 0.9889 - DiceMetric: 0.8342 - val_loss: 0.0149 - val_accuracy: 0.9873 - val_DiceMetric: 0.8048\n",
            "Epoch 10/25\n",
            "99/99 [==============================] - 8s 79ms/step - loss: 0.0112 - accuracy: 0.9890 - DiceMetric: 0.8528 - val_loss: 0.0141 - val_accuracy: 0.9872 - val_DiceMetric: 0.8316\n",
            "Epoch 11/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0106 - accuracy: 0.9890 - DiceMetric: 0.8555 - val_loss: 0.0140 - val_accuracy: 0.9868 - val_DiceMetric: 0.8431\n",
            "Epoch 12/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0100 - accuracy: 0.9891 - DiceMetric: 0.8658 - val_loss: 0.0121 - val_accuracy: 0.9874 - val_DiceMetric: 0.8560\n",
            "Epoch 13/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0095 - accuracy: 0.9891 - DiceMetric: 0.8745 - val_loss: 0.0122 - val_accuracy: 0.9872 - val_DiceMetric: 0.8578\n",
            "Epoch 14/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0092 - accuracy: 0.9891 - DiceMetric: 0.8793 - val_loss: 0.0113 - val_accuracy: 0.9875 - val_DiceMetric: 0.8651\n",
            "Epoch 15/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0088 - accuracy: 0.9892 - DiceMetric: 0.8829 - val_loss: 0.0113 - val_accuracy: 0.9878 - val_DiceMetric: 0.8457\n",
            "Epoch 16/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0088 - accuracy: 0.9892 - DiceMetric: 0.8825 - val_loss: 0.0116 - val_accuracy: 0.9875 - val_DiceMetric: 0.8576\n",
            "Epoch 17/25\n",
            "99/99 [==============================] - 8s 80ms/step - loss: 0.0086 - accuracy: 0.9892 - DiceMetric: 0.8866 - val_loss: 0.0116 - val_accuracy: 0.9879 - val_DiceMetric: 0.8441\n",
            "Epoch 18/25\n",
            "99/99 [==============================] - 8s 79ms/step - loss: 0.0084 - accuracy: 0.9892 - DiceMetric: 0.8899 - val_loss: 0.0102 - val_accuracy: 0.9878 - val_DiceMetric: 0.8724\n",
            "Epoch 19/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0079 - accuracy: 0.9893 - DiceMetric: 0.8990 - val_loss: 0.0098 - val_accuracy: 0.9879 - val_DiceMetric: 0.8779\n",
            "Epoch 20/25\n",
            "99/99 [==============================] - 8s 79ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8957 - val_loss: 0.0108 - val_accuracy: 0.9875 - val_DiceMetric: 0.8647\n",
            "Epoch 21/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0077 - accuracy: 0.9893 - DiceMetric: 0.9001 - val_loss: 0.0092 - val_accuracy: 0.9879 - val_DiceMetric: 0.8891\n",
            "Epoch 22/25\n",
            "99/99 [==============================] - 8s 79ms/step - loss: 0.0074 - accuracy: 0.9893 - DiceMetric: 0.9081 - val_loss: 0.0099 - val_accuracy: 0.9877 - val_DiceMetric: 0.8802\n",
            "Epoch 23/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.9018 - val_loss: 0.0100 - val_accuracy: 0.9879 - val_DiceMetric: 0.8721\n",
            "Epoch 24/25\n",
            "99/99 [==============================] - 8s 78ms/step - loss: 0.0073 - accuracy: 0.9893 - DiceMetric: 0.9075 - val_loss: 0.0092 - val_accuracy: 0.9879 - val_DiceMetric: 0.8903\n",
            "Epoch 25/25\n",
            "99/99 [==============================] - 8s 79ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9136 - val_loss: 0.0091 - val_accuracy: 0.9880 - val_DiceMetric: 0.8896\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd01b4686d0>"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "model.fit(X_train,Y_train,batch_size=20,epochs=25,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "b0yz227RzmyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f03beea-92b9-4209-a4a4-9bd55060843f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 0: loss of 0.007760524749755859; accuracy of 98.95282983779907% DiceMetric of 89.5677924156189%\n",
            "['loss', 'accuracy', 'DiceMetric']\n"
          ]
        }
      ],
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.metrics_names)"
      ],
      "metadata": {
        "id": "oud92q0zgO7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFP-3gQ9BO8f"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "5P0-RG0wjzA6"
      },
      "outputs": [],
      "source": [
        "Ypred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "J1ra2EoVycvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "38904fb5-0e9f-4fb0-e47b-efaaefb1c4d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(np.reshape(Ypred[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "plt.imshow(np.reshape(Y_test[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "ikB_eI_ZmVtq"
      },
      "outputs": [],
      "source": [
        "def dice(true_mask, pred_mask):\n",
        "    \"\"\"\n",
        "        Computes the Dice coefficient.\n",
        "        Args:\n",
        "            true_mask : Array of arbitrary shape.\n",
        "            pred_mask : Array with the same shape than true_mask.  \n",
        "        \n",
        "        Returns:\n",
        "            A scalar representing the Dice coefficient between the two segmentations. \n",
        "        \n",
        "    \"\"\"\n",
        "    non_seg_score=1.0\n",
        "    if type(pred_mask) != np.ndarray:\n",
        "      t = torch.Tensor([0.5])\n",
        "      pred_mask=(pred_mask > t)\n",
        "    else:\n",
        "      pred_mask[pred_mask>=0.5]=1\n",
        "      pred_mask[pred_mask<0.5]=0\n",
        "\n",
        "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
        "    im_sum = true_mask.sum() + pred_mask.sum()\n",
        "    if im_sum == 0:\n",
        "        return non_seg_score\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(true_mask, pred_mask)\n",
        "    return 2. * intersection.sum() / im_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBVFzRQmXI-",
        "outputId": "ff7ece72-2c55-4971-bd61-4eaef21916c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9644930416996391"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "diceaux=dice(Y_test[160],Ypred[160])\n",
        "diceaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2Sj8PPuJBv"
      },
      "source": [
        "## Model Fit Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Fh7g3mmhuNPV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "kj7ESdJAuOaW"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25 epochs"
      ],
      "metadata": {
        "id": "erWVgdGfhXKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7gaF3LTvOU-",
        "outputId": "ad5e1ee5-aa54-4157-f0a4-dca106c3c200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.015375291928648949; accuracy of 98.92076849937439% DiceMetric of 85.06117463111877%\n",
            "Score for fold 2: loss of 0.0173810888081789; accuracy of 98.82116317749023% DiceMetric of 86.53574585914612%\n",
            "Score for fold 3: loss of 0.029940396547317505; accuracy of 98.86179566383362% DiceMetric of 80.37702441215515%\n",
            "Score for fold 4: loss of 0.021816927939653397; accuracy of 98.8555908203125% DiceMetric of 80.62856793403625%\n",
            "Score for fold 5: loss of 0.02643907628953457; accuracy of 98.90596866607666% DiceMetric of 81.0717225074768%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50 epochs"
      ],
      "metadata": {
        "id": "uOWim3VxhZsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7om-UlhbP9",
        "outputId": "8107afdc-6668-4c3b-8463-e458f5237e9e"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.026712991297245026; accuracy of 98.92278909683228% DiceMetric of 85.75470447540283%\n",
            "Score for fold 2: loss of 0.03414676710963249; accuracy of 98.79527688026428% DiceMetric of 84.62190628051758%\n",
            "Score for fold 3: loss of 0.030036412179470062; accuracy of 98.86870980262756% DiceMetric of 83.15788507461548%\n",
            "Score for fold 4: loss of 0.030506402254104614; accuracy of 98.86631369590759% DiceMetric of 85.55557131767273%\n",
            "Score for fold 5: loss of 0.030434055253863335; accuracy of 98.91023635864258% DiceMetric of 82.77513980865479%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attention Resnet SDP Attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO87lDAD13vi6STlkEIVs6N",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}