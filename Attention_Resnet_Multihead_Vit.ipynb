{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeSotoG/U-Net-ResNetBlocks/blob/main/Attention_Resnet_Multihead_Vit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcH2AvTh97gF"
      },
      "source": [
        "##Descarga datos\n",
        "Los datos se encuentran en el drive, por lo que usara gdown para sacarlos directamente y no tener que hacer la coneccion, ya que estamos descargando un zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GLXIiagwH3",
        "outputId": "c10d21f8-a31d-4eb6-a0bc-c214233f944f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt\n",
            "To: /content/input.zip\n",
            "100% 597M/597M [00:05<00:00, 112MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ol26WSlHEN2",
        "outputId": "705a0d37-ecc4-41f1-d765-8a96ad431e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nCyz7YYjTFCQRaYeN5pIM7-bz2neRl4O\n",
            "To: /content/input.zip\n",
            "100% 354M/354M [00:06<00:00, 57.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1nCyz7YYjTFCQRaYeN5pIM7-bz2neRl4O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8I6BfRhogZ"
      },
      "outputs": [],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyKSUcWNcQWb"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Y2Q3Bce47L"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jqAPlaMKtpg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmJoArAEd41C"
      },
      "outputs": [],
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  X[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)\n",
        "mas=\"/masks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  Y[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)/255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qveCPGJRyVC0",
        "outputId": "7401c684-fa5f-4456-e048-9a34498e285c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "002\n",
            "003\n",
            "004\n",
            "005\n",
            "006\n",
            "007\n",
            "008\n",
            "009\n",
            "010\n",
            "10_85902_1\n",
            "10_85902_3\n",
            "14_85914_0\n",
            "27_86410_0\n",
            "29_86490_1\n",
            "29_86491_1\n",
            "36_86526_0\n",
            "40_86625_0\n",
            "4_85506_1\n",
            "7_85703_0\n"
          ]
        }
      ],
      "source": [
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "Siz=0\n",
        "id=0\n",
        "ri=0\n",
        "metadat=[]\n",
        "c=\"001\"\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  if x.find(\"radiopaedia\") == -1:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\")]\n",
        "  else:\n",
        "    x=x[x.find(\"_\")+1:]\n",
        "    x=x[:x.find(\"_\",9)]\n",
        "  if c!=x:\n",
        "    print(x)\n",
        "    metadat.append([id,Siz,ri])\n",
        "    ri=i\n",
        "    c=x\n",
        "    id+=1\n",
        "    Siz=0\n",
        "  Siz+=1\n",
        "metadat.append([id,Siz,ri])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvHo47wUxDuU"
      },
      "source": [
        "##New Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKr1uaLjxHQd",
        "outputId": "acc81cd7-3f87-4069-cae8-53d99d70c394"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  2  4  5  7  8  9 11 12 13 14 15 17 18 19] [ 3  6 10 16]\n",
            "[ 0  1  3  5  6  7  8  9 10 11 12 13 15 16 18 19] [ 2  4 14 17]\n",
            "[ 2  3  4  5  6  8  9 10 11 12 14 15 16 17 18 19] [ 0  1  7 13]\n",
            "[ 0  1  2  3  4  5  6  7  8 10 11 12 13 14 16 17] [ 9 15 18 19]\n",
            "[ 0  1  2  3  4  6  7  9 10 13 14 15 16 17 18 19] [ 5  8 11 12]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
        "IDX=[x for x in range(10)]\n",
        "IDY=[x for x in range(10)]\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  print(train_index, val_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eNTqOVrBtji"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "IDX=[x for x in range(len(metadat))]\n",
        "train_index, val_index,_,_=train_test_split(\n",
        "   IDX,metadat, test_size=0.05, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdmpbD9H47d0"
      },
      "outputs": [],
      "source": [
        "for i,x in enumerate(train_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_train=X[ri:ri+size]\n",
        "    Y_train=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_train=np.concatenate((X_train,X[ri:ri+size]),axis=0)\n",
        "    Y_train=np.concatenate((Y_train,Y[ri:ri+size]),axis=0)\n",
        "for i,x in enumerate(val_index):\n",
        "  size=metadat[x][1]\n",
        "  ri=metadat[x][2]\n",
        "  if i == 0:\n",
        "    X_test=X[ri:ri+size]\n",
        "    Y_test=Y[ri:ri+size]\n",
        "  else:\n",
        "    X_test=np.concatenate((X_test,X[ri:ri+size]),axis=0)\n",
        "    Y_test=np.concatenate((Y_test,Y[ri:ri+size]),axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def patch(X,psize):\n",
        "  try:\n",
        "    npatch=int(X.shape[1]/psize)\n",
        "  except:\n",
        "    print(\"kek\")\n",
        "  for i in range(npatch):\n",
        "    st=i*psize\n",
        "    end=st+psize\n",
        "    for j in range(npatch):\n",
        "      st2=j*psize\n",
        "      end2=st2+psize\n",
        "      patch= X[:,st:end,st2:end2,:]\n",
        "      print(patch.shape)\n",
        "  return"
      ],
      "metadata": {
        "id": "Bms3gaKAvmIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch(X_train,16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUC60_aEkcgY",
        "outputId": "e80812f2-2adb-4f5f-dc5e-530dfcdd4e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n",
            "(3410, 16, 16, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NcX5DC3VvAR"
      },
      "source": [
        "##Borrar directorio /input en caso de error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QywVJIaZjHC"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6IAoTKTuO5B"
      },
      "source": [
        "##Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub5XKwbdAQ32"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CambWxdEwn"
      },
      "source": [
        "##Patch wise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlAI4R5nZPxD"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Urgfb0zMWlz-"
      },
      "outputs": [],
      "source": [
        "def DiceMetric(y_true, y_pred):\n",
        "  smooth=1e-6 \n",
        "  gama=2\n",
        "  y_true, y_pred = tf.cast(\n",
        "      y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
        "  nominator = 2 * \\\n",
        "      tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
        "  denominator = tf.reduce_sum(\n",
        "      y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
        "  result = tf.divide(nominator, denominator)\n",
        "  return result\n",
        "def DiceLoss(y_true, y_pred):\n",
        "      result= 1- DiceMetric(y_true, y_pred)\n",
        "      return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDQHBprOXZXG"
      },
      "outputs": [],
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (3, 3), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (3, 3), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (1, 1), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,c"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vit(X,n_heads=16,psize=16):\n",
        "  npatch=int(X.shape[1]/psize)\n",
        "  for i in range(npatch):\n",
        "    st=i*psize\n",
        "    end=st+psize\n",
        "    for j in range(npatch):\n",
        "      st2=j*psize\n",
        "      end2=st2+psize\n",
        "      patch= X[:,st:end,st2:end2,:]\n",
        "      pm=tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=3, attention_axes=(1, 2))(patch,patch)\n",
        "      if j==0:\n",
        "        A=pm\n",
        "      else:\n",
        "        A=tf.concat([A,pm],2)\n",
        "    if i==0:\n",
        "      V=A\n",
        "    else: \n",
        "      V=tf.concat([V,A],1)\n",
        "  return V\n",
        "\n"
      ],
      "metadata": {
        "id": "cfMyCT7a5iB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNWwE-DOcxpY"
      },
      "source": [
        "### Max pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJgrBPxjiV-c",
        "outputId": "4350e5fc-4eb3-4ab4-df24-23bebf5e6c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 128, 128, 1)  0           ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_282 (Conv2D)            (None, 128, 128, 16  80          ['lambda_11[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_273 (Batch  (None, 128, 128, 16  64         ['conv2d_282[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 128, 128, 16  0           ['batch_normalization_273[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_283 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_91[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_274 (Batch  (None, 128, 128, 16  64         ['conv2d_283[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_284 (Conv2D)            (None, 128, 128, 16  80          ['lambda_11[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 128, 128, 16  0           ['batch_normalization_274[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_275 (Batch  (None, 128, 128, 16  64         ['conv2d_284[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_91 (Add)                   (None, 128, 128, 16  0           ['dropout_92[0][0]',             \n",
            "                                )                                 'batch_normalization_275[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_91 (ReLU)                (None, 128, 128, 16  0           ['add_91[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_57 (MaxPooling2D  (None, 64, 64, 16)  0           ['re_lu_91[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_285 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_57[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_276 (Batch  (None, 64, 64, 32)  128         ['conv2d_285[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 64, 64, 32)   0           ['batch_normalization_276[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_286 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_277 (Batch  (None, 64, 64, 32)  128         ['conv2d_286[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_287 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_57[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 64, 64, 32)   0           ['batch_normalization_277[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_278 (Batch  (None, 64, 64, 32)  128         ['conv2d_287[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_92 (Add)                   (None, 64, 64, 32)   0           ['dropout_94[0][0]',             \n",
            "                                                                  'batch_normalization_278[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_92 (ReLU)                (None, 64, 64, 32)   0           ['add_92[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_58 (MaxPooling2D  (None, 32, 32, 32)  0           ['re_lu_92[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_288 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_58[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_279 (Batch  (None, 32, 32, 64)  256         ['conv2d_288[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)           (None, 32, 32, 64)   0           ['batch_normalization_279[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_289 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_95[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_280 (Batch  (None, 32, 32, 64)  256         ['conv2d_289[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_290 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_58[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_96 (Dropout)           (None, 32, 32, 64)   0           ['batch_normalization_280[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_281 (Batch  (None, 32, 32, 64)  256         ['conv2d_290[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_93 (Add)                   (None, 32, 32, 64)   0           ['dropout_96[0][0]',             \n",
            "                                                                  'batch_normalization_281[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_93 (ReLU)                (None, 32, 32, 64)   0           ['add_93[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_59 (MaxPooling2D  (None, 16, 16, 64)  0           ['re_lu_93[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_291 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_59[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_282 (Batch  (None, 16, 16, 128)  512        ['conv2d_291[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_97 (Dropout)           (None, 16, 16, 128)  0           ['batch_normalization_282[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_292 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_97[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_283 (Batch  (None, 16, 16, 128)  512        ['conv2d_292[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_293 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_59[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_98 (Dropout)           (None, 16, 16, 128)  0           ['batch_normalization_283[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_284 (Batch  (None, 16, 16, 128)  512        ['conv2d_293[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_94 (Add)                   (None, 16, 16, 128)  0           ['dropout_98[0][0]',             \n",
            "                                                                  'batch_normalization_284[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_94 (ReLU)                (None, 16, 16, 128)  0           ['add_94[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_60 (MaxPooling2D  (None, 8, 8, 128)   0           ['re_lu_94[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_294 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_60[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_285 (Batch  (None, 8, 8, 256)   1024        ['conv2d_294[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_99 (Dropout)           (None, 8, 8, 256)    0           ['batch_normalization_285[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_295 (Conv2D)            (None, 8, 8, 256)    262400      ['dropout_99[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_286 (Batch  (None, 8, 8, 256)   1024        ['conv2d_295[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_296 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_60[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_100 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_286[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_287 (Batch  (None, 8, 8, 256)   1024        ['conv2d_296[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_95 (Add)                   (None, 8, 8, 256)    0           ['dropout_100[0][0]',            \n",
            "                                                                  'batch_normalization_287[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_95 (ReLU)                (None, 8, 8, 256)    0           ['add_95[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_61 (MaxPooling2D  (None, 2, 2, 256)   0           ['batch_normalization_287[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_36 (Conv2DTra  (None, 16, 16, 128)  131200     ['re_lu_95[0][0]']               \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (Multi  (None, 16, 16, 128)  74144      ['batch_normalization_284[0][0]',\n",
            " HeadAttention)                                                   'max_pooling2d_61[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_36[0][0]',    \n",
            "                                                                  'multi_head_attention_37[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_297 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_36[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_288 (Batch  (None, 16, 16, 128)  512        ['conv2d_297[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_101 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_288[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_298 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_101[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_289 (Batch  (None, 16, 16, 128)  512        ['conv2d_298[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_299 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_36[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_289[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_290 (Batch  (None, 16, 16, 128)  512        ['conv2d_299[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_96 (Add)                   (None, 16, 16, 128)  0           ['dropout_102[0][0]',            \n",
            "                                                                  'batch_normalization_290[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_96 (ReLU)                (None, 16, 16, 128)  0           ['add_96[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_62 (MaxPooling2D  (None, 4, 4, 128)   0           ['batch_normalization_290[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_37 (Conv2DTra  (None, 32, 32, 64)  32832       ['re_lu_96[0][0]']               \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_38 (Multi  (None, 32, 32, 64)  37216       ['batch_normalization_281[0][0]',\n",
            " HeadAttention)                                                   'max_pooling2d_62[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_37[0][0]',    \n",
            "                                                                  'multi_head_attention_38[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_300 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_37[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_291 (Batch  (None, 32, 32, 64)  256         ['conv2d_300[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_291[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_301 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_103[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_292 (Batch  (None, 32, 32, 64)  256         ['conv2d_301[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_302 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_37[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_292[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_293 (Batch  (None, 32, 32, 64)  256         ['conv2d_302[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_97 (Add)                   (None, 32, 32, 64)   0           ['dropout_104[0][0]',            \n",
            "                                                                  'batch_normalization_293[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_97 (ReLU)                (None, 32, 32, 64)   0           ['add_97[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_63 (MaxPooling2D  (None, 8, 8, 64)    0           ['batch_normalization_293[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_38 (Conv2DTra  (None, 64, 64, 32)  8224        ['re_lu_97[0][0]']               \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 64, 64, 32)  18752       ['batch_normalization_278[0][0]',\n",
            " HeadAttention)                                                   'max_pooling2d_63[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_38[0][0]',    \n",
            "                                                                  'multi_head_attention_39[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_303 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_294 (Batch  (None, 64, 64, 32)  128         ['conv2d_303[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_294[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_304 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_105[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_295 (Batch  (None, 64, 64, 32)  128         ['conv2d_304[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_305 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_106 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_295[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_296 (Batch  (None, 64, 64, 32)  128         ['conv2d_305[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_98 (Add)                   (None, 64, 64, 32)   0           ['dropout_106[0][0]',            \n",
            "                                                                  'batch_normalization_296[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_98 (ReLU)                (None, 64, 64, 32)   0           ['add_98[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_64 (MaxPooling2D  (None, 16, 16, 32)  0           ['batch_normalization_296[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_39 (Conv2DTra  (None, 128, 128, 16  2064       ['re_lu_98[0][0]']               \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (Multi  (None, 128, 128, 16  9520       ['batch_normalization_275[0][0]',\n",
            " HeadAttention)                 )                                 'max_pooling2d_64[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_39[0][0]',    \n",
            "                                )                                 'multi_head_attention_40[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_306 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_39[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_297 (Batch  (None, 128, 128, 16  64         ['conv2d_306[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_297[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_307 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_107[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_298 (Batch  (None, 128, 128, 16  64         ['conv2d_307[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_308 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_39[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_108 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_298[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_299 (Batch  (None, 128, 128, 16  64         ['conv2d_308[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_99 (Add)                   (None, 128, 128, 16  0           ['dropout_108[0][0]',            \n",
            "                                )                                 'batch_normalization_299[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_99 (ReLU)                (None, 128, 128, 16  0           ['add_99[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_309 (Conv2D)            (None, 128, 128, 1)  17          ['re_lu_99[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,457,681\n",
            "Trainable params: 1,453,265\n",
            "Non-trainable params: 4,416\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "nheads=32\n",
        "\n",
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "#s= inputs\n",
        "#Contraction path\n",
        "c1,z1 = conv_block(s,[16,16])\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "c2,z2 = conv_block(p1,[32,32])\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "#Expansive path \n",
        "z5=tf.keras.layers.MaxPooling2D((4, 4))(z5)\n",
        "m1= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z4,z5)\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "z6=tf.keras.layers.MaxPooling2D((4, 4))(z6)\n",
        "m2= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z3,z6)\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "z7=tf.keras.layers.MaxPooling2D((4, 4))(z7)\n",
        "m3= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z2,z7) \n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "z8=tf.keras.layers.MaxPooling2D((4, 4))(z8)\n",
        "m4= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z1,z8) \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "c9,_ = conv_block(u9,[16,16])\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCsYVONdWxZ"
      },
      "source": [
        "### Average pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q_AsdR1dWxo",
        "outputId": "2d3eae77-fc7c-46be-a604-4a20db39ebf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 128, 128, 1)  0           ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_310 (Conv2D)            (None, 128, 128, 16  80          ['lambda_12[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_300 (Batch  (None, 128, 128, 16  64         ['conv2d_310[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_109 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_300[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_311 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_109[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_301 (Batch  (None, 128, 128, 16  64         ['conv2d_311[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_312 (Conv2D)            (None, 128, 128, 16  80          ['lambda_12[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_110 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_301[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_302 (Batch  (None, 128, 128, 16  64         ['conv2d_312[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_100 (Add)                  (None, 128, 128, 16  0           ['dropout_110[0][0]',            \n",
            "                                )                                 'batch_normalization_302[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_100 (ReLU)               (None, 128, 128, 16  0           ['add_100[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_65 (MaxPooling2D  (None, 64, 64, 16)  0           ['re_lu_100[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_313 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_65[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_303 (Batch  (None, 64, 64, 32)  128         ['conv2d_313[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_303[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_314 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_111[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_304 (Batch  (None, 64, 64, 32)  128         ['conv2d_314[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_315 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_65[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_112 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_304[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_305 (Batch  (None, 64, 64, 32)  128         ['conv2d_315[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_101 (Add)                  (None, 64, 64, 32)   0           ['dropout_112[0][0]',            \n",
            "                                                                  'batch_normalization_305[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_101 (ReLU)               (None, 64, 64, 32)   0           ['add_101[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_66 (MaxPooling2D  (None, 32, 32, 32)  0           ['re_lu_101[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_316 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_306 (Batch  (None, 32, 32, 64)  256         ['conv2d_316[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_306[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_317 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_113[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_307 (Batch  (None, 32, 32, 64)  256         ['conv2d_317[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_318 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_66[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_114 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_307[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_308 (Batch  (None, 32, 32, 64)  256         ['conv2d_318[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_102 (Add)                  (None, 32, 32, 64)   0           ['dropout_114[0][0]',            \n",
            "                                                                  'batch_normalization_308[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_102 (ReLU)               (None, 32, 32, 64)   0           ['add_102[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_67 (MaxPooling2D  (None, 16, 16, 64)  0           ['re_lu_102[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_319 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_67[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_309 (Batch  (None, 16, 16, 128)  512        ['conv2d_319[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_309[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_320 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_115[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_310 (Batch  (None, 16, 16, 128)  512        ['conv2d_320[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_321 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_67[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_310[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_311 (Batch  (None, 16, 16, 128)  512        ['conv2d_321[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_103 (Add)                  (None, 16, 16, 128)  0           ['dropout_116[0][0]',            \n",
            "                                                                  'batch_normalization_311[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_103 (ReLU)               (None, 16, 16, 128)  0           ['add_103[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_68 (MaxPooling2D  (None, 8, 8, 128)   0           ['re_lu_103[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_322 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_68[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_312 (Batch  (None, 8, 8, 256)   1024        ['conv2d_322[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_117 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_312[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_323 (Conv2D)            (None, 8, 8, 256)    262400      ['dropout_117[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_313 (Batch  (None, 8, 8, 256)   1024        ['conv2d_323[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_324 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_68[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_118 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_313[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_314 (Batch  (None, 8, 8, 256)   1024        ['conv2d_324[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_104 (Add)                  (None, 8, 8, 256)    0           ['dropout_118[0][0]',            \n",
            "                                                                  'batch_normalization_314[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_104 (ReLU)               (None, 8, 8, 256)    0           ['add_104[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_20 (AverageP  (None, 3, 3, 256)   0           ['batch_normalization_314[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_40 (Conv2DTra  (None, 16, 16, 128)  131200     ['re_lu_104[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (Multi  (None, 16, 16, 128)  37136      ['batch_normalization_311[0][0]',\n",
            " HeadAttention)                                                   'average_pooling2d_20[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_40[0][0]',    \n",
            "                                                                  'multi_head_attention_41[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_325 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_315 (Batch  (None, 16, 16, 128)  512        ['conv2d_325[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_119 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_315[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_326 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_119[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_316 (Batch  (None, 16, 16, 128)  512        ['conv2d_326[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_327 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_120 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_316[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_317 (Batch  (None, 16, 16, 128)  512        ['conv2d_327[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_105 (Add)                  (None, 16, 16, 128)  0           ['dropout_120[0][0]',            \n",
            "                                                                  'batch_normalization_317[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_105 (ReLU)               (None, 16, 16, 128)  0           ['add_105[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_21 (AverageP  (None, 5, 5, 128)   0           ['batch_normalization_317[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_41 (Conv2DTra  (None, 32, 32, 64)  32832       ['re_lu_105[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (Multi  (None, 32, 32, 64)  18640       ['batch_normalization_308[0][0]',\n",
            " HeadAttention)                                                   'average_pooling2d_21[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_41[0][0]',    \n",
            "                                                                  'multi_head_attention_42[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_328 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_318 (Batch  (None, 32, 32, 64)  256         ['conv2d_328[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_121 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_318[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_329 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_121[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_319 (Batch  (None, 32, 32, 64)  256         ['conv2d_329[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_330 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_122 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_319[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_320 (Batch  (None, 32, 32, 64)  256         ['conv2d_330[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_106 (Add)                  (None, 32, 32, 64)   0           ['dropout_122[0][0]',            \n",
            "                                                                  'batch_normalization_320[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_106 (ReLU)               (None, 32, 32, 64)   0           ['add_106[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_22 (AverageP  (None, 11, 11, 64)  0           ['batch_normalization_320[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_42 (Conv2DTra  (None, 64, 64, 32)  8224        ['re_lu_106[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (Multi  (None, 64, 64, 32)  9392        ['batch_normalization_305[0][0]',\n",
            " HeadAttention)                                                   'average_pooling2d_22[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_42[0][0]',    \n",
            "                                                                  'multi_head_attention_43[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_331 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_42[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_321 (Batch  (None, 64, 64, 32)  128         ['conv2d_331[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_123 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_321[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_332 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_123[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_322 (Batch  (None, 64, 64, 32)  128         ['conv2d_332[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_333 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_42[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_124 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_322[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_323 (Batch  (None, 64, 64, 32)  128         ['conv2d_333[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_107 (Add)                  (None, 64, 64, 32)   0           ['dropout_124[0][0]',            \n",
            "                                                                  'batch_normalization_323[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_107 (ReLU)               (None, 64, 64, 32)   0           ['add_107[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_23 (AverageP  (None, 13, 13, 32)  0           ['batch_normalization_323[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_43 (Conv2DTra  (None, 128, 128, 16  2064       ['re_lu_107[0][0]']              \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_44 (Multi  (None, 128, 128, 16  4768       ['batch_normalization_302[0][0]',\n",
            " HeadAttention)                 )                                 'average_pooling2d_23[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_43[0][0]',    \n",
            "                                )                                 'multi_head_attention_44[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_334 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_43[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_324 (Batch  (None, 128, 128, 16  64         ['conv2d_334[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_125 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_324[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_335 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_125[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_325 (Batch  (None, 128, 128, 16  64         ['conv2d_335[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_336 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_43[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_126 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_325[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_326 (Batch  (None, 128, 128, 16  64         ['conv2d_336[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_108 (Add)                  (None, 128, 128, 16  0           ['dropout_126[0][0]',            \n",
            "                                )                                 'batch_normalization_326[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_108 (ReLU)               (None, 128, 128, 16  0           ['add_108[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_337 (Conv2D)            (None, 128, 128, 1)  17          ['re_lu_108[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,387,985\n",
            "Trainable params: 1,383,569\n",
            "Non-trainable params: 4,416\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "nheads=16\n",
        "\n",
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "#s= inputs\n",
        "#Contraction path\n",
        "c1,z1 = conv_block(s,[16,16])\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "c2,z2 = conv_block(p1,[32,32])\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "#Expansive path \n",
        "z5=tf.keras.layers.AveragePooling2D(strides=(3,3))(z5)\n",
        "m1= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z4,z5)\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "z6=tf.keras.layers.AveragePooling2D(strides=(3,3))(z6)\n",
        "m2= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z3,z6)\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "z7=tf.keras.layers.AveragePooling2D(strides=(3,3))(z7)\n",
        "m3= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z2,z7) \n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "z8=tf.keras.layers.AveragePooling2D(strides=(5,5))(z8)\n",
        "m4= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z1,z8) \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "c9,_ = conv_block(u9,[16,16])\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJJTOmsMgSTa"
      },
      "source": [
        "### Vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKKfBKZCgSTc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  nheads=8\n",
        "  sizep=8\n",
        "  gr=8\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16],group=1)\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32],group=gr)\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2,group=gr)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "  \n",
        "  c4,z4 = conv_block(p3,[128,128],0.2,group=gr)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "  \n",
        "  c5,z5 = conv_block(p4,[256,256],0.3,group=gr)\n",
        "\n",
        "  #Expansive path \n",
        "  m1= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z4,z4)\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "  c6,z6 = conv_block(u6,[128,128],0.2,group=gr)\n",
        "\n",
        "  m2= vit(z3,nheads,sizep) \n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "  c7,z7 = conv_block(u7,[64,64],0.2,group=gr)\n",
        "\n",
        "  m3= vit(z2,nheads,sizep) \n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "  c8,z8 = conv_block(u8,[32,32],group=gr)\n",
        "\n",
        "  m4= vit(z1,nheads,sizep) \n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16],group=gr)\n",
        "  \n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "  #model.summary()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## extra"
      ],
      "metadata": {
        "id": "9YLsg2le7OQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  IMG_WIDTH = 128\n",
        "  IMG_HEIGHT = 128\n",
        "  IMG_CHANNELS = 1\n",
        "  nheads=16\n",
        "\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16])\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32])\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "  \n",
        "  c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "  \n",
        "  c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "  #Expansive path \n",
        "  z5=tf.keras.layers.Conv2D(128, (2,2), padding='same')(c4)\n",
        "  m1= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z5,z5)\n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "  c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "  z6=tf.keras.layers.Conv2D(64, (4,4), padding='same')(c3)\n",
        "  m2= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z6,z6)\n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "  c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "  z7=tf.keras.layers.Conv2D(32, (6,6), padding='same')(c2)\n",
        "  m3= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z7,z7) \n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "  c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "  z8=tf.keras.layers.Conv2D(16, (8,8), padding='same')(c1)\n",
        "  m4= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z8,z8) \n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "  c9,_ = conv_block(u9,[16,16])\n",
        "  \n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "g-wRdkQrK38r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def create_model():\n",
        "  IMG_WIDTH = 128\n",
        "  IMG_HEIGHT = 128\n",
        "  IMG_CHANNELS = 1\n",
        "  nheads=16\n",
        "\n",
        "  #Build the model\n",
        "  inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "  s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "  #s= inputs\n",
        "  #Contraction path\n",
        "  c1,z1 = conv_block(s,[16,16])\n",
        "  p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "  c2,z2 = conv_block(p1,[32,32])\n",
        "  p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "  c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "  p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        "  \n",
        "  c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "  p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "  \n",
        "  c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "  #Expansive path \n",
        "  \n",
        "  u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "  u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "  m1 = tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(u6,u6)\n",
        "  c6,z6 = conv_block(m1,[128,128],0.2)\n",
        "\n",
        " \n",
        "  u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "  u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "  m2 = tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(u7,u7)\n",
        "  c7,z7 = conv_block(m2,[64,64],0.2)\n",
        "\n",
        "  \n",
        "  u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "  u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "  m3 = tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(u8,u8) \n",
        "  c8,z8 = conv_block(m3,[32,32])\n",
        "\n",
        "   \n",
        "  u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "  u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "  u9 = tf.keras.layers.Conv2D(16, (8,8),strides=(8,8), padding='same')(u9)\n",
        "  m4 = tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(u9,u9)\n",
        "  c9,_ = conv_block(m4[16,16])\n",
        "  \n",
        "  outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        "  \n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "  model.summary()\n",
        "  return model"
      ],
      "metadata": {
        "id": "ATOwgPS5HqdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFyLMRezeWO"
      },
      "source": [
        "##Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCB8atJ1zda4",
        "outputId": "2b20053f-8c99-419a-809c-6678afd18b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "198/198 [==============================] - 65s 303ms/step - loss: 0.2668 - accuracy: 0.9514 - DiceMetric: 0.0615 - val_loss: 0.1316 - val_accuracy: 0.9855 - val_DiceMetric: 0.0901\n",
            "Epoch 2/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0752 - accuracy: 0.9868 - DiceMetric: 0.0736 - val_loss: 0.0709 - val_accuracy: 0.9855 - val_DiceMetric: 0.0673\n",
            "Epoch 3/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0477 - accuracy: 0.9870 - DiceMetric: 0.1435 - val_loss: 0.0613 - val_accuracy: 0.9855 - val_DiceMetric: 0.0522\n",
            "Epoch 4/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0355 - accuracy: 0.9872 - DiceMetric: 0.3380 - val_loss: 0.0531 - val_accuracy: 0.9855 - val_DiceMetric: 0.0471\n",
            "Epoch 5/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0276 - accuracy: 0.9873 - DiceMetric: 0.4814 - val_loss: 0.0283 - val_accuracy: 0.9852 - val_DiceMetric: 0.4993\n",
            "Epoch 6/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0183 - accuracy: 0.9879 - DiceMetric: 0.6727 - val_loss: 0.0196 - val_accuracy: 0.9871 - val_DiceMetric: 0.6225\n",
            "Epoch 7/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0153 - accuracy: 0.9884 - DiceMetric: 0.7374 - val_loss: 0.0294 - val_accuracy: 0.9817 - val_DiceMetric: 0.6248\n",
            "Epoch 8/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0190 - accuracy: 0.9878 - DiceMetric: 0.6116 - val_loss: 0.0181 - val_accuracy: 0.9870 - val_DiceMetric: 0.6384\n",
            "Epoch 9/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0142 - accuracy: 0.9884 - DiceMetric: 0.7477 - val_loss: 0.0182 - val_accuracy: 0.9871 - val_DiceMetric: 0.6078\n",
            "Epoch 10/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0128 - accuracy: 0.9886 - DiceMetric: 0.7842 - val_loss: 0.0139 - val_accuracy: 0.9873 - val_DiceMetric: 0.7682\n",
            "Epoch 11/25\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0123 - accuracy: 0.9887 - DiceMetric: 0.7847 - val_loss: 0.0153 - val_accuracy: 0.9874 - val_DiceMetric: 0.7214\n",
            "Epoch 12/25\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0118 - accuracy: 0.9887 - DiceMetric: 0.7986 - val_loss: 0.0135 - val_accuracy: 0.9876 - val_DiceMetric: 0.7652\n",
            "Epoch 13/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0112 - accuracy: 0.9888 - DiceMetric: 0.8069 - val_loss: 0.0158 - val_accuracy: 0.9875 - val_DiceMetric: 0.7419\n",
            "Epoch 14/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0110 - accuracy: 0.9888 - DiceMetric: 0.8151 - val_loss: 0.0125 - val_accuracy: 0.9874 - val_DiceMetric: 0.8174\n",
            "Epoch 15/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0106 - accuracy: 0.9889 - DiceMetric: 0.8321 - val_loss: 0.0126 - val_accuracy: 0.9871 - val_DiceMetric: 0.8306\n",
            "Epoch 16/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0106 - accuracy: 0.9888 - DiceMetric: 0.8253 - val_loss: 0.0161 - val_accuracy: 0.9876 - val_DiceMetric: 0.7133\n",
            "Epoch 17/25\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0103 - accuracy: 0.9889 - DiceMetric: 0.8328 - val_loss: 0.0178 - val_accuracy: 0.9873 - val_DiceMetric: 0.6861\n",
            "Epoch 18/25\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0100 - accuracy: 0.9890 - DiceMetric: 0.8357 - val_loss: 0.0129 - val_accuracy: 0.9877 - val_DiceMetric: 0.7887\n",
            "Epoch 19/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0098 - accuracy: 0.9890 - DiceMetric: 0.8455 - val_loss: 0.0341 - val_accuracy: 0.9779 - val_DiceMetric: 0.5949\n",
            "Epoch 20/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0158 - accuracy: 0.9880 - DiceMetric: 0.7030 - val_loss: 0.0137 - val_accuracy: 0.9873 - val_DiceMetric: 0.7796\n",
            "Epoch 21/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0116 - accuracy: 0.9887 - DiceMetric: 0.7971 - val_loss: 0.0136 - val_accuracy: 0.9872 - val_DiceMetric: 0.7553\n",
            "Epoch 22/25\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0106 - accuracy: 0.9889 - DiceMetric: 0.8206 - val_loss: 0.0148 - val_accuracy: 0.9875 - val_DiceMetric: 0.7148\n",
            "Epoch 23/25\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0105 - accuracy: 0.9889 - DiceMetric: 0.8166 - val_loss: 0.0115 - val_accuracy: 0.9878 - val_DiceMetric: 0.8174\n",
            "Epoch 24/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0100 - accuracy: 0.9890 - DiceMetric: 0.8452 - val_loss: 0.0113 - val_accuracy: 0.9875 - val_DiceMetric: 0.8510\n",
            "Epoch 25/25\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0094 - accuracy: 0.9890 - DiceMetric: 0.8458 - val_loss: 0.0120 - val_accuracy: 0.9877 - val_DiceMetric: 0.8250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5d76b85290>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model.fit(X_train,Y_train,batch_size=10,epochs=25,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train,batch_size=10,epochs=200,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvbQrIDSWZih",
        "outputId": "9b09deb2-77fc-4d49-e226-145e8bc5b570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "198/198 [==============================] - 65s 303ms/step - loss: 0.2087 - accuracy: 0.9531 - DiceMetric: 0.0658 - val_loss: 0.0793 - val_accuracy: 0.9855 - val_DiceMetric: 0.0743\n",
            "Epoch 2/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0631 - accuracy: 0.9870 - DiceMetric: 0.0871 - val_loss: 0.0561 - val_accuracy: 0.9855 - val_DiceMetric: 0.0342\n",
            "Epoch 3/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0423 - accuracy: 0.9869 - DiceMetric: 0.2681 - val_loss: 0.0462 - val_accuracy: 0.9855 - val_DiceMetric: 0.0725\n",
            "Epoch 4/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0290 - accuracy: 0.9871 - DiceMetric: 0.4765 - val_loss: 0.0311 - val_accuracy: 0.9862 - val_DiceMetric: 0.3760\n",
            "Epoch 5/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0225 - accuracy: 0.9874 - DiceMetric: 0.5914 - val_loss: 0.0363 - val_accuracy: 0.9859 - val_DiceMetric: 0.2317\n",
            "Epoch 6/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0197 - accuracy: 0.9876 - DiceMetric: 0.6305 - val_loss: 0.0256 - val_accuracy: 0.9862 - val_DiceMetric: 0.5663\n",
            "Epoch 7/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0193 - accuracy: 0.9877 - DiceMetric: 0.6332 - val_loss: 0.0195 - val_accuracy: 0.9871 - val_DiceMetric: 0.5729\n",
            "Epoch 8/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0160 - accuracy: 0.9881 - DiceMetric: 0.7079 - val_loss: 0.0197 - val_accuracy: 0.9871 - val_DiceMetric: 0.5610\n",
            "Epoch 9/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0153 - accuracy: 0.9882 - DiceMetric: 0.7161 - val_loss: 0.0225 - val_accuracy: 0.9867 - val_DiceMetric: 0.4710\n",
            "Epoch 10/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0153 - accuracy: 0.9882 - DiceMetric: 0.7192 - val_loss: 0.0193 - val_accuracy: 0.9873 - val_DiceMetric: 0.5895\n",
            "Epoch 11/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0133 - accuracy: 0.9885 - DiceMetric: 0.7726 - val_loss: 0.0172 - val_accuracy: 0.9867 - val_DiceMetric: 0.6963\n",
            "Epoch 12/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0128 - accuracy: 0.9886 - DiceMetric: 0.7757 - val_loss: 0.0207 - val_accuracy: 0.9869 - val_DiceMetric: 0.5135\n",
            "Epoch 13/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0121 - accuracy: 0.9888 - DiceMetric: 0.7873 - val_loss: 0.0156 - val_accuracy: 0.9874 - val_DiceMetric: 0.6833\n",
            "Epoch 14/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0114 - accuracy: 0.9888 - DiceMetric: 0.8074 - val_loss: 0.0188 - val_accuracy: 0.9874 - val_DiceMetric: 0.6242\n",
            "Epoch 15/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0114 - accuracy: 0.9888 - DiceMetric: 0.8101 - val_loss: 0.0143 - val_accuracy: 0.9872 - val_DiceMetric: 0.7619\n",
            "Epoch 16/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0111 - accuracy: 0.9888 - DiceMetric: 0.8095 - val_loss: 0.0160 - val_accuracy: 0.9874 - val_DiceMetric: 0.6777\n",
            "Epoch 17/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0104 - accuracy: 0.9890 - DiceMetric: 0.8299 - val_loss: 0.0130 - val_accuracy: 0.9876 - val_DiceMetric: 0.7767\n",
            "Epoch 18/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0101 - accuracy: 0.9890 - DiceMetric: 0.8332 - val_loss: 0.0128 - val_accuracy: 0.9877 - val_DiceMetric: 0.7703\n",
            "Epoch 19/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0101 - accuracy: 0.9890 - DiceMetric: 0.8350 - val_loss: 0.0109 - val_accuracy: 0.9877 - val_DiceMetric: 0.8511\n",
            "Epoch 20/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0104 - accuracy: 0.9889 - DiceMetric: 0.8285 - val_loss: 0.0120 - val_accuracy: 0.9878 - val_DiceMetric: 0.8005\n",
            "Epoch 21/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0100 - accuracy: 0.9889 - DiceMetric: 0.8327 - val_loss: 0.0141 - val_accuracy: 0.9876 - val_DiceMetric: 0.7728\n",
            "Epoch 22/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0096 - accuracy: 0.9890 - DiceMetric: 0.8438 - val_loss: 0.0108 - val_accuracy: 0.9877 - val_DiceMetric: 0.8501\n",
            "Epoch 23/200\n",
            "198/198 [==============================] - 63s 319ms/step - loss: 0.0094 - accuracy: 0.9890 - DiceMetric: 0.8489 - val_loss: 0.0119 - val_accuracy: 0.9875 - val_DiceMetric: 0.8384\n",
            "Epoch 24/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0089 - accuracy: 0.9891 - DiceMetric: 0.8624 - val_loss: 0.0111 - val_accuracy: 0.9879 - val_DiceMetric: 0.8237\n",
            "Epoch 25/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0087 - accuracy: 0.9891 - DiceMetric: 0.8587 - val_loss: 0.0100 - val_accuracy: 0.9878 - val_DiceMetric: 0.8633\n",
            "Epoch 26/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0085 - accuracy: 0.9892 - DiceMetric: 0.8703 - val_loss: 0.0101 - val_accuracy: 0.9879 - val_DiceMetric: 0.8551\n",
            "Epoch 27/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0084 - accuracy: 0.9892 - DiceMetric: 0.8728 - val_loss: 0.0109 - val_accuracy: 0.9879 - val_DiceMetric: 0.8388\n",
            "Epoch 28/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0085 - accuracy: 0.9891 - DiceMetric: 0.8630 - val_loss: 0.0101 - val_accuracy: 0.9875 - val_DiceMetric: 0.8727\n",
            "Epoch 29/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0081 - accuracy: 0.9892 - DiceMetric: 0.8795 - val_loss: 0.0107 - val_accuracy: 0.9873 - val_DiceMetric: 0.8658\n",
            "Epoch 30/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0098 - accuracy: 0.9889 - DiceMetric: 0.8358 - val_loss: 0.0107 - val_accuracy: 0.9878 - val_DiceMetric: 0.8480\n",
            "Epoch 31/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0084 - accuracy: 0.9891 - DiceMetric: 0.8701 - val_loss: 0.0102 - val_accuracy: 0.9879 - val_DiceMetric: 0.8584\n",
            "Epoch 32/200\n",
            "198/198 [==============================] - 59s 298ms/step - loss: 0.0082 - accuracy: 0.9892 - DiceMetric: 0.8737 - val_loss: 0.0099 - val_accuracy: 0.9880 - val_DiceMetric: 0.8581\n",
            "Epoch 33/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0084 - accuracy: 0.9892 - DiceMetric: 0.8723 - val_loss: 0.0096 - val_accuracy: 0.9878 - val_DiceMetric: 0.8755\n",
            "Epoch 34/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0081 - accuracy: 0.9892 - DiceMetric: 0.8848 - val_loss: 0.0098 - val_accuracy: 0.9879 - val_DiceMetric: 0.8691\n",
            "Epoch 35/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8856 - val_loss: 0.0100 - val_accuracy: 0.9880 - val_DiceMetric: 0.8593\n",
            "Epoch 36/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0079 - accuracy: 0.9892 - DiceMetric: 0.8872 - val_loss: 0.0096 - val_accuracy: 0.9879 - val_DiceMetric: 0.8734\n",
            "Epoch 37/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0076 - accuracy: 0.9892 - DiceMetric: 0.8876 - val_loss: 0.0094 - val_accuracy: 0.9879 - val_DiceMetric: 0.8777\n",
            "Epoch 38/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0075 - accuracy: 0.9893 - DiceMetric: 0.8892 - val_loss: 0.0093 - val_accuracy: 0.9879 - val_DiceMetric: 0.8816\n",
            "Epoch 39/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8825 - val_loss: 0.0095 - val_accuracy: 0.9880 - val_DiceMetric: 0.8754\n",
            "Epoch 40/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0075 - accuracy: 0.9893 - DiceMetric: 0.8962 - val_loss: 0.0091 - val_accuracy: 0.9879 - val_DiceMetric: 0.8864\n",
            "Epoch 41/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0079 - accuracy: 0.9892 - DiceMetric: 0.8834 - val_loss: 0.0115 - val_accuracy: 0.9879 - val_DiceMetric: 0.8209\n",
            "Epoch 42/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.8938 - val_loss: 0.0185 - val_accuracy: 0.9853 - val_DiceMetric: 0.7629\n",
            "Epoch 43/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0136 - accuracy: 0.9884 - DiceMetric: 0.7462 - val_loss: 0.0121 - val_accuracy: 0.9877 - val_DiceMetric: 0.7995\n",
            "Epoch 44/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0099 - accuracy: 0.9890 - DiceMetric: 0.8323 - val_loss: 0.0105 - val_accuracy: 0.9878 - val_DiceMetric: 0.8486\n",
            "Epoch 45/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0087 - accuracy: 0.9891 - DiceMetric: 0.8627 - val_loss: 0.0111 - val_accuracy: 0.9879 - val_DiceMetric: 0.8429\n",
            "Epoch 46/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0084 - accuracy: 0.9891 - DiceMetric: 0.8704 - val_loss: 0.0103 - val_accuracy: 0.9878 - val_DiceMetric: 0.8613\n",
            "Epoch 47/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0081 - accuracy: 0.9892 - DiceMetric: 0.8773 - val_loss: 0.0103 - val_accuracy: 0.9881 - val_DiceMetric: 0.8476\n",
            "Epoch 48/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8887 - val_loss: 0.0102 - val_accuracy: 0.9880 - val_DiceMetric: 0.8631\n",
            "Epoch 49/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0077 - accuracy: 0.9892 - DiceMetric: 0.8816 - val_loss: 0.0095 - val_accuracy: 0.9879 - val_DiceMetric: 0.8824\n",
            "Epoch 50/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.8887 - val_loss: 0.0093 - val_accuracy: 0.9878 - val_DiceMetric: 0.8831\n",
            "Epoch 51/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0075 - accuracy: 0.9893 - DiceMetric: 0.8919 - val_loss: 0.0124 - val_accuracy: 0.9872 - val_DiceMetric: 0.8396\n",
            "Epoch 52/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0103 - accuracy: 0.9888 - DiceMetric: 0.8268 - val_loss: 0.0113 - val_accuracy: 0.9879 - val_DiceMetric: 0.8229\n",
            "Epoch 53/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0081 - accuracy: 0.9892 - DiceMetric: 0.8817 - val_loss: 0.0107 - val_accuracy: 0.9880 - val_DiceMetric: 0.8417\n",
            "Epoch 54/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8856 - val_loss: 0.0099 - val_accuracy: 0.9879 - val_DiceMetric: 0.8636\n",
            "Epoch 55/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.8891 - val_loss: 0.0090 - val_accuracy: 0.9880 - val_DiceMetric: 0.8891\n",
            "Epoch 56/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0074 - accuracy: 0.9893 - DiceMetric: 0.8930 - val_loss: 0.0095 - val_accuracy: 0.9880 - val_DiceMetric: 0.8731\n",
            "Epoch 57/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8776 - val_loss: 0.0093 - val_accuracy: 0.9880 - val_DiceMetric: 0.8752\n",
            "Epoch 58/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0074 - accuracy: 0.9893 - DiceMetric: 0.8914 - val_loss: 0.0089 - val_accuracy: 0.9880 - val_DiceMetric: 0.8907\n",
            "Epoch 59/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0072 - accuracy: 0.9893 - DiceMetric: 0.9032 - val_loss: 0.0090 - val_accuracy: 0.9878 - val_DiceMetric: 0.8938\n",
            "Epoch 60/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0073 - accuracy: 0.9893 - DiceMetric: 0.8951 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.8932\n",
            "Epoch 61/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9051 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.8888\n",
            "Epoch 62/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9026 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.8901\n",
            "Epoch 63/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0069 - accuracy: 0.9893 - DiceMetric: 0.9051 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.8955\n",
            "Epoch 64/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0069 - accuracy: 0.9893 - DiceMetric: 0.9035 - val_loss: 0.0086 - val_accuracy: 0.9880 - val_DiceMetric: 0.8994\n",
            "Epoch 65/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0070 - accuracy: 0.9893 - DiceMetric: 0.8986 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.8907\n",
            "Epoch 66/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0069 - accuracy: 0.9893 - DiceMetric: 0.9051 - val_loss: 0.0087 - val_accuracy: 0.9879 - val_DiceMetric: 0.8986\n",
            "Epoch 67/200\n",
            "198/198 [==============================] - 60s 303ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9043 - val_loss: 0.0097 - val_accuracy: 0.9880 - val_DiceMetric: 0.8689\n",
            "Epoch 68/200\n",
            "198/198 [==============================] - 60s 306ms/step - loss: 0.0069 - accuracy: 0.9893 - DiceMetric: 0.9032 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.8977\n",
            "Epoch 69/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0069 - accuracy: 0.9893 - DiceMetric: 0.9031 - val_loss: 0.0089 - val_accuracy: 0.9880 - val_DiceMetric: 0.8927\n",
            "Epoch 70/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0070 - accuracy: 0.9893 - DiceMetric: 0.8994 - val_loss: 0.0104 - val_accuracy: 0.9879 - val_DiceMetric: 0.8584\n",
            "Epoch 71/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0090 - accuracy: 0.9891 - DiceMetric: 0.8611 - val_loss: 0.0112 - val_accuracy: 0.9871 - val_DiceMetric: 0.8581\n",
            "Epoch 72/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8852 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.8861\n",
            "Epoch 73/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0070 - accuracy: 0.9893 - DiceMetric: 0.9021 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.8896\n",
            "Epoch 74/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9045 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.8868\n",
            "Epoch 75/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9063 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.8908\n",
            "Epoch 76/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0067 - accuracy: 0.9894 - DiceMetric: 0.9084 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.8892\n",
            "Epoch 77/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9072 - val_loss: 0.0085 - val_accuracy: 0.9880 - val_DiceMetric: 0.9019\n",
            "Epoch 78/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9109 - val_loss: 0.0089 - val_accuracy: 0.9878 - val_DiceMetric: 0.8963\n",
            "Epoch 79/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9173 - val_loss: 0.0097 - val_accuracy: 0.9876 - val_DiceMetric: 0.8875\n",
            "Epoch 80/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0085 - accuracy: 0.9891 - DiceMetric: 0.8676 - val_loss: 0.0089 - val_accuracy: 0.9878 - val_DiceMetric: 0.8976\n",
            "Epoch 81/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0070 - accuracy: 0.9893 - DiceMetric: 0.9039 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.8910\n",
            "Epoch 82/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0068 - accuracy: 0.9893 - DiceMetric: 0.9019 - val_loss: 0.0085 - val_accuracy: 0.9880 - val_DiceMetric: 0.9033\n",
            "Epoch 83/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9166 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.8965\n",
            "Epoch 84/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9133 - val_loss: 0.0085 - val_accuracy: 0.9880 - val_DiceMetric: 0.9000\n",
            "Epoch 85/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9190 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.8928\n",
            "Epoch 86/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9142 - val_loss: 0.0107 - val_accuracy: 0.9870 - val_DiceMetric: 0.8770\n",
            "Epoch 87/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0073 - accuracy: 0.9893 - DiceMetric: 0.8942 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.8928\n",
            "Epoch 88/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9136 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9021\n",
            "Epoch 89/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9188 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.8956\n",
            "Epoch 90/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9213 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9037\n",
            "Epoch 91/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9206 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9036\n",
            "Epoch 92/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9197 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.8991\n",
            "Epoch 93/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9172 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.9012\n",
            "Epoch 94/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9198 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9034\n",
            "Epoch 95/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0062 - accuracy: 0.9894 - DiceMetric: 0.9241 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9040\n",
            "Epoch 96/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0063 - accuracy: 0.9894 - DiceMetric: 0.9188 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.8992\n",
            "Epoch 97/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0062 - accuracy: 0.9894 - DiceMetric: 0.9210 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9084\n",
            "Epoch 98/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9894 - DiceMetric: 0.9205 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9006\n",
            "Epoch 99/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0063 - accuracy: 0.9894 - DiceMetric: 0.9180 - val_loss: 0.0085 - val_accuracy: 0.9879 - val_DiceMetric: 0.9047\n",
            "Epoch 100/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9231 - val_loss: 0.0082 - val_accuracy: 0.9880 - val_DiceMetric: 0.9081\n",
            "Epoch 101/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9231 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9015\n",
            "Epoch 102/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9227 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9035\n",
            "Epoch 103/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9259 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9049\n",
            "Epoch 104/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0062 - accuracy: 0.9894 - DiceMetric: 0.9246 - val_loss: 0.0085 - val_accuracy: 0.9880 - val_DiceMetric: 0.9058\n",
            "Epoch 105/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9207 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.8979\n",
            "Epoch 106/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9199 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.8980\n",
            "Epoch 107/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9274 - val_loss: 0.0084 - val_accuracy: 0.9879 - val_DiceMetric: 0.9077\n",
            "Epoch 108/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9894 - DiceMetric: 0.9205 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9090\n",
            "Epoch 109/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9237 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9056\n",
            "Epoch 110/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9211 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9096\n",
            "Epoch 111/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9242 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9083\n",
            "Epoch 112/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0062 - accuracy: 0.9894 - DiceMetric: 0.9220 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9085\n",
            "Epoch 113/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9213 - val_loss: 0.0081 - val_accuracy: 0.9882 - val_DiceMetric: 0.9068\n",
            "Epoch 114/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9238 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9012\n",
            "Epoch 115/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9258 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9038\n",
            "Epoch 116/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9266 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9106\n",
            "Epoch 117/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9219 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.8975\n",
            "Epoch 118/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9285 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9116\n",
            "Epoch 119/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9268 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9097\n",
            "Epoch 120/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9220 - val_loss: 0.0095 - val_accuracy: 0.9881 - val_DiceMetric: 0.9030\n",
            "Epoch 121/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9262 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9061\n",
            "Epoch 122/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9265 - val_loss: 0.0081 - val_accuracy: 0.9880 - val_DiceMetric: 0.9131\n",
            "Epoch 123/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9348 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9056\n",
            "Epoch 124/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9266 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9114\n",
            "Epoch 125/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9292 - val_loss: 0.0081 - val_accuracy: 0.9881 - val_DiceMetric: 0.9122\n",
            "Epoch 126/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9315 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9112\n",
            "Epoch 127/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9317 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9056\n",
            "Epoch 128/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9308 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9097\n",
            "Epoch 129/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9324 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.9078\n",
            "Epoch 130/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9306 - val_loss: 0.0084 - val_accuracy: 0.9883 - val_DiceMetric: 0.9081\n",
            "Epoch 131/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9325 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9051\n",
            "Epoch 132/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9334 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9116\n",
            "Epoch 133/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9345 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9041\n",
            "Epoch 134/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9207 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9055\n",
            "Epoch 135/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9292 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9030\n",
            "Epoch 136/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9247 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9102\n",
            "Epoch 137/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9362 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9082\n",
            "Epoch 138/200\n",
            "198/198 [==============================] - 63s 320ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9289 - val_loss: 0.0080 - val_accuracy: 0.9881 - val_DiceMetric: 0.9163\n",
            "Epoch 139/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9328 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9046\n",
            "Epoch 140/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9344 - val_loss: 0.0082 - val_accuracy: 0.9880 - val_DiceMetric: 0.9123\n",
            "Epoch 141/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9237 - val_loss: 0.0081 - val_accuracy: 0.9881 - val_DiceMetric: 0.9134\n",
            "Epoch 142/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9343 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9148\n",
            "Epoch 143/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9386 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9069\n",
            "Epoch 144/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.8986 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9050\n",
            "Epoch 145/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9271 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9137\n",
            "Epoch 146/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9385 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.8995\n",
            "Epoch 147/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9382 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9100\n",
            "Epoch 148/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9373 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9179\n",
            "Epoch 149/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9406 - val_loss: 0.0080 - val_accuracy: 0.9881 - val_DiceMetric: 0.9178\n",
            "Epoch 150/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9896 - DiceMetric: 0.9362 - val_loss: 0.0080 - val_accuracy: 0.9881 - val_DiceMetric: 0.9169\n",
            "Epoch 151/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9896 - DiceMetric: 0.9364 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9165\n",
            "Epoch 152/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9417 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9151\n",
            "Epoch 153/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9344 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9078\n",
            "Epoch 154/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9354 - val_loss: 0.0086 - val_accuracy: 0.9883 - val_DiceMetric: 0.9059\n",
            "Epoch 155/200\n",
            "198/198 [==============================] - 59s 299ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9330 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9171\n",
            "Epoch 156/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9366 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9011\n",
            "Epoch 157/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9285 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9117\n",
            "Epoch 158/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9361 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9115\n",
            "Epoch 159/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9350 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9145\n",
            "Epoch 160/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9388 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9163\n",
            "Epoch 161/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9431 - val_loss: 0.0080 - val_accuracy: 0.9880 - val_DiceMetric: 0.9152\n",
            "Epoch 162/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9365 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9132\n",
            "Epoch 163/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0053 - accuracy: 0.9895 - DiceMetric: 0.9408 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9026\n",
            "Epoch 164/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9194 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9156\n",
            "Epoch 165/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9397 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9113\n",
            "Epoch 166/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9335 - val_loss: 0.0080 - val_accuracy: 0.9880 - val_DiceMetric: 0.9128\n",
            "Epoch 167/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9345 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9168\n",
            "Epoch 168/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9896 - DiceMetric: 0.9413 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9137\n",
            "Epoch 169/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9402 - val_loss: 0.0080 - val_accuracy: 0.9881 - val_DiceMetric: 0.9150\n",
            "Epoch 170/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9338 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9178\n",
            "Epoch 171/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9369 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9177\n",
            "Epoch 172/200\n",
            "198/198 [==============================] - 64s 321ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9417 - val_loss: 0.0081 - val_accuracy: 0.9881 - val_DiceMetric: 0.9165\n",
            "Epoch 173/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0053 - accuracy: 0.9895 - DiceMetric: 0.9358 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9169\n",
            "Epoch 174/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9359 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9169\n",
            "Epoch 175/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9381 - val_loss: 0.0080 - val_accuracy: 0.9881 - val_DiceMetric: 0.9170\n",
            "Epoch 176/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9460 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9176\n",
            "Epoch 177/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9430 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9103\n",
            "Epoch 178/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9376 - val_loss: 0.0082 - val_accuracy: 0.9880 - val_DiceMetric: 0.9145\n",
            "Epoch 179/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9412 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9183\n",
            "Epoch 180/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9404 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9147\n",
            "Epoch 181/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9436 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9163\n",
            "Epoch 182/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9411 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9161\n",
            "Epoch 183/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9450 - val_loss: 0.0097 - val_accuracy: 0.9874 - val_DiceMetric: 0.8929\n",
            "Epoch 184/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9039 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9116\n",
            "Epoch 185/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9342 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9110\n",
            "Epoch 186/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9416 - val_loss: 0.0081 - val_accuracy: 0.9882 - val_DiceMetric: 0.9151\n",
            "Epoch 187/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9353 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9153\n",
            "Epoch 188/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9403 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9165\n",
            "Epoch 189/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9456 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9162\n",
            "Epoch 190/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9455 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9091\n",
            "Epoch 191/200\n",
            "198/198 [==============================] - 63s 321ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9417 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9096\n",
            "Epoch 192/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9423 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9176\n",
            "Epoch 193/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9474 - val_loss: 0.0080 - val_accuracy: 0.9881 - val_DiceMetric: 0.9163\n",
            "Epoch 194/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9483 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9160\n",
            "Epoch 195/200\n",
            "198/198 [==============================] - 64s 321ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9455 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9199\n",
            "Epoch 196/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9459 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9191\n",
            "Epoch 197/200\n",
            "198/198 [==============================] - 64s 321ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9488 - val_loss: 0.0081 - val_accuracy: 0.9881 - val_DiceMetric: 0.9178\n",
            "Epoch 198/200\n",
            "198/198 [==============================] - 64s 321ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9484 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9173\n",
            "Epoch 199/200\n",
            "198/198 [==============================] - 59s 300ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9451 - val_loss: 0.0083 - val_accuracy: 0.9883 - val_DiceMetric: 0.9162\n",
            "Epoch 200/200\n",
            "198/198 [==============================] - 64s 321ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9490 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9208\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f60bfee8f50>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_model()\n",
        "model.fit(X_train,Y_train,batch_size=16,epochs=25)"
      ],
      "metadata": {
        "id": "IKTZfL-G6uW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27315517-fe68-48cc-b432-2269bbc278ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "214/214 [==============================] - 284s 463ms/step - loss: 0.1899 - accuracy: 0.9534 - DiceMetric: 0.1967\n",
            "Epoch 2/25\n",
            "214/214 [==============================] - 95s 446ms/step - loss: 0.0295 - accuracy: 0.9899 - DiceMetric: 0.7143\n",
            "Epoch 3/25\n",
            "214/214 [==============================] - 95s 446ms/step - loss: 0.0158 - accuracy: 0.9902 - DiceMetric: 0.8299\n",
            "Epoch 4/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0113 - accuracy: 0.9904 - DiceMetric: 0.8668\n",
            "Epoch 5/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0095 - accuracy: 0.9904 - DiceMetric: 0.8781\n",
            "Epoch 6/25\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0082 - accuracy: 0.9905 - DiceMetric: 0.8978\n",
            "Epoch 7/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0076 - accuracy: 0.9905 - DiceMetric: 0.8970\n",
            "Epoch 8/25\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0071 - accuracy: 0.9905 - DiceMetric: 0.9086\n",
            "Epoch 9/25\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0066 - accuracy: 0.9905 - DiceMetric: 0.9146\n",
            "Epoch 10/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0063 - accuracy: 0.9905 - DiceMetric: 0.9166\n",
            "Epoch 11/25\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0061 - accuracy: 0.9906 - DiceMetric: 0.9193\n",
            "Epoch 12/25\n",
            "214/214 [==============================] - 95s 446ms/step - loss: 0.0059 - accuracy: 0.9906 - DiceMetric: 0.9240\n",
            "Epoch 13/25\n",
            "214/214 [==============================] - 95s 446ms/step - loss: 0.0057 - accuracy: 0.9906 - DiceMetric: 0.9270\n",
            "Epoch 14/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0056 - accuracy: 0.9906 - DiceMetric: 0.9296\n",
            "Epoch 15/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0055 - accuracy: 0.9906 - DiceMetric: 0.9319\n",
            "Epoch 16/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0053 - accuracy: 0.9906 - DiceMetric: 0.9343\n",
            "Epoch 17/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0052 - accuracy: 0.9906 - DiceMetric: 0.9314\n",
            "Epoch 18/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0051 - accuracy: 0.9906 - DiceMetric: 0.9369\n",
            "Epoch 19/25\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0051 - accuracy: 0.9906 - DiceMetric: 0.9380\n",
            "Epoch 20/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0050 - accuracy: 0.9906 - DiceMetric: 0.9392\n",
            "Epoch 21/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0050 - accuracy: 0.9906 - DiceMetric: 0.9369\n",
            "Epoch 22/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0049 - accuracy: 0.9906 - DiceMetric: 0.9410\n",
            "Epoch 23/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0049 - accuracy: 0.9906 - DiceMetric: 0.9421\n",
            "Epoch 24/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0048 - accuracy: 0.9906 - DiceMetric: 0.9385\n",
            "Epoch 25/25\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0047 - accuracy: 0.9906 - DiceMetric: 0.9456\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1d00ad190>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {1}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LIwoLHNO8zx",
        "outputId": "7d5377d9-8263-4cc1-c324-09b87ce5da2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.12995441257953644; accuracy of 94.89102363586426% DiceMetric of 58.48127603530884%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=create_model()\n",
        "model.fit(X_train,Y_train,batch_size=16,epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrasvdY5fRF8",
        "outputId": "9cc2dd6a-0be4-4f61-b707-19eb0a3bd9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "214/214 [==============================] - 270s 442ms/step - loss: 0.2142 - accuracy: 0.9353 - DiceMetric: 0.1456\n",
            "Epoch 2/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0414 - accuracy: 0.9890 - DiceMetric: 0.5053\n",
            "Epoch 3/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0221 - accuracy: 0.9900 - DiceMetric: 0.7597\n",
            "Epoch 4/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0150 - accuracy: 0.9902 - DiceMetric: 0.8352\n",
            "Epoch 5/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0117 - accuracy: 0.9904 - DiceMetric: 0.8662\n",
            "Epoch 6/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0099 - accuracy: 0.9904 - DiceMetric: 0.8792\n",
            "Epoch 7/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0087 - accuracy: 0.9904 - DiceMetric: 0.8928\n",
            "Epoch 8/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0079 - accuracy: 0.9905 - DiceMetric: 0.8971\n",
            "Epoch 9/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0074 - accuracy: 0.9905 - DiceMetric: 0.9051\n",
            "Epoch 10/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0070 - accuracy: 0.9905 - DiceMetric: 0.9116\n",
            "Epoch 11/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0067 - accuracy: 0.9905 - DiceMetric: 0.9106\n",
            "Epoch 12/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0063 - accuracy: 0.9906 - DiceMetric: 0.9165\n",
            "Epoch 13/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0061 - accuracy: 0.9906 - DiceMetric: 0.9183\n",
            "Epoch 14/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0059 - accuracy: 0.9906 - DiceMetric: 0.9223\n",
            "Epoch 15/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0058 - accuracy: 0.9906 - DiceMetric: 0.9278\n",
            "Epoch 16/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0056 - accuracy: 0.9906 - DiceMetric: 0.9260\n",
            "Epoch 17/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0055 - accuracy: 0.9906 - DiceMetric: 0.9310\n",
            "Epoch 18/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0054 - accuracy: 0.9906 - DiceMetric: 0.9332\n",
            "Epoch 19/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0053 - accuracy: 0.9906 - DiceMetric: 0.9354\n",
            "Epoch 20/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0053 - accuracy: 0.9906 - DiceMetric: 0.9308\n",
            "Epoch 21/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0051 - accuracy: 0.9906 - DiceMetric: 0.9379\n",
            "Epoch 22/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0051 - accuracy: 0.9906 - DiceMetric: 0.9393\n",
            "Epoch 23/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0050 - accuracy: 0.9906 - DiceMetric: 0.9396\n",
            "Epoch 24/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0049 - accuracy: 0.9906 - DiceMetric: 0.9412\n",
            "Epoch 25/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0049 - accuracy: 0.9906 - DiceMetric: 0.9413\n",
            "Epoch 26/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0048 - accuracy: 0.9906 - DiceMetric: 0.9435\n",
            "Epoch 27/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0047 - accuracy: 0.9906 - DiceMetric: 0.9412\n",
            "Epoch 28/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0047 - accuracy: 0.9906 - DiceMetric: 0.9448\n",
            "Epoch 29/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0047 - accuracy: 0.9906 - DiceMetric: 0.9475\n",
            "Epoch 30/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0046 - accuracy: 0.9906 - DiceMetric: 0.9427\n",
            "Epoch 31/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0046 - accuracy: 0.9906 - DiceMetric: 0.9486\n",
            "Epoch 32/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0046 - accuracy: 0.9907 - DiceMetric: 0.9482\n",
            "Epoch 33/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0045 - accuracy: 0.9907 - DiceMetric: 0.9496\n",
            "Epoch 34/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0045 - accuracy: 0.9907 - DiceMetric: 0.9513\n",
            "Epoch 35/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0044 - accuracy: 0.9907 - DiceMetric: 0.9465\n",
            "Epoch 36/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0044 - accuracy: 0.9907 - DiceMetric: 0.9522\n",
            "Epoch 37/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0044 - accuracy: 0.9907 - DiceMetric: 0.9526\n",
            "Epoch 38/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0043 - accuracy: 0.9907 - DiceMetric: 0.9542\n",
            "Epoch 39/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0043 - accuracy: 0.9907 - DiceMetric: 0.9545\n",
            "Epoch 40/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0047 - accuracy: 0.9906 - DiceMetric: 0.9443\n",
            "Epoch 41/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0043 - accuracy: 0.9907 - DiceMetric: 0.9498\n",
            "Epoch 42/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0042 - accuracy: 0.9907 - DiceMetric: 0.9557\n",
            "Epoch 43/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0042 - accuracy: 0.9907 - DiceMetric: 0.9554\n",
            "Epoch 44/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0042 - accuracy: 0.9907 - DiceMetric: 0.9559\n",
            "Epoch 45/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0042 - accuracy: 0.9907 - DiceMetric: 0.9568\n",
            "Epoch 46/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0042 - accuracy: 0.9907 - DiceMetric: 0.9569\n",
            "Epoch 47/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9579\n",
            "Epoch 48/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9575\n",
            "Epoch 49/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9587\n",
            "Epoch 50/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9578\n",
            "Epoch 51/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9592\n",
            "Epoch 52/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9603\n",
            "Epoch 53/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0040 - accuracy: 0.9907 - DiceMetric: 0.9607\n",
            "Epoch 54/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0041 - accuracy: 0.9907 - DiceMetric: 0.9591\n",
            "Epoch 55/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0040 - accuracy: 0.9907 - DiceMetric: 0.9592\n",
            "Epoch 56/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0040 - accuracy: 0.9907 - DiceMetric: 0.9602\n",
            "Epoch 57/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0040 - accuracy: 0.9907 - DiceMetric: 0.9614\n",
            "Epoch 58/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0040 - accuracy: 0.9907 - DiceMetric: 0.9620\n",
            "Epoch 59/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0040 - accuracy: 0.9907 - DiceMetric: 0.9619\n",
            "Epoch 60/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9627\n",
            "Epoch 61/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9625\n",
            "Epoch 62/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9630\n",
            "Epoch 63/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9625\n",
            "Epoch 64/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9635\n",
            "Epoch 65/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9639\n",
            "Epoch 66/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0039 - accuracy: 0.9907 - DiceMetric: 0.9642\n",
            "Epoch 67/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9602\n",
            "Epoch 68/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9650\n",
            "Epoch 69/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9653\n",
            "Epoch 70/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9646\n",
            "Epoch 71/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9655\n",
            "Epoch 72/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9659\n",
            "Epoch 73/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9665\n",
            "Epoch 74/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9622\n",
            "Epoch 75/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9670\n",
            "Epoch 76/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9667\n",
            "Epoch 77/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0038 - accuracy: 0.9907 - DiceMetric: 0.9654\n",
            "Epoch 78/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9665\n",
            "Epoch 79/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9621\n",
            "Epoch 80/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9684\n",
            "Epoch 81/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9677\n",
            "Epoch 82/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9680\n",
            "Epoch 83/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9691\n",
            "Epoch 84/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9679\n",
            "Epoch 85/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9642\n",
            "Epoch 86/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9694\n",
            "Epoch 87/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9647\n",
            "Epoch 88/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9694\n",
            "Epoch 89/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9651\n",
            "Epoch 90/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9703\n",
            "Epoch 91/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0037 - accuracy: 0.9907 - DiceMetric: 0.9688\n",
            "Epoch 92/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9699\n",
            "Epoch 93/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9708\n",
            "Epoch 94/100\n",
            "214/214 [==============================] - 95s 445ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9707\n",
            "Epoch 95/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9708\n",
            "Epoch 96/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9665\n",
            "Epoch 97/100\n",
            "214/214 [==============================] - 95s 444ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9715\n",
            "Epoch 98/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9715\n",
            "Epoch 99/100\n",
            "214/214 [==============================] - 95s 443ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9713\n",
            "Epoch 100/100\n",
            "214/214 [==============================] - 95s 442ms/step - loss: 0.0036 - accuracy: 0.9907 - DiceMetric: 0.9701\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe17ab88090>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0yz227RzmyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c88059-4609-434d-b7f3-c83750545b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 0: loss of 0.006549478508532047; accuracy of 98.9666998386383% DiceMetric of 92.21749305725098%\n",
            "['loss', 'accuracy', 'DiceMetric']\n"
          ]
        }
      ],
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oud92q0zgO7x"
      },
      "outputs": [],
      "source": [
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFP-3gQ9BO8f"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0-RG0wjzA6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a772a8fe-f307-4356-a2d0-96e418a56a06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 3s 332ms/step\n"
          ]
        }
      ],
      "source": [
        "Ypred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "J1ra2EoVycvl",
        "outputId": "33cfe349-30e5-42bc-bba0-ffa140bf0518"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b3f8d543cfee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Ypred' is not defined"
          ]
        }
      ],
      "source": [
        "plt.imshow(np.reshape(Ypred[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "plt.imshow(np.reshape(Y_test[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "Ypred=model.predict(X_test)\n",
        "rid=random.randint(0,Ypred.shape[0]-1)\n",
        "plt.imshow(np.reshape(Ypred[rid],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "plt.imshow(np.reshape(Y_test[rid],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "mj_YQVerfsFs",
        "outputId": "1f74501b-7d76-47fc-a235-4b906209e0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 192ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikB_eI_ZmVtq"
      },
      "outputs": [],
      "source": [
        "def dice(true_mask, pred_mask):\n",
        "    \"\"\"\n",
        "        Computes the Dice coefficient.\n",
        "        Args:\n",
        "            true_mask : Array of arbitrary shape.\n",
        "            pred_mask : Array with the same shape than true_mask.  \n",
        "        \n",
        "        Returns:\n",
        "            A scalar representing the Dice coefficient between the two segmentations. \n",
        "        \n",
        "    \"\"\"\n",
        "    non_seg_score=1.0\n",
        "    if type(pred_mask) != np.ndarray:\n",
        "      t = torch.Tensor([0.5])\n",
        "      pred_mask=(pred_mask > t)\n",
        "    else:\n",
        "      pred_mask[pred_mask>=0.5]=1\n",
        "      pred_mask[pred_mask<0.5]=0\n",
        "\n",
        "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
        "    im_sum = true_mask.sum() + pred_mask.sum()\n",
        "    if im_sum == 0:\n",
        "        return non_seg_score\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(true_mask, pred_mask)\n",
        "    return 2. * intersection.sum() / im_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBVFzRQmXI-",
        "outputId": "51ce2c6f-6865-46b0-fa59-3128c9f76d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9357399154707194"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diceaux=dice(Y_test[160],Ypred[160])\n",
        "diceaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2Sj8PPuJBv"
      },
      "source": [
        "## Model Fit Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh7g3mmhuNPV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj7ESdJAuOaW"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erWVgdGfhXKu"
      },
      "source": [
        "25 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7gaF3LTvOU-",
        "outputId": "9618694e-c8fb-4139-9afc-63238c7bd3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.010589624755084515; accuracy of 98.96852374076843% DiceMetric of 90.11987447738647%\n",
            "Score for fold 2: loss of 0.017074687406420708; accuracy of 98.8399863243103% DiceMetric of 88.72661590576172%\n",
            "Score for fold 3: loss of 0.019004367291927338; accuracy of 98.89352321624756% DiceMetric of 85.86874604225159%\n",
            "Score for fold 4: loss of 0.021750498563051224; accuracy of 98.8770842552185% DiceMetric of 87.11013793945312%\n",
            "Score for fold 5: loss of 0.02564629353582859; accuracy of 98.92171621322632% DiceMetric of 84.32435989379883%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')"
      ],
      "metadata": {
        "id": "7tddx2knzApM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804af329-0df8-46b0-e952-e116d832f0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.005324899218976498; accuracy of 99.5372474193573% DiceMetric of 42.12750792503357%\n",
            "Score for fold 1: loss of 0.024395179003477097; accuracy of 98.28765392303467% DiceMetric of 52.39614248275757%\n",
            "Score for fold 1: loss of 0.021177247166633606; accuracy of 98.77491593360901% DiceMetric of 41.878390312194824%\n",
            "Score for fold 1: loss of 0.030100753530859947; accuracy of 97.71497249603271% DiceMetric of 57.58134126663208%\n",
            "Score for fold 1: loss of 0.04715128615498543; accuracy of 98.46388101577759% DiceMetric of 30.85877299308777%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(metadat,metadat):\n",
        "  model = create_model()\n",
        "  for i,x in enumerate(train_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      X_fold=X[ri:ri+size]\n",
        "      Y_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      X_fold=np.concatenate((X_fold,X[ri:ri+size]),axis=0)\n",
        "      Y_fold=np.concatenate((Y_fold,Y[ri:ri+size]),axis=0)\n",
        "  for i,x in enumerate(val_index):\n",
        "    size=metadat[x][1]\n",
        "    ri=metadat[x][2]\n",
        "    if i == 0:\n",
        "      Xtest_fold=X[ri:ri+size]\n",
        "      Ytest_fold=Y[ri:ri+size]\n",
        "    else:\n",
        "      Xtest_fold=np.concatenate((Xtest_fold,X[ri:ri+size]),axis=0)\n",
        "      Ytest_fold=np.concatenate((Ytest_fold,Y[ri:ri+size]),axis=0)\n",
        "\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,verbose=0)\n",
        "\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svks7GQyaIsL",
        "outputId": "b6c3008e-7ebd-40fb-863d-5dc318027871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.005309466738253832; accuracy of 99.54627752304077% DiceMetric of 41.23751223087311%\n",
            "Score for fold 1: loss of 0.02331027388572693; accuracy of 98.27497005462646% DiceMetric of 53.798168897628784%\n",
            "Score for fold 1: loss of 0.024359682574868202; accuracy of 98.72991442680359% DiceMetric of 41.83901846408844%\n",
            "Score for fold 1: loss of 0.025078967213630676; accuracy of 98.13305139541626% DiceMetric of 65.87681174278259%\n",
            "Score for fold 1: loss of 0.015188219025731087; accuracy of 99.03878569602966% DiceMetric of 40.995147824287415%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOWim3VxhZsj"
      },
      "source": [
        "50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7om-UlhbP9",
        "outputId": "7142d030-2897-49ad-b206-aea1d08d28a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.016142617911100388; accuracy of 98.95281195640564% DiceMetric of 86.89823746681213%\n",
            "Score for fold 2: loss of 0.02502174861729145; accuracy of 98.81399273872375% DiceMetric of 86.29720211029053%\n",
            "Score for fold 3: loss of 0.029437260702252388; accuracy of 98.87399077415466% DiceMetric of 82.6685905456543%\n",
            "Score for fold 4: loss of 0.027194472029805183; accuracy of 98.87163639068604% DiceMetric of 86.16846203804016%\n",
            "Score for fold 5: loss of 0.029874801635742188; accuracy of 98.91415238380432% DiceMetric of 83.36634039878845%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMMVWntcFFRRt59KOnyBR52",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}