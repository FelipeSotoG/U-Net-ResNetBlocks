{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeSotoG/U-Net-ResNetBlocks/blob/main/Attention_Resnet_Multihead_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcH2AvTh97gF"
      },
      "source": [
        "##Descarga datos\n",
        "Los datos se encuentran en el drive, por lo que usara gdown para sacarlos directamente y no tener que hacer la coneccion, ya que estamos descargando un zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GLXIiagwH3",
        "outputId": "a47058c8-9d75-4b29-e29c-5273178115e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt\n",
            "To: /content/input.zip\n",
            "100% 597M/597M [00:05<00:00, 109MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uq8I6BfRhogZ"
      },
      "outputs": [],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9Y2Q3Bce47L"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jqAPlaMKtpg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmJoArAEd41C"
      },
      "outputs": [],
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  X[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)\n",
        "mas=\"/masks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  Y[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)/255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Borrar directorio /input en caso de error"
      ],
      "metadata": {
        "id": "3NcX5DC3VvAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/input"
      ],
      "metadata": {
        "id": "-QywVJIaZjHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6IAoTKTuO5B"
      },
      "source": [
        "##Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ub5XKwbdAQ32"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CambWxdEwn"
      },
      "source": [
        "##Resnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "FlAI4R5nZPxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DiceMetric(y_true, y_pred):\n",
        "  smooth=1e-6 \n",
        "  gama=2\n",
        "  y_true, y_pred = tf.cast(\n",
        "      y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
        "  nominator = 2 * \\\n",
        "      tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
        "  denominator = tf.reduce_sum(\n",
        "      y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
        "  result = tf.divide(nominator, denominator)\n",
        "  return result\n",
        "def DiceLoss(y_true, y_pred):\n",
        "      result= 1- DiceMetric(y_true, y_pred)\n",
        "      return result"
      ],
      "metadata": {
        "id": "Urgfb0zMWlz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (2, 2), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (2, 2), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (2, 2), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,s"
      ],
      "metadata": {
        "id": "MDQHBprOXZXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Intento layer"
      ],
      "metadata": {
        "id": "fGSmVFONoWbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class LinearTransform(tf.keras.layers.Layer):\n",
        "    \"\"\"Layer that implements y=m*x+b, where m and b are\n",
        "    learnable parameters.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_channels=16,\n",
        "        gamma_initializer=\"ones\",\n",
        "        beta_initializer=\"zeros\",\n",
        "        dtype=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(dtype=dtype, **kwargs)\n",
        "        self.gamma_initializer = gamma_initializer\n",
        "        self.beta_initializer = beta_initializer\n",
        "        self.num_channels=num_channels\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        num_channels = self.num_channels\n",
        "        self.gamma = self.add_weight(\n",
        "            \"gamma\",\n",
        "            shape=[num_channels],\n",
        "            initializer=self.gamma_initializer,\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "        self.beta = self.add_weight(\n",
        "            \"beta\",\n",
        "            shape=[num_channels],\n",
        "            initializer=self.beta_initializer,\n",
        "            dtype=self.dtype,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.gamma * inputs + self.beta"
      ],
      "metadata": {
        "id": "KGN5bzIoaqFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class Attention(Layer):\n",
        "  def __init__(self,heads=4):\n",
        "        super().__init__()\n",
        "        self.heads=heads\n",
        "\n",
        "  def build(self,input_shape):\n",
        "      d=input_shape[-1]/self.heads\n",
        "      self.q=tf.keras.layers.Conv2D(d, (1, 1), kernel_initializer='he_normal', padding='same')\n",
        "      self.v=tf.keras.layers.Conv2D(d, (1, 1), kernel_initializer='he_normal', padding='same')\n",
        "      self.k=tf.keras.layers.Conv2D(d, (1, 1), kernel_initializer='he_normal', padding='same')\n",
        "  def call(self,Q,V):\n",
        "      conc = []\n",
        "      for i in range(self.heads):\n",
        "        Q1  = self.q(Q)\n",
        "        V1  = self.v(Q.shape[-1], (2, 2), strides=(2, 2), padding='same')(V) \n",
        "        V1  = self.v(V1)\n",
        "        K1  = self.k(V1)\n",
        "        conc.append(tf.keras.layers.Attention()([Q1,V1,K1]))\n",
        "        out = tf.keras.layers.Concatenate()(conc)\n",
        "      return out"
      ],
      "metadata": {
        "id": "wiz8y6pEk_Yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### block"
      ],
      "metadata": {
        "id": "rl7cVxWBohSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attentionblock(Q,V,heads): \n",
        "  conc = []\n",
        "  V  = tf.keras.layers.Conv2DTranspose(Q.shape[-1], (2, 2), strides=(2, 2), padding='same')(V)\n",
        "  for i in range(heads):\n",
        "        Q1  = tf.keras.layers.Conv2D(Q.shape[-1]/heads, (1, 1), kernel_initializer='he_normal', padding='same')(Q)\n",
        "        V1  = tf.keras.layers.Conv2D(Q.shape[-1]/heads, (1, 1), kernel_initializer='he_normal', padding='same')(V)\n",
        "        K1  = tf.keras.layers.Conv2D(Q.shape[-1]/heads, (1, 1), kernel_initializer='he_normal', padding='same')(V)\n",
        "        conc.append(tf.keras.layers.Attention()([Q1,V1,K1]))\n",
        "  out = tf.keras.layers.Concatenate()(conc)\n",
        "  return out"
      ],
      "metadata": {
        "id": "XW4JysTrfOv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def attentionblock(Q,V,heads): \n",
        "  conc = []\n",
        "  V  = tf.keras.layers.Conv2DTranspose(Q.shape[-1], (2, 2), strides=(2, 2), padding='same')(V)\n",
        "  for i in range(heads):\n",
        "        Q1  = tf.keras.layers.Conv2D(Q.shape[-1]/heads, (1))(Q)\n",
        "        V1  = tf.keras.layers.Conv2D(Q.shape[-1]/heads, (1))(V)\n",
        "        K1  = tf.keras.layers.Conv2D(Q.shape[-1]/heads, (1))(V)\n",
        "        conc.append(tf.keras.layers.Attention()([Q1,V1,K1]))\n",
        "  out = tf.keras.layers.Concatenate()(conc)\n",
        "  return out"
      ],
      "metadata": {
        "id": "vEOzQFpIGhU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "KfUBKdBUocQF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJgrBPxjiV-c",
        "outputId": "d52765f0-3366-438c-8ba6-335879e4b7bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_18\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_52 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_51 (Lambda)             (None, 128, 128, 1)  0           ['input_52[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_983 (Conv2D)            (None, 128, 128, 16  80          ['lambda_51[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_861 (Batch  (None, 128, 128, 16  64         ['conv2d_983[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_287 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_861[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_984 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_287[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_985 (Conv2D)            (None, 128, 128, 16  80          ['lambda_51[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_862 (Batch  (None, 128, 128, 16  64         ['conv2d_984[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_863 (Batch  (None, 128, 128, 16  64         ['conv2d_985[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_287 (Add)                  (None, 128, 128, 16  0           ['batch_normalization_862[0][0]',\n",
            "                                )                                 'batch_normalization_863[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_287 (ReLU)               (None, 128, 128, 16  0           ['add_287[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_204 (MaxPooling2  (None, 64, 64, 16)  0           ['re_lu_287[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_986 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_204[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_864 (Batch  (None, 64, 64, 32)  128         ['conv2d_986[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_288 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_864[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_987 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_288[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_988 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_204[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_865 (Batch  (None, 64, 64, 32)  128         ['conv2d_987[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_866 (Batch  (None, 64, 64, 32)  128         ['conv2d_988[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_288 (Add)                  (None, 64, 64, 32)   0           ['batch_normalization_865[0][0]',\n",
            "                                                                  'batch_normalization_866[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_288 (ReLU)               (None, 64, 64, 32)   0           ['add_288[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_205 (MaxPooling2  (None, 32, 32, 32)  0           ['re_lu_288[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_989 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_205[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_867 (Batch  (None, 32, 32, 64)  256         ['conv2d_989[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_289 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_867[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_990 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_289[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_991 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_205[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_868 (Batch  (None, 32, 32, 64)  256         ['conv2d_990[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_869 (Batch  (None, 32, 32, 64)  256         ['conv2d_991[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_289 (Add)                  (None, 32, 32, 64)   0           ['batch_normalization_868[0][0]',\n",
            "                                                                  'batch_normalization_869[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_289 (ReLU)               (None, 32, 32, 64)   0           ['add_289[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_206 (MaxPooling2  (None, 16, 16, 64)  0           ['re_lu_289[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_992 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_206[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_870 (Batch  (None, 16, 16, 128)  512        ['conv2d_992[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_290 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_870[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_993 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_290[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_994 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_206[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_871 (Batch  (None, 16, 16, 128)  512        ['conv2d_993[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_872 (Batch  (None, 16, 16, 128)  512        ['conv2d_994[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_290 (Add)                  (None, 16, 16, 128)  0           ['batch_normalization_871[0][0]',\n",
            "                                                                  'batch_normalization_872[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_290 (ReLU)               (None, 16, 16, 128)  0           ['add_290[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_207 (MaxPooling2  (None, 8, 8, 128)   0           ['re_lu_290[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_995 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_207[0][0]']      \n",
            "                                                                                                  \n",
            " batch_normalization_873 (Batch  (None, 8, 8, 256)   1024        ['conv2d_995[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_291 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_873[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_997 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_207[0][0]']      \n",
            "                                                                                                  \n",
            " conv2d_996 (Conv2D)            (None, 8, 8, 256)    262400      ['dropout_291[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_875 (Batch  (None, 8, 8, 256)   1024        ['conv2d_997[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_874 (Batch  (None, 8, 8, 256)   1024        ['conv2d_996[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_transpose_89 (Conv2DTra  (None, 16, 16, 128)  131200     ['batch_normalization_875[0][0]']\n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " add_291 (Add)                  (None, 8, 8, 256)    0           ['batch_normalization_874[0][0]',\n",
            "                                                                  'batch_normalization_875[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_998 (Conv2D)            (None, 16, 16, 32)   4128        ['batch_normalization_872[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_999 (Conv2D)            (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1000 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1001 (Conv2D)           (None, 16, 16, 32)   4128        ['batch_normalization_872[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1002 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1003 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1004 (Conv2D)           (None, 16, 16, 32)   4128        ['batch_normalization_872[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1005 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1006 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1007 (Conv2D)           (None, 16, 16, 32)   4128        ['batch_normalization_872[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1008 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1009 (Conv2D)           (None, 16, 16, 32)   4128        ['conv2d_transpose_89[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu_291 (ReLU)               (None, 8, 8, 256)    0           ['add_291[0][0]']                \n",
            "                                                                                                  \n",
            " attention_125 (Attention)      (None, 16, 16, 32)   0           ['conv2d_998[0][0]',             \n",
            "                                                                  'conv2d_999[0][0]',             \n",
            "                                                                  'conv2d_1000[0][0]']            \n",
            "                                                                                                  \n",
            " attention_126 (Attention)      (None, 16, 16, 32)   0           ['conv2d_1001[0][0]',            \n",
            "                                                                  'conv2d_1002[0][0]',            \n",
            "                                                                  'conv2d_1003[0][0]']            \n",
            "                                                                                                  \n",
            " attention_127 (Attention)      (None, 16, 16, 32)   0           ['conv2d_1004[0][0]',            \n",
            "                                                                  'conv2d_1005[0][0]',            \n",
            "                                                                  'conv2d_1006[0][0]']            \n",
            "                                                                                                  \n",
            " attention_128 (Attention)      (None, 16, 16, 32)   0           ['conv2d_1007[0][0]',            \n",
            "                                                                  'conv2d_1008[0][0]',            \n",
            "                                                                  'conv2d_1009[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_90 (Conv2DTra  (None, 16, 16, 128)  131200     ['re_lu_291[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 16, 16, 128)  0           ['attention_125[0][0]',          \n",
            "                                                                  'attention_126[0][0]',          \n",
            "                                                                  'attention_127[0][0]',          \n",
            "                                                                  'attention_128[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_90[0][0]',    \n",
            "                                                                  'concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1010 (Conv2D)           (None, 16, 16, 128)  131200      ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_876 (Batch  (None, 16, 16, 128)  512        ['conv2d_1010[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_292 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_876[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1012 (Conv2D)           (None, 16, 16, 128)  131200      ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1011 (Conv2D)           (None, 16, 16, 128)  65664       ['dropout_292[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_878 (Batch  (None, 16, 16, 128)  512        ['conv2d_1012[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_877 (Batch  (None, 16, 16, 128)  512        ['conv2d_1011[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_transpose_91 (Conv2DTra  (None, 32, 32, 64)  32832       ['batch_normalization_878[0][0]']\n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " add_292 (Add)                  (None, 16, 16, 128)  0           ['batch_normalization_877[0][0]',\n",
            "                                                                  'batch_normalization_878[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1013 (Conv2D)           (None, 32, 32, 16)   1040        ['batch_normalization_869[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1014 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1015 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1016 (Conv2D)           (None, 32, 32, 16)   1040        ['batch_normalization_869[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1017 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1018 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1019 (Conv2D)           (None, 32, 32, 16)   1040        ['batch_normalization_869[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1020 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1021 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1022 (Conv2D)           (None, 32, 32, 16)   1040        ['batch_normalization_869[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1023 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1024 (Conv2D)           (None, 32, 32, 16)   1040        ['conv2d_transpose_91[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu_292 (ReLU)               (None, 16, 16, 128)  0           ['add_292[0][0]']                \n",
            "                                                                                                  \n",
            " attention_129 (Attention)      (None, 32, 32, 16)   0           ['conv2d_1013[0][0]',            \n",
            "                                                                  'conv2d_1014[0][0]',            \n",
            "                                                                  'conv2d_1015[0][0]']            \n",
            "                                                                                                  \n",
            " attention_130 (Attention)      (None, 32, 32, 16)   0           ['conv2d_1016[0][0]',            \n",
            "                                                                  'conv2d_1017[0][0]',            \n",
            "                                                                  'conv2d_1018[0][0]']            \n",
            "                                                                                                  \n",
            " attention_131 (Attention)      (None, 32, 32, 16)   0           ['conv2d_1019[0][0]',            \n",
            "                                                                  'conv2d_1020[0][0]',            \n",
            "                                                                  'conv2d_1021[0][0]']            \n",
            "                                                                                                  \n",
            " attention_132 (Attention)      (None, 32, 32, 16)   0           ['conv2d_1022[0][0]',            \n",
            "                                                                  'conv2d_1023[0][0]',            \n",
            "                                                                  'conv2d_1024[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_92 (Conv2DTra  (None, 32, 32, 64)  32832       ['re_lu_292[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 32, 32, 64)   0           ['attention_129[0][0]',          \n",
            "                                                                  'attention_130[0][0]',          \n",
            "                                                                  'attention_131[0][0]',          \n",
            "                                                                  'attention_132[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_92[0][0]',    \n",
            "                                                                  'concatenate_42[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1025 (Conv2D)           (None, 32, 32, 64)   32832       ['concatenate_43[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_879 (Batch  (None, 32, 32, 64)  256         ['conv2d_1025[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_293 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_879[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1027 (Conv2D)           (None, 32, 32, 64)   32832       ['concatenate_43[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1026 (Conv2D)           (None, 32, 32, 64)   16448       ['dropout_293[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_881 (Batch  (None, 32, 32, 64)  256         ['conv2d_1027[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_880 (Batch  (None, 32, 32, 64)  256         ['conv2d_1026[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_transpose_93 (Conv2DTra  (None, 64, 64, 32)  8224        ['batch_normalization_881[0][0]']\n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " add_293 (Add)                  (None, 32, 32, 64)   0           ['batch_normalization_880[0][0]',\n",
            "                                                                  'batch_normalization_881[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1028 (Conv2D)           (None, 64, 64, 8)    264         ['batch_normalization_866[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1029 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1030 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1031 (Conv2D)           (None, 64, 64, 8)    264         ['batch_normalization_866[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1032 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1033 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1034 (Conv2D)           (None, 64, 64, 8)    264         ['batch_normalization_866[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1035 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1036 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1037 (Conv2D)           (None, 64, 64, 8)    264         ['batch_normalization_866[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1038 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1039 (Conv2D)           (None, 64, 64, 8)    264         ['conv2d_transpose_93[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu_293 (ReLU)               (None, 32, 32, 64)   0           ['add_293[0][0]']                \n",
            "                                                                                                  \n",
            " attention_133 (Attention)      (None, 64, 64, 8)    0           ['conv2d_1028[0][0]',            \n",
            "                                                                  'conv2d_1029[0][0]',            \n",
            "                                                                  'conv2d_1030[0][0]']            \n",
            "                                                                                                  \n",
            " attention_134 (Attention)      (None, 64, 64, 8)    0           ['conv2d_1031[0][0]',            \n",
            "                                                                  'conv2d_1032[0][0]',            \n",
            "                                                                  'conv2d_1033[0][0]']            \n",
            "                                                                                                  \n",
            " attention_135 (Attention)      (None, 64, 64, 8)    0           ['conv2d_1034[0][0]',            \n",
            "                                                                  'conv2d_1035[0][0]',            \n",
            "                                                                  'conv2d_1036[0][0]']            \n",
            "                                                                                                  \n",
            " attention_136 (Attention)      (None, 64, 64, 8)    0           ['conv2d_1037[0][0]',            \n",
            "                                                                  'conv2d_1038[0][0]',            \n",
            "                                                                  'conv2d_1039[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_94 (Conv2DTra  (None, 64, 64, 32)  8224        ['re_lu_293[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 64, 64, 32)   0           ['attention_133[0][0]',          \n",
            "                                                                  'attention_134[0][0]',          \n",
            "                                                                  'attention_135[0][0]',          \n",
            "                                                                  'attention_136[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_94[0][0]',    \n",
            "                                                                  'concatenate_44[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1040 (Conv2D)           (None, 64, 64, 32)   8224        ['concatenate_45[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_882 (Batch  (None, 64, 64, 32)  128         ['conv2d_1040[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_294 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_882[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1042 (Conv2D)           (None, 64, 64, 32)   8224        ['concatenate_45[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1041 (Conv2D)           (None, 64, 64, 32)   4128        ['dropout_294[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_884 (Batch  (None, 64, 64, 32)  128         ['conv2d_1042[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " batch_normalization_883 (Batch  (None, 64, 64, 32)  128         ['conv2d_1041[0][0]']            \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_transpose_95 (Conv2DTra  (None, 128, 128, 16  2064       ['batch_normalization_884[0][0]']\n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " add_294 (Add)                  (None, 64, 64, 32)   0           ['batch_normalization_883[0][0]',\n",
            "                                                                  'batch_normalization_884[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1043 (Conv2D)           (None, 128, 128, 4)  68          ['batch_normalization_863[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1044 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1045 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1046 (Conv2D)           (None, 128, 128, 4)  68          ['batch_normalization_863[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1047 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1048 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1049 (Conv2D)           (None, 128, 128, 4)  68          ['batch_normalization_863[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1050 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1051 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1052 (Conv2D)           (None, 128, 128, 4)  68          ['batch_normalization_863[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_1053 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1054 (Conv2D)           (None, 128, 128, 4)  68          ['conv2d_transpose_95[0][0]']    \n",
            "                                                                                                  \n",
            " re_lu_294 (ReLU)               (None, 64, 64, 32)   0           ['add_294[0][0]']                \n",
            "                                                                                                  \n",
            " attention_137 (Attention)      (None, 128, 128, 4)  0           ['conv2d_1043[0][0]',            \n",
            "                                                                  'conv2d_1044[0][0]',            \n",
            "                                                                  'conv2d_1045[0][0]']            \n",
            "                                                                                                  \n",
            " attention_138 (Attention)      (None, 128, 128, 4)  0           ['conv2d_1046[0][0]',            \n",
            "                                                                  'conv2d_1047[0][0]',            \n",
            "                                                                  'conv2d_1048[0][0]']            \n",
            "                                                                                                  \n",
            " attention_139 (Attention)      (None, 128, 128, 4)  0           ['conv2d_1049[0][0]',            \n",
            "                                                                  'conv2d_1050[0][0]',            \n",
            "                                                                  'conv2d_1051[0][0]']            \n",
            "                                                                                                  \n",
            " attention_140 (Attention)      (None, 128, 128, 4)  0           ['conv2d_1052[0][0]',            \n",
            "                                                                  'conv2d_1053[0][0]',            \n",
            "                                                                  'conv2d_1054[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_96 (Conv2DTra  (None, 128, 128, 16  2064       ['re_lu_294[0][0]']              \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 128, 128, 16  0           ['attention_137[0][0]',          \n",
            "                                )                                 'attention_138[0][0]',          \n",
            "                                                                  'attention_139[0][0]',          \n",
            "                                                                  'attention_140[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_96[0][0]',    \n",
            "                                )                                 'concatenate_46[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_1055 (Conv2D)           (None, 128, 128, 16  2064        ['concatenate_47[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_885 (Batch  (None, 128, 128, 16  64         ['conv2d_1055[0][0]']            \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_295 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_885[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1056 (Conv2D)           (None, 128, 128, 16  1040        ['dropout_295[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1057 (Conv2D)           (None, 128, 128, 16  2064        ['concatenate_47[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_886 (Batch  (None, 128, 128, 16  64         ['conv2d_1056[0][0]']            \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_887 (Batch  (None, 128, 128, 16  64         ['conv2d_1057[0][0]']            \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_295 (Add)                  (None, 128, 128, 16  0           ['batch_normalization_886[0][0]',\n",
            "                                )                                 'batch_normalization_887[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_295 (ReLU)               (None, 128, 128, 16  0           ['add_295[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_1058 (Conv2D)           (None, 128, 128, 1)  17          ['re_lu_295[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,558,369\n",
            "Trainable params: 1,553,953\n",
            "Non-trainable params: 4,416\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "nheads=8\n",
        "\n",
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "#s= inputs\n",
        "#Contraction path\n",
        "c1,z1 = conv_block(s,[16,16])\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "c2,z2 = conv_block(p1,[32,32])\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "#Expansive path \n",
        "m1=attentionblock(z4,z5,nheads)\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "m2=attentionblock(z3,z6,nheads)\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "m3=attentionblock(z2,z7,nheads)\n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "m4=attentionblock(z1,z8,nheads) \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "c9,_ = conv_block(u9,[16,16])\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFyLMRezeWO"
      },
      "source": [
        "##Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCB8atJ1zda4",
        "outputId": "389eb85f-04f6-4429-dfac-a21db0e57591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "99/99 [==============================] - 25s 162ms/step - loss: 0.1123 - accuracy: 0.9823 - DiceMetric: 0.1085 - val_loss: 0.0627 - val_accuracy: 0.9855 - val_DiceMetric: 0.0516\n",
            "Epoch 2/25\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0447 - accuracy: 0.9871 - DiceMetric: 0.2500 - val_loss: 0.0571 - val_accuracy: 0.9855 - val_DiceMetric: 0.0416\n",
            "Epoch 3/25\n",
            "99/99 [==============================] - 16s 167ms/step - loss: 0.0276 - accuracy: 0.9879 - DiceMetric: 0.5621 - val_loss: 0.0406 - val_accuracy: 0.9855 - val_DiceMetric: 0.2671\n",
            "Epoch 4/25\n",
            "99/99 [==============================] - 15s 157ms/step - loss: 0.0185 - accuracy: 0.9886 - DiceMetric: 0.7492 - val_loss: 0.0352 - val_accuracy: 0.9795 - val_DiceMetric: 0.5987\n",
            "Epoch 5/25\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0149 - accuracy: 0.9888 - DiceMetric: 0.8025 - val_loss: 0.0267 - val_accuracy: 0.9866 - val_DiceMetric: 0.4211\n",
            "Epoch 6/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0132 - accuracy: 0.9889 - DiceMetric: 0.8224 - val_loss: 0.0178 - val_accuracy: 0.9860 - val_DiceMetric: 0.7937\n",
            "Epoch 7/25\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0116 - accuracy: 0.9889 - DiceMetric: 0.8431 - val_loss: 0.0142 - val_accuracy: 0.9871 - val_DiceMetric: 0.8251\n",
            "Epoch 8/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0108 - accuracy: 0.9890 - DiceMetric: 0.8546 - val_loss: 0.0126 - val_accuracy: 0.9871 - val_DiceMetric: 0.8483\n",
            "Epoch 9/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0101 - accuracy: 0.9890 - DiceMetric: 0.8621 - val_loss: 0.0124 - val_accuracy: 0.9875 - val_DiceMetric: 0.8406\n",
            "Epoch 10/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0094 - accuracy: 0.9891 - DiceMetric: 0.8741 - val_loss: 0.0129 - val_accuracy: 0.9867 - val_DiceMetric: 0.8456\n",
            "Epoch 11/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0090 - accuracy: 0.9891 - DiceMetric: 0.8829 - val_loss: 0.0124 - val_accuracy: 0.9870 - val_DiceMetric: 0.8478\n",
            "Epoch 12/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0089 - accuracy: 0.9891 - DiceMetric: 0.8829 - val_loss: 0.0106 - val_accuracy: 0.9876 - val_DiceMetric: 0.8699\n",
            "Epoch 13/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0085 - accuracy: 0.9892 - DiceMetric: 0.8876 - val_loss: 0.0114 - val_accuracy: 0.9872 - val_DiceMetric: 0.8562\n",
            "Epoch 14/25\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0082 - accuracy: 0.9892 - DiceMetric: 0.8943 - val_loss: 0.0103 - val_accuracy: 0.9875 - val_DiceMetric: 0.8742\n",
            "Epoch 15/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0082 - accuracy: 0.9892 - DiceMetric: 0.8913 - val_loss: 0.0122 - val_accuracy: 0.9870 - val_DiceMetric: 0.8537\n",
            "Epoch 16/25\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0078 - accuracy: 0.9893 - DiceMetric: 0.9018 - val_loss: 0.0115 - val_accuracy: 0.9869 - val_DiceMetric: 0.8589\n",
            "Epoch 17/25\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.9015 - val_loss: 0.0120 - val_accuracy: 0.9869 - val_DiceMetric: 0.8541\n",
            "Epoch 18/25\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8996 - val_loss: 0.0097 - val_accuracy: 0.9879 - val_DiceMetric: 0.8772\n",
            "Epoch 19/25\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0075 - accuracy: 0.9893 - DiceMetric: 0.9059 - val_loss: 0.0098 - val_accuracy: 0.9876 - val_DiceMetric: 0.8788\n",
            "Epoch 20/25\n",
            "99/99 [==============================] - 15s 151ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9115 - val_loss: 0.0103 - val_accuracy: 0.9873 - val_DiceMetric: 0.8781\n",
            "Epoch 21/25\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0072 - accuracy: 0.9893 - DiceMetric: 0.9085 - val_loss: 0.0099 - val_accuracy: 0.9876 - val_DiceMetric: 0.8781\n",
            "Epoch 22/25\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9127 - val_loss: 0.0103 - val_accuracy: 0.9873 - val_DiceMetric: 0.8766\n",
            "Epoch 23/25\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0070 - accuracy: 0.9893 - DiceMetric: 0.9143 - val_loss: 0.0093 - val_accuracy: 0.9879 - val_DiceMetric: 0.8837\n",
            "Epoch 24/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0070 - accuracy: 0.9894 - DiceMetric: 0.9134 - val_loss: 0.0091 - val_accuracy: 0.9877 - val_DiceMetric: 0.8926\n",
            "Epoch 25/25\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0067 - accuracy: 0.9894 - DiceMetric: 0.9168 - val_loss: 0.0090 - val_accuracy: 0.9879 - val_DiceMetric: 0.8907\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe16e6d8b10>"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "model.fit(X_train,Y_train,batch_size=20,epochs=25,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train,batch_size=20,epochs=200,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnZHfzdPIbNo",
        "outputId": "692f670c-3c77-4337-f1f5-b390323747e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9164 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.8965\n",
            "Epoch 2/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0067 - accuracy: 0.9894 - DiceMetric: 0.9187 - val_loss: 0.0089 - val_accuracy: 0.9879 - val_DiceMetric: 0.8928\n",
            "Epoch 3/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9191 - val_loss: 0.0090 - val_accuracy: 0.9880 - val_DiceMetric: 0.8907\n",
            "Epoch 4/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9207 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.8837\n",
            "Epoch 5/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9215 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.8998\n",
            "Epoch 6/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0063 - accuracy: 0.9894 - DiceMetric: 0.9250 - val_loss: 0.0098 - val_accuracy: 0.9875 - val_DiceMetric: 0.8847\n",
            "Epoch 7/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0062 - accuracy: 0.9894 - DiceMetric: 0.9271 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.8910\n",
            "Epoch 8/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9233 - val_loss: 0.0094 - val_accuracy: 0.9876 - val_DiceMetric: 0.8900\n",
            "Epoch 9/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9238 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9045\n",
            "Epoch 10/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0062 - accuracy: 0.9895 - DiceMetric: 0.9289 - val_loss: 0.0097 - val_accuracy: 0.9875 - val_DiceMetric: 0.8880\n",
            "Epoch 11/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9223 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.8913\n",
            "Epoch 12/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9242 - val_loss: 0.0087 - val_accuracy: 0.9878 - val_DiceMetric: 0.8967\n",
            "Epoch 13/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9313 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9034\n",
            "Epoch 14/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9296 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9057\n",
            "Epoch 15/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9316 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9030\n",
            "Epoch 16/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9326 - val_loss: 0.0087 - val_accuracy: 0.9879 - val_DiceMetric: 0.9026\n",
            "Epoch 17/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9340 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.8996\n",
            "Epoch 18/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9347 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.8983\n",
            "Epoch 19/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9299 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9052\n",
            "Epoch 20/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9305 - val_loss: 0.0088 - val_accuracy: 0.9878 - val_DiceMetric: 0.9023\n",
            "Epoch 21/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9340 - val_loss: 0.0082 - val_accuracy: 0.9880 - val_DiceMetric: 0.9107\n",
            "Epoch 22/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9333 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9006\n",
            "Epoch 23/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9302 - val_loss: 0.0085 - val_accuracy: 0.9879 - val_DiceMetric: 0.9005\n",
            "Epoch 24/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9332 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9046\n",
            "Epoch 25/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9363 - val_loss: 0.0081 - val_accuracy: 0.9881 - val_DiceMetric: 0.9111\n",
            "Epoch 26/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9383 - val_loss: 0.0087 - val_accuracy: 0.9878 - val_DiceMetric: 0.8991\n",
            "Epoch 27/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9394 - val_loss: 0.0085 - val_accuracy: 0.9879 - val_DiceMetric: 0.9062\n",
            "Epoch 28/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9391 - val_loss: 0.0085 - val_accuracy: 0.9880 - val_DiceMetric: 0.9023\n",
            "Epoch 29/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9398 - val_loss: 0.0086 - val_accuracy: 0.9880 - val_DiceMetric: 0.9060\n",
            "Epoch 30/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9403 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9061\n",
            "Epoch 31/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9393 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9017\n",
            "Epoch 32/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9399 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.8996\n",
            "Epoch 33/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9396 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9082\n",
            "Epoch 34/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9431 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9064\n",
            "Epoch 35/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9433 - val_loss: 0.0085 - val_accuracy: 0.9879 - val_DiceMetric: 0.9061\n",
            "Epoch 36/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9450 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9088\n",
            "Epoch 37/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9447 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9111\n",
            "Epoch 38/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9439 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9087\n",
            "Epoch 39/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9446 - val_loss: 0.0086 - val_accuracy: 0.9878 - val_DiceMetric: 0.9060\n",
            "Epoch 40/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9458 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9090\n",
            "Epoch 41/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9448 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9094\n",
            "Epoch 42/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9455 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9109\n",
            "Epoch 43/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9481 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9141\n",
            "Epoch 44/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9493 - val_loss: 0.0081 - val_accuracy: 0.9881 - val_DiceMetric: 0.9141\n",
            "Epoch 45/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9485 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9109\n",
            "Epoch 46/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9497 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9087\n",
            "Epoch 47/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9505 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9138\n",
            "Epoch 48/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9473 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9007\n",
            "Epoch 49/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9474 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9113\n",
            "Epoch 50/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9479 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9134\n",
            "Epoch 51/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9474 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9056\n",
            "Epoch 52/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9495 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9109\n",
            "Epoch 53/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9483 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9050\n",
            "Epoch 54/200\n",
            "99/99 [==============================] - 15s 154ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9491 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9065\n",
            "Epoch 55/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9486 - val_loss: 0.0086 - val_accuracy: 0.9879 - val_DiceMetric: 0.9069\n",
            "Epoch 56/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9498 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9126\n",
            "Epoch 57/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9518 - val_loss: 0.0086 - val_accuracy: 0.9880 - val_DiceMetric: 0.9079\n",
            "Epoch 58/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9507 - val_loss: 0.0089 - val_accuracy: 0.9879 - val_DiceMetric: 0.9035\n",
            "Epoch 59/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9503 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9125\n",
            "Epoch 60/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9539 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9128\n",
            "Epoch 61/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0048 - accuracy: 0.9896 - DiceMetric: 0.9549 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9138\n",
            "Epoch 62/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9522 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9125\n",
            "Epoch 63/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9540 - val_loss: 0.0086 - val_accuracy: 0.9880 - val_DiceMetric: 0.9092\n",
            "Epoch 64/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0048 - accuracy: 0.9896 - DiceMetric: 0.9550 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9127\n",
            "Epoch 65/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0048 - accuracy: 0.9896 - DiceMetric: 0.9549 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9137\n",
            "Epoch 66/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0048 - accuracy: 0.9896 - DiceMetric: 0.9561 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9134\n",
            "Epoch 67/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9570 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9158\n",
            "Epoch 68/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9578 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9140\n",
            "Epoch 69/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9565 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.9139\n",
            "Epoch 70/200\n",
            "99/99 [==============================] - 14s 144ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9596 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9131\n",
            "Epoch 71/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9587 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9123\n",
            "Epoch 72/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9584 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.9129\n",
            "Epoch 73/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9599 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9160\n",
            "Epoch 74/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9603 - val_loss: 0.0086 - val_accuracy: 0.9880 - val_DiceMetric: 0.9133\n",
            "Epoch 75/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9588 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9080\n",
            "Epoch 76/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9599 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9140\n",
            "Epoch 77/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9595 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9167\n",
            "Epoch 78/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9614 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9140\n",
            "Epoch 79/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9613 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9148\n",
            "Epoch 80/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9597 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9076\n",
            "Epoch 81/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9596 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.9138\n",
            "Epoch 82/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9610 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.9118\n",
            "Epoch 83/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9602 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.9122\n",
            "Epoch 84/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9600 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9124\n",
            "Epoch 85/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9623 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9142\n",
            "Epoch 86/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9620 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9145\n",
            "Epoch 87/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9629 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9153\n",
            "Epoch 88/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9644 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9144\n",
            "Epoch 89/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9625 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9104\n",
            "Epoch 90/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9607 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9138\n",
            "Epoch 91/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9624 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9142\n",
            "Epoch 92/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9639 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9161\n",
            "Epoch 93/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9624 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9102\n",
            "Epoch 94/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9637 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9125\n",
            "Epoch 95/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9640 - val_loss: 0.0085 - val_accuracy: 0.9881 - val_DiceMetric: 0.9127\n",
            "Epoch 96/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9642 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.9132\n",
            "Epoch 97/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9617 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9144\n",
            "Epoch 98/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9645 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9147\n",
            "Epoch 99/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9646 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9118\n",
            "Epoch 100/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9632 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.9137\n",
            "Epoch 101/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9659 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9139\n",
            "Epoch 102/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9671 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9109\n",
            "Epoch 103/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9666 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9160\n",
            "Epoch 104/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9660 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.9171\n",
            "Epoch 105/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9676 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9145\n",
            "Epoch 106/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9660 - val_loss: 0.0094 - val_accuracy: 0.9880 - val_DiceMetric: 0.9102\n",
            "Epoch 107/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9669 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9140\n",
            "Epoch 108/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9681 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9153\n",
            "Epoch 109/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9686 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9155\n",
            "Epoch 110/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9685 - val_loss: 0.0090 - val_accuracy: 0.9880 - val_DiceMetric: 0.9130\n",
            "Epoch 111/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9677 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9173\n",
            "Epoch 112/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9677 - val_loss: 0.0090 - val_accuracy: 0.9880 - val_DiceMetric: 0.9129\n",
            "Epoch 113/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9690 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9134\n",
            "Epoch 114/200\n",
            "99/99 [==============================] - 14s 145ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9677 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9122\n",
            "Epoch 115/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9655 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9150\n",
            "Epoch 116/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9682 - val_loss: 0.0086 - val_accuracy: 0.9880 - val_DiceMetric: 0.9167\n",
            "Epoch 117/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9691 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9159\n",
            "Epoch 118/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9712 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9157\n",
            "Epoch 119/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9694 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9150\n",
            "Epoch 120/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9697 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9143\n",
            "Epoch 121/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9694 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9148\n",
            "Epoch 122/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9711 - val_loss: 0.0088 - val_accuracy: 0.9880 - val_DiceMetric: 0.9156\n",
            "Epoch 123/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0041 - accuracy: 0.9897 - DiceMetric: 0.9711 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.9162\n",
            "Epoch 124/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9709 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9159\n",
            "Epoch 125/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9712 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.9144\n",
            "Epoch 126/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9717 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9171\n",
            "Epoch 127/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9701 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.9150\n",
            "Epoch 128/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9704 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9136\n",
            "Epoch 129/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9701 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9162\n",
            "Epoch 130/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9707 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9147\n",
            "Epoch 131/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9684 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9099\n",
            "Epoch 132/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9625 - val_loss: 0.0091 - val_accuracy: 0.9880 - val_DiceMetric: 0.9107\n",
            "Epoch 133/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9694 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9179\n",
            "Epoch 134/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9707 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9177\n",
            "Epoch 135/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9724 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9176\n",
            "Epoch 136/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9718 - val_loss: 0.0088 - val_accuracy: 0.9881 - val_DiceMetric: 0.9169\n",
            "Epoch 137/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9724 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9170\n",
            "Epoch 138/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9724 - val_loss: 0.0087 - val_accuracy: 0.9881 - val_DiceMetric: 0.9172\n",
            "Epoch 139/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9719 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9145\n",
            "Epoch 140/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9731 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9119\n",
            "Epoch 141/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9718 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9163\n",
            "Epoch 142/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9716 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9167\n",
            "Epoch 143/200\n",
            "99/99 [==============================] - 14s 146ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9718 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9137\n",
            "Epoch 144/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9712 - val_loss: 0.0090 - val_accuracy: 0.9880 - val_DiceMetric: 0.9148\n",
            "Epoch 145/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9705 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9135\n",
            "Epoch 146/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9718 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9170\n",
            "Epoch 147/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9731 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9143\n",
            "Epoch 148/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9731 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9176\n",
            "Epoch 149/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9743 - val_loss: 0.0091 - val_accuracy: 0.9880 - val_DiceMetric: 0.9158\n",
            "Epoch 150/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.9152\n",
            "Epoch 151/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9146\n",
            "Epoch 152/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9744 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9138\n",
            "Epoch 153/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9754 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9146\n",
            "Epoch 154/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9744 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.9156\n",
            "Epoch 155/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9152\n",
            "Epoch 156/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9720 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9157\n",
            "Epoch 157/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9716 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9119\n",
            "Epoch 158/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9744 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9141\n",
            "Epoch 159/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9742 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9151\n",
            "Epoch 160/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9729 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9144\n",
            "Epoch 161/200\n",
            "99/99 [==============================] - 15s 150ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9740 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.9157\n",
            "Epoch 162/200\n",
            "99/99 [==============================] - 15s 150ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9728 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9144\n",
            "Epoch 163/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9171\n",
            "Epoch 164/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9742 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9149\n",
            "Epoch 165/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9744 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.9154\n",
            "Epoch 166/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.9153\n",
            "Epoch 167/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9729 - val_loss: 0.0089 - val_accuracy: 0.9881 - val_DiceMetric: 0.9130\n",
            "Epoch 168/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9710 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.9124\n",
            "Epoch 169/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9715 - val_loss: 0.0097 - val_accuracy: 0.9882 - val_DiceMetric: 0.9129\n",
            "Epoch 170/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9733 - val_loss: 0.0094 - val_accuracy: 0.9881 - val_DiceMetric: 0.9152\n",
            "Epoch 171/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9751 - val_loss: 0.0094 - val_accuracy: 0.9881 - val_DiceMetric: 0.9147\n",
            "Epoch 172/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9752 - val_loss: 0.0097 - val_accuracy: 0.9881 - val_DiceMetric: 0.9113\n",
            "Epoch 173/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9740 - val_loss: 0.0090 - val_accuracy: 0.9880 - val_DiceMetric: 0.9139\n",
            "Epoch 174/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9755 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9152\n",
            "Epoch 175/200\n",
            "99/99 [==============================] - 15s 150ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0094 - val_accuracy: 0.9882 - val_DiceMetric: 0.9144\n",
            "Epoch 176/200\n",
            "99/99 [==============================] - 15s 151ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9761 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9186\n",
            "Epoch 177/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9768 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9159\n",
            "Epoch 178/200\n",
            "99/99 [==============================] - 15s 150ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9757 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.9154\n",
            "Epoch 179/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9775 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9160\n",
            "Epoch 180/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9777 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9153\n",
            "Epoch 181/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9773 - val_loss: 0.0095 - val_accuracy: 0.9881 - val_DiceMetric: 0.9150\n",
            "Epoch 182/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9782 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9169\n",
            "Epoch 183/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9783 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.9166\n",
            "Epoch 184/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9794 - val_loss: 0.0094 - val_accuracy: 0.9882 - val_DiceMetric: 0.9169\n",
            "Epoch 185/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9790 - val_loss: 0.0094 - val_accuracy: 0.9881 - val_DiceMetric: 0.9158\n",
            "Epoch 186/200\n",
            "99/99 [==============================] - 15s 150ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9788 - val_loss: 0.0098 - val_accuracy: 0.9881 - val_DiceMetric: 0.9133\n",
            "Epoch 187/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9759 - val_loss: 0.0094 - val_accuracy: 0.9881 - val_DiceMetric: 0.9143\n",
            "Epoch 188/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9772 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9164\n",
            "Epoch 189/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9781 - val_loss: 0.0095 - val_accuracy: 0.9882 - val_DiceMetric: 0.9168\n",
            "Epoch 190/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9770 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9166\n",
            "Epoch 191/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9786 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9158\n",
            "Epoch 192/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9790 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9169\n",
            "Epoch 193/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9792 - val_loss: 0.0095 - val_accuracy: 0.9881 - val_DiceMetric: 0.9161\n",
            "Epoch 194/200\n",
            "99/99 [==============================] - 15s 147ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9783 - val_loss: 0.0093 - val_accuracy: 0.9880 - val_DiceMetric: 0.9131\n",
            "Epoch 195/200\n",
            "99/99 [==============================] - 15s 149ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9776 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9148\n",
            "Epoch 196/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9789 - val_loss: 0.0094 - val_accuracy: 0.9881 - val_DiceMetric: 0.9169\n",
            "Epoch 197/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9792 - val_loss: 0.0095 - val_accuracy: 0.9882 - val_DiceMetric: 0.9141\n",
            "Epoch 198/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9773 - val_loss: 0.0095 - val_accuracy: 0.9881 - val_DiceMetric: 0.9158\n",
            "Epoch 199/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9782 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.9165\n",
            "Epoch 200/200\n",
            "99/99 [==============================] - 15s 148ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9779 - val_loss: 0.0095 - val_accuracy: 0.9881 - val_DiceMetric: 0.9124\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe1cf364990>"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0yz227RzmyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ca7e95-b653-4346-aaf9-e60b519ae9e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 0: loss of 0.0077279419638216496; accuracy of 98.95653128623962% DiceMetric of 89.44644331932068%\n",
            "['loss', 'accuracy', 'DiceMetric']\n"
          ]
        }
      ],
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.metrics_names)"
      ],
      "metadata": {
        "id": "oud92q0zgO7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFP-3gQ9BO8f"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5P0-RG0wjzA6"
      },
      "outputs": [],
      "source": [
        "Ypred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ra2EoVycvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "a36eddd0-b297-4016-a48f-25ca2285a71c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD7CAYAAABqkiE2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfNUlEQVR4nO3da3Bb6X3f8e8fAAFeQBIC7yIliqKo6C6tVnuXMl7baWTXs+t67MymmkZundlpJ804ccfJbv2i7bumzaRxZly7mviy9tjruIrrXXvcajeb9XVWWl1irlYUJVIixYtI8QoSBAkQl6cvcA4W0lIriSBAkOf/meEQOLich4c4PzznOc95HjHGoJRyLtdqF0Aptbo0BJRyOA0BpRxOQ0Aph9MQUMrhNASUcri8hYCIHBWRKyLSKyIv5Gs9SqncSD76CYiIG7gK/A4wBJwFft8Y07XiK1NK5cSTp/d9FOg1xlwHEJHvA88CS4aAiGiPJaXyb8IYU3fnwnwdDjQDg1n3h6xlGSLyvIicE5FzeSqDUup2N5ZamK+awD0ZY04AJ0BrAkqtpnzVBIaBTVn3W6xlSqkik68QOAt0iEibiHiB54BX87QupVQO8nI4YIxJiMi/B04BbuAbxphL+ViXUio3eTlF+MCF0DYBpQrhvDHm0J0LtcegUg6nIaCUw2kIKOVwGgJKOZyGgFIOpyGglMNpCCjlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg6nIaCUw2kIKOVwGgJKOZyGgFIOpyGglMNpCCjlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg637BAQkU0i8qaIdInIJRH5vLU8KCKvi0iP9XvDyhVXKbXScqkJJID/YIzZBTwO/JGI7AJeAN4wxnQAb1j3lVJFatkhYIwZMcZcsG6HgctAM/As8JL1tJeAT+ZaSKVU/qzIrMQisgV4CDgDNBhjRqyHRoGGu7zmeeD5lVi/Umr5cm4YFBE/8PfAnxhjZrMfM+kpj5eccdgYc8IYc2ipWVKVUoWTUwiISAnpAPiuMeaH1uJbItJkPd4EjOVWRKVUPuVydkCArwOXjTF/lfXQq8Bx6/Zx4JXlF08plW+SrrEv44Uih4FfAheBlLX4P5JuF/gBsBm4AfyeMWbqHu+1vEIopR7E+aUOv5cdAitJQ0CpglgyBLTHoFIOpyGglMNpCCjlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg6nIaCUw2kIKOVwGgJKOZyGgFIOpyGglMNpCCjlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg6nIaCUw2kIKOVwGgJKOZyGgFIOtxKzErtF5J9E5CfW/TYROSMivSLydyLizb2YSql8WYmawOeBy1n3/wL4H8aYbcA08LkVWIdSKk9ynZq8BfjnwN9a9wX4MHDSespLwCdzWYdSKr9yrQn8NfBnvDcrcQ0QMsYkrPtDQPNSLxSR50XknIicy7EMSqkcLDsEROQTwJgx5vxyXm+MOWGMObTULKlKqcLx5PDap4BnROTjQClQBXwZCIiIx6oNtADDuRdTKZUvy64JGGNeNMa0GGO2AM8B/2iMOQa8CXzaetpx4JWcS6mUypt89BP4c+ALItJLuo3g63lYh1JqhYgxZrXLgIisfiGUWv/OL9UGpz0GlXI4DQGlHE5DQCmH0xBQyuE0BJRyOA0BpRwulx6DShWMy+XC7XaTvkYN7FPbiUSCYjjNvZZpCKii53a78Xg8VFVVISKICKlUimQyyezsLMlkUoMgBxoCqmjZO/+hQ4dobm5m9+7dlJSU4HK5iMfjzM/Pc/r0aUZGRrh69SqpVOreb6reR0NAFS2Px0NZWRkPPfQQ+/fv5+mnn8bn8+FyuYjFYszNzWGM4dKlS1y7dg1Ag2AZNARU0dq0aRN79+7lmWeeYffu3QSDQVyudFu2MYZ4PM5zzz3H2bNn+c1vfkMoFCIcDq9yqdceDQFVlESEQCDA1q1baWpqora2lpKSksxjxhhKSkpobm5mZGSEhoYG4vF4pnag7p+eIlRFR0TweDy0tbXxoQ99iPr6ekpKSjKNgvZzRITq6mpaWlo4cuQIW7ZswePxZJ6j7o/WBFTRcrvdeL1eXC7Xkju2iOB2u6mtreXw4cMsLi4SjUbp6ekhEomsQonXJg0BVZTsb/p7fbO7XC5qa2v56Ec/SjweZ3FxkZGREQ2BB6CHA6oopVIp4vE44XD4nh2C3G435eXlbN26lUcffZTKykrcbncBS7u2aQioomOMwRhDJBJhcnKSaDT6gUFgHxZUVVVRX1+Pz+fTdoEHoIcDqiglk0m6urp4+eWXqa+vx+1209DQgNvtzpwmzGYHwZ0NiPmUvZ613D9BQ0AVrdnZWQYGBjh79izhcJhdu3ZRVVVFMBikrKwss8NDuvawuLjI/Px8Xq8nEBEqKiqoqKigrq4uE0i9vb1Eo9E1GQYaAqpoTU9PEwqF+Pa3v83GjRs5evQobW1tHDhwgObmZqqrq297fiQSYXx8nHg8nred0e1209jYSGtrK08++SQlJSUkk0m++c1vMjo6yuLiYl7Wm08aAqqoGWOYmJhgfn6eH/3oR9TW1vKLX/yCo0ePsmfPHlpbW/F4PMTjcfr7+zl//jzhcDgvNQGv10t1dTWf+cxn2LZtG/v372dhYYG5uTleeeUVpqenNQSUyoe5uTnm5uYYHx+noqKCK1eu0NjYiN/vzzQELiwsMDo6mqmW5yME3G43ZWVlPPzww3R0dLBz506mp6cZGxujtLQUj2dt7k5rs9RFxu7GqvLLGMP8/DyxWIyXX36ZX/3qV3zxi1+krKyMgYEBfvnLX9LZ2Zm3PgJ2m8PQ0BDl5eXU1dXR2dnJ5cuXGR8fZ2FhIS/rzTcNgWUSEfx+P6WlpVRWVjI5OUk4HF6TDUNrSSqVIpVKMT4+TiKR4Pz58/h8PoaGhhgYGCASieTtf2A3Pvb29rK4uEgikeDdd9+lp6eHubk5EonEvd+kCOnkI8vk9XrZtWsXmzZt4sCBA5w6dYrOzk4WFxe1VlAgLpcr0zgYjUaJx+N53xFdLhd1dXX4fD7Ky8uZmJhgZmZmrYxwtPKTj4hIQEROiki3iFwWkSdEJCgir4tIj/V7Qy7rKFYej4fW1lZ27drFE088QXt7O8FgUHuqFVAqlWJ+fp75+XkWFxdJJpN5X6cxhnA4nGkLiEQiayUA7irXHoNfBv6fMWYHsB+4DLwAvGGM6QDesO6vOx6Ph5aWFjo6OnjkkUfYsmWLhsAqiMVixGKxgg0xZrdLhMNhpqamWFhYWNMBADmEgIhUA7+NNeGoMWbRGBMCngVesp72EvDJXAtZjOLxONevX2dsbAyv10t9fT2bN2/OXPOu1h+7V2J5eTmlpaVL9lxci3JpGGwDxoFvish+4DzweaDBGDNiPWcUaFjqxSLyPPB8DutfValUiunpaWZmZojFYgQCAVpbW7lw4QKxWIx4PL7aRVQrwOVy4fF48Hq9+P1+fD4flZWVRKNRbt26lWmLWMtyCQEPcBD4Y2PMGRH5MndU/Y0x5m6NfsaYE8AJWJsNg4lEgv7+fpqbm+np6WHfvn3s3LmTrq4uent7uXnz5moXUeXI5XJRUVFBfX09bW1tPPbYY2zevJmOjg56enr4zne+w7Vr1xgZGbn3mxWxXEJgCBgyxpyx7p8kHQK3RKTJGDMiIk3AWK6FLEapVIpIJMLIyAgXLlzg4MGDNDY2cuDAAbxeb+YU1lo/XnSq6upqAoFAZqTjrVu30t7eTk1NDX6/n8HBwcx1CmvdskPAGDMqIoMi8lvGmCvAR4Au6+c48F+t36+sSEmLjN1KfOPGDX72s5/R3t7Ovn37ePrpp6mqquL06dMYY9bFh8Rp7NOA7e3tfPazn6W1tZW2tjZ8Ph8Ao6OjuFyuzGXOa12unYX+GPiuiHiB68C/Jt3Y+AMR+RxwA/i9HNdR1OLxODMzMywuLuJyudi/fz8A27Zt4+bNm4yNrcuK0LokItTV1dHY2MixY8fYuXMnBw8epKKiItMQGIvF6O7upqenh8nJSWKx2GoXO2c5hYAx5jfA+zofkK4VOEIikWBubo5oNEoymaSuro6NGzfS2NjI7OyshsAaICK4XC5KS0tpbGykvb2dQ4cOsWPHDhoaGjJjHBpjSCaTmXC3/+drnXYbzlEkEqGvr4/r16/T399Pe3s7dXV1PPXUU0SjUfr6+rRdoMiVlZVRW1vLk08+yZEjR3jqqafYsmULFRUVtw1ymkgkmJ+f56233uLdd99dN4d6GgI5SiaTzM/Pc+PGDS5dusTGjRsz4+EHAgFKSkqIx+MaBEXCHg3IviKwtLSUjo4ONm7cyBNPPMHOnTtpamqirKzstglQIR0CCwsLjI2NMT09vYp/xcrSEMhRIpEgHA7z1ltvMT09zcGDB6murmbXrl2cP3+e8vLyNX1xyXpi7/wej4fS0lJaWlpoamri2LFjtLe38/DDD1NSUpLp9ZkdAHZPwVAoRF9fH6Ojo6v1Z6w4DYEVMjo6iohw9epVtmzZQnNzM48++ijhcJg333yTsbGxddGIVCyqqqooLS297YrBhYUFFhYW8Pl8lJSU4Pf78Xq9+Hw+ampqqKysZNOmTfj9foLBII2NjQSDQXbt2pWptd1tjgOAiYkJBgcHCYfD6+KsgE1DYIWEQiGSySSDg4NUV1fT0dHBjh07WFhYoLu7m/n5+cxhgR4a5MaeoiwQCNwWAlNTU0A6IMrKyggGg/j9fvx+P5s2baK2tpZ9+/axYcMGGhoaqK2txe/3U1ZW9oE7v216eppbt24RiUTW5AhCd6MhsELsMfJ/+tOfcuvWLfbv309HRwebN28mGAzS3d3Nt771rcy4eRoEy+Pz+fD7/Rw7dowjR45QXl6euc7/2rVrDAwMcPDgQYLBIMFgkJKSEnw+X6Z2UFZWhsfjwePx4Ha7M8f9HxQA9hgGly5d4vTp00QikXVxVsCmIbBC7NNHQ0NDNDQ0MDMzkxkZd/v27ZSUlLB//34GBwfp6+sjHA6vq2+TQvH5fNTV1bFlyxa2b99OeXk5kB71p6ysjJqaGnbv3k1VVRVVVVWZHd7lct32bf8gQ5KnUikWFxcZHx9neHh43fUE1RBYQclkkqtXr+LxeOjs7GTHjh20t7ezZ88e2tvbaW9v58yZM7z66qucO3duzfc5Xw3BYJBDhw6xfft2WlpaMsOOG2NoaWkhlUpldvbl7PBLWVxcZGZmht7eXi5fvrzuwltDYIUtLi4yPT3N22+/TXl5OZs3b8bj8eDz+WhoaKCxsZGmpqZMF9QP4nK5tA0hS0lJSea4vqamJjNPof1jjLmvY/sHYYwhFArR29vLrVu3mJmZWXf/j/VxQXQRSSQSTE1Ncfr0afr7+4nFYqRSKTweD7W1tZkgsKuxS7F7sGUfu2Z/2As1w04xERG8Xi91dXWZxr07d/iV3i52AIdCIbq7uxkdHV2X40hqTSAPIpEIFy9eZPPmzQQCAQ4fPkxNTU1mx7Znz/F4PCSTycz560AgQGVlJa2trdTU1LBjxw7C4TChUIgbN24wNzeXaaSC9BmJSCTC7OzsbSPrZM/mm0gkMufGg8EgXq+X2dnZzDTeyWQy857F+g1nz/pz5MgRDh8+zL59+6iurs57EKZSKebm5uju7ubHP/4xw8PD66pB0KYhkAfxeJzp6WkGBgbo6upiz549VFZWZlqpg8Eg9fX1mavQPB4Pfr+f2tpaampq6OjooL6+nj179jAzM8PU1BSBQOC2nd2elGN2dpaJiYnM6Ld2raG0tBRID8Dp9Xoz/eJ9Ph/T09NEo1Hm5uYyg3PaE2cU4/lvn89HVVUVO3bsoK2tjUAggNfrzes67StAp6amGB0d5fr160QikaINylxoCOSBMYZYLMbZs2fp6+tjx44dmYFJ7bEI6+rquHHjBkNDQwQCAQ4cOEBTUxM1NTVUV1dTUlJCSUkJxpjMNN3Z39Z2NTUSiTA8PEwkEmF6epqysjLKysoIBAJAuoNLRUUFgUAg0xZhB4A9s8/s7CwnT56kv7+fd955p6g+6CJCe3s7HR0dHD9+nMbGRkpLS/NaC7C3eTgc5o033uCtt97i+vXr665B0KYhkEfRaJSpqSkuXLiAMYba2trMt/62bdvYsGEDmzdvpqKigtbWVqqrqzNzGdintCD9ofT5fLcFAKQHO7V7zsViMSKRCF6vF6/Xmzl/Xltbi8/no6Kigqqqqsy1DFVVVVRWVmbCo5jGRrQPZ0pLSykrK+Ohhx5i9+7d1NXVUVFRkff1G2OYmppiaGiICxcu0N/fv66v/9AQyKNoNMri4iKnTp1ieHiYxx57jA0bNuD3+9m9ezeQPu60GwJh6dNZSy0zxlBVVQVAbW3tA5WrrKwMgLq6OsLhMJWVlQBFc/7b5XJlevw1NDTwsY99jAMHDmSmKM8n+1BrcHCQrq4uXnvtNSYmJtZlW4BNQyDPUqkUN27cIJlM8tprr7F7924eeeSRTMv2Uher3I9cq8N2lffSpUtcunSJy5cvMzw8nNN75kpEaG5upqWlhWeeeYa6urpM24jdsGq7M6zu1bB551mV7CnNbclkkomJCcbGxvje975HV1dXZtyA9UxDoACmp6dxu91cvHgRv9/P3r17KS0tfd+lqoWUSqVIJBIMDQ3R1dXFrVu3mJ2dXZWy2ESE6upqmpubOXz4MHV1ddTU1GS6+tqj+mbP/Wg34NmTj9zt9J19daD9O7tGkUgkSCQSxGIxbt68mZnduLu7m7m5uaKoHeWThkABpFIppqamOHnyJJOTk9TU1LB3717q6+uB3L/VlyMSiTA6OsrPf/5zTp06xezsbNGc/04kEoRCIWKxGOPj43fdPsYYIpEIExMTdHZ2EgqFCIfDtz3Hfm1LS0vmwq6Ghga2bduWaXDt7+9nbGyM8+fP09vbS29vLwMDA8zPz6/7AAANgYJJJpOZ8/3nzp2juroaj8dDIBAoeI3AvjZ+aGiIsbExpqamimK8A3vw1lu3bnH27NlMX4q7PTeVShGNRgmFQly9epVwOMz8/Pz7nisiTE5O4vf7mZiYoLa2lqGhoUxtaHBwkMnJSS5fvszNmzcZGRlhYWFhXbcDZNMQKBB7x+vs7GRoaAi32000GuXgwYOZQwPIf60gu4+BfQozFAoVRS3AGMPAwACDg4NcuHABuPv2yD4cuN9Rne2zDvZEInYnKXs+wWIIwtWgIVBg9mnD1157jZ6eHgYGBmhsbMx0gvH7/ZnBLfIhmUwyNjZGT08Pv/71rxkbGyuKAMj2oEO132+V3b7S0+7KbS+z+2A4lYZAgcXjceLxOGfOnKG7u5tkMsnWrVtJpVJs3rwZEcHv9+N2u1f0Yhj7GzMejzMyMkJfXx+dnZ1MTk6uyPuvtHztlPa3v1O/9ZcixdDwsRanIcuVy+XC7XZTVVVFeXl5ptGqtbWVT33qU2zcuJHm5ubMBUSwvEMFe+c3xjA+Ps7IyAhf+cpXuHLlCm+//TaJRMLR34IOc94Y874pArQmsErsb6TJyUlCoVCm/38oFGLr1q1MTk6ysLBARUVFZggsu6Es+9JZuH1Hzx4j3x5xZ2FhgUgkwvXr1xkcHKS7u5vh4eF12w1WPRitCRQRu+fghg0bCAQC7N27l+3bt7Nz50727t1LTU0NNTU1mUMF+38Xi8VIJpPE4/HMRUr2se/w8DC9vb1cvHiRN998kxs3bqzL0XHUfVn5moCI/Cnwh4ABLpKehqwJ+D5QQ3q68n9ljNGvnPtgN1zZQ5RfvnyZyclJ+vr6uHTpUuYiIDsE4L2JUePxONFolMrKSsrLyzNj5A8PDzM6OsrQ0BD9/f2EQiENAHWbZdcERKQZ+BWwyxizICI/AH4KfBz4oTHm+yLyNaDTGPPVe7yXfiLvwq4dVFdX3zZSkdvtvm3Qi1gsxtzcHMFgkKqqKuLxOAsLCwwODhKNRolGo7rjq7y0CXiAMhGJA+XACPBh4F9aj78E/GfgA0NA3Z3dKWZ2dpZIJEI4HH7f6UP7FJfdIcnj8WRqFdFotKgHDFGrL5epyYdF5C+BAWABeI109T9kjLHPvwwBzUu9XkSeB55f7vqdxD5vbvdv/yA6wYl6UMvukSIiG4BngTZgI1ABHL3f1xtjThhjDi1VPVFKFU4u3dI+CvQZY8aNMXHgh8BTQEBE7BpGC7C616cqpT5QLiEwADwuIuWS7sXyEaALeBP4tPWc48AruRVRKZVPyw4BY8wZ4CRwgfTpQRdwAvhz4Asi0kv6NOHXV6CcSqk80c5CSjnHkqcIdfIRpRxOQ0Aph9MQUMrhNASUcjgNAaUcTkNAKYfTEFDK4TQElHI4DQGlHE5DQCmH0xBQyuE0BJRyOA0BpRxOQ0Aph9MQUMrhNASUcjgNAaUcTkNAKYfTEFDK4TQElHI4DQGlHE5DQCmH0xBQyuE0BJRyOA0BpRzuniEgIt8QkTEReTdrWVBEXheRHuv3Bmu5iMjfiEiviLwjIgfzWXilVO7upybwLd4/5fgLwBvGmA7gDes+wMeADuvneeCrK1NMpVS+3DMEjDG/AKbuWPws8JJ1+yXgk1nLv23STpOeprxppQqrlFp5y20TaDDGjFi3R4EG63YzMJj1vCFr2fuIyPMick5Ezi2zDEqpFeDJ9Q2MMWY5swobY06QnspcZyVWahUttyZwy67mW7/HrOXDwKas57VYy5RSRWq5IfAqcNy6fRx4JWv5H1hnCR4HZrIOG5RSxcgY84E/wMvACBAnfYz/OaCG9FmBHuAfgKD1XAG+AlwDLgKH7vX+1uuM/uiP/uT959xS+59YO+Gq0jYBpQrivDHm0J0LtcegUg6nIaCUw2kIKOVwGgJKOZyGgFIOpyGglMNpCCjlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg6nIaCUw2kIKOVwGgJKOZyGgFIOpyGglMNpCCjlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg53zxAQkW+IyJiIvJu17L+LSLeIvCMi/0dEAlmPvSgivSJyRUR+N18FV0qtjPupCXwLOHrHsteBPcaYfcBV4EUAEdkFPAfstl7zP0XEvWKlVUqtuHuGgDHmF8DUHcteM8YkrLunSU9BDvAs8H1jTMwY0wf0Ao+uYHmVUitsJdoE/g3wf63bzcBg1mND1rL3EZHnReSciJxbgTIopZbJk8uLReRLQAL47oO+1hhzAjhhvY/OSqzUKll2CIjIZ4FPAB8x781vPgxsynpai7VMKVWklnU4ICJHgT8DnjHGzGc99CrwnIj4RKQN6ADezr2YSql8uWdNQEReBj4E1IrIEPCfSJ8N8AGviwjAaWPMvzXGXBKRHwBdpA8T/sgYk8xX4ZVSuZP3avKrWAhtE1CqEM4bYw7duVB7DCrlcBoCSjmchoBSDqchoJTDaQgo5XAaAko5nIaAUg6X07UDK2gCiFi/V1stWo5sWo7breVytC61sCg6CwGIyLmlOjJoObQcWo78lkMPB5RyOA0BpRyumELgxGoXwKLluJ2W43brrhxF0yaglFodxVQTUEqtAg0BpRyuKEJARI5a8xT0isgLBVrnJhF5U0S6ROSSiHzeWh4UkddFpMf6vaFA5XGLyD+JyE+s+20icsbaJn8nIt4ClCEgIietOSUui8gTq7E9RORPrf/JuyLysoiUFmp73GWejSW3gaT9jVWmd0TkYJ7LkZ/5Powxq/oDuIFrwFbAC3QCuwqw3ibgoHW7kvT8CbuA/wa8YC1/AfiLAm2HLwDfA35i3f8B8Jx1+2vAvytAGV4C/tC67QUChd4epEen7gPKsrbDZwu1PYDfBg4C72YtW3IbAB8nPdK2AI8DZ/Jcjn8GeKzbf5FVjl3WfuMD2qz9yX3f68r3B+s+/tgngFNZ918EXlyFcrwC/A5wBWiyljUBVwqw7hbgDeDDwE+sD9VE1j/8tm2UpzJUWzuf3LG8oNuD94atD5Lu0foT4HcLuT2ALXfsfEtuA+B/Ab+/1PPyUY47HvsXwHet27ftM8Ap4In7XU8xHA7c91wF+SIiW4CHgDNAgzFmxHpoFGgoQBH+mvTArSnrfg0QMu9N8FKIbdIGjAPftA5L/lZEKijw9jDGDAN/CQwAI8AMcJ7Cb49sd9sGq/nZXdZ8H0sphhBYVSLiB/4e+BNjzGz2YyYdq3k9hyoinwDGjDHn87me++AhXf38qjHmIdLXctzWPlOg7bGB9ExWbcBGoIL3T4O3agqxDe4ll/k+llIMIbBqcxWISAnpAPiuMeaH1uJbItJkPd4EjOW5GE8Bz4hIP/B90ocEXwYCImJf4FWIbTIEDBljzlj3T5IOhUJvj48CfcaYcWNMHPgh6W1U6O2R7W7boOCf3az5Po5ZgZRzOYohBM4CHVbrr5f0hKav5nulkh4r/evAZWPMX2U99Cpw3Lp9nHRbQd4YY140xrQYY7aQ/tv/0RhzDHgT+HQByzEKDIrIb1mLPkJ66PiCbg/ShwGPi0i59T+yy1HQ7XGHu22DV4E/sM4SPA7MZB02rLi8zfeRz0aeB2gA+Tjp1vlrwJcKtM7DpKt17wC/sX4+Tvp4/A2gB/gHIFjA7fAh3js7sNX6R/YC/xvwFWD9B4Bz1jb5EbBhNbYH8F+AbuBd4DukW70Lsj2Al0m3RcRJ144+d7dtQLoB9yvW5/YicCjP5eglfexvf16/lvX8L1nluAJ87EHWpd2GlXK4YjgcUEqtIg0BpRxOQ0Aph9MQUMrhNASUcjgNAaUcTkNAKYf7/195xLC6VxOcAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(np.reshape(Ypred[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "plt.imshow(np.reshape(Y_test[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikB_eI_ZmVtq"
      },
      "outputs": [],
      "source": [
        "def dice(true_mask, pred_mask):\n",
        "    \"\"\"\n",
        "        Computes the Dice coefficient.\n",
        "        Args:\n",
        "            true_mask : Array of arbitrary shape.\n",
        "            pred_mask : Array with the same shape than true_mask.  \n",
        "        \n",
        "        Returns:\n",
        "            A scalar representing the Dice coefficient between the two segmentations. \n",
        "        \n",
        "    \"\"\"\n",
        "    non_seg_score=1.0\n",
        "    if type(pred_mask) != np.ndarray:\n",
        "      t = torch.Tensor([0.5])\n",
        "      pred_mask=(pred_mask > t)\n",
        "    else:\n",
        "      pred_mask[pred_mask>=0.5]=1\n",
        "      pred_mask[pred_mask<0.5]=0\n",
        "\n",
        "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
        "    im_sum = true_mask.sum() + pred_mask.sum()\n",
        "    if im_sum == 0:\n",
        "        return non_seg_score\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(true_mask, pred_mask)\n",
        "    return 2. * intersection.sum() / im_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBVFzRQmXI-",
        "outputId": "ff7ece72-2c55-4971-bd61-4eaef21916c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9644930416996391"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "diceaux=dice(Y_test[160],Ypred[160])\n",
        "diceaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2Sj8PPuJBv"
      },
      "source": [
        "## Model Fit Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh7g3mmhuNPV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj7ESdJAuOaW"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25 epochs"
      ],
      "metadata": {
        "id": "erWVgdGfhXKu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7gaF3LTvOU-",
        "outputId": "ee8aef34-e7f9-4246-e03f-96f3bb0a2fe8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.02252729795873165; accuracy of 98.91234040260315% DiceMetric of 82.19624161720276%\n",
            "Score for fold 2: loss of 0.027991706505417824; accuracy of 98.78780245780945% DiceMetric of 83.0060362815857%\n",
            "Score for fold 3: loss of 0.03762810304760933; accuracy of 98.84026050567627% DiceMetric of 77.28747725486755%\n",
            "Score for fold 4: loss of 0.027162110432982445; accuracy of 98.84926080703735% DiceMetric of 81.70056939125061%\n",
            "Score for fold 5: loss of 0.03008453920483589; accuracy of 98.90668988227844% DiceMetric of 81.10297918319702%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50 epochs"
      ],
      "metadata": {
        "id": "uOWim3VxhZsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7om-UlhbP9",
        "outputId": "dde144d8-2471-4467-e2c5-1bb427d5e669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.03411516919732094; accuracy of 98.90981912612915% DiceMetric of 82.90141820907593%\n",
            "Score for fold 2: loss of 0.032817862927913666; accuracy of 98.79266619682312% DiceMetric of 83.4233283996582%\n",
            "Score for fold 3: loss of 0.03963197022676468; accuracy of 98.85213971138% DiceMetric of 79.96106743812561%\n",
            "Score for fold 4: loss of 0.02851882390677929; accuracy of 98.86234402656555% DiceMetric of 84.72890257835388%\n",
            "Score for fold 5: loss of 0.02618882805109024; accuracy of 98.91459941864014% DiceMetric of 82.52821564674377%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attention Resnet Multihead V1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fGSmVFONoWbQ"
      ],
      "authorship_tag": "ABX9TyMdUZwnSqYG78mHhqYz2ldb",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}