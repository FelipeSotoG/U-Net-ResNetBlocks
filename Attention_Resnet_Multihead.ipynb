{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FelipeSotoG/U-Net-ResNetBlocks/blob/main/Attention_Resnet_Multihead.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcH2AvTh97gF"
      },
      "source": [
        "##Descarga datos\n",
        "Los datos se encuentran en el drive, por lo que usara gdown para sacarlos directamente y no tener que hacer la coneccion, ya que estamos descargando un zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7GLXIiagwH3",
        "outputId": "2ee09d72-c5a8-4598-f6a0-65e1dc16d3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt\n",
            "To: /content/input.zip\n",
            "100% 597M/597M [00:06<00:00, 88.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1f3hc0IdnyN60NjGoPO9Za9Vnmj9pk3zt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uq8I6BfRhogZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a59576c-317d-41c5-a488-8479e1fd2cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace input/train/masks/radiopaedia_10_85902_3_397.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "!unzip -q input.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "H9Y2Q3Bce47L"
      },
      "outputs": [],
      "source": [
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "_jqAPlaMKtpg"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "import os\n",
        "import numpy as np\n",
        "from nibabel.testing import data_path\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "hmJoArAEd41C"
      },
      "outputs": [],
      "source": [
        "import imageio as iio\n",
        "import glob\n",
        "from skimage.transform import resize\n",
        "src=\"/content/input/train\"\n",
        "imag=\"/images/\"\n",
        "X=np.zeros((len(glob.glob(src+imag+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+imag+\"*.png\"))):\n",
        "  X[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)\n",
        "mas=\"/masks/\"\n",
        "Y=np.zeros((len(glob.glob(src+mas+\"*.png\")),IMG_WIDTH,IMG_HEIGHT,1))\n",
        "for i,x in enumerate(sorted(glob.glob(src+mas+\"*.png\"))):\n",
        "  Y[i]=resize(iio.imread(x),(IMG_WIDTH,IMG_HEIGHT,1),mode=\"constant\",preserve_range=True)/255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NcX5DC3VvAR"
      },
      "source": [
        "##Borrar directorio /input en caso de error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QywVJIaZjHC"
      },
      "outputs": [],
      "source": [
        "!rm -r /content/input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6IAoTKTuO5B"
      },
      "source": [
        "##Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ub5XKwbdAQ32"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.3, random_state=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6CambWxdEwn"
      },
      "source": [
        "##Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FlAI4R5nZPxD"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Urgfb0zMWlz-"
      },
      "outputs": [],
      "source": [
        "def DiceMetric(y_true, y_pred):\n",
        "  smooth=1e-6 \n",
        "  gama=2\n",
        "  y_true, y_pred = tf.cast(\n",
        "      y_true, dtype=tf.float32), tf.cast(y_pred, tf.float32)\n",
        "  nominator = 2 * \\\n",
        "      tf.reduce_sum(tf.multiply(y_pred, y_true)) + smooth\n",
        "  denominator = tf.reduce_sum(\n",
        "      y_pred ** gama) + tf.reduce_sum(y_true ** gama) + smooth\n",
        "  result = tf.divide(nominator, denominator)\n",
        "  return result\n",
        "def DiceLoss(y_true, y_pred):\n",
        "      result= 1- DiceMetric(y_true, y_pred)\n",
        "      return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "MDQHBprOXZXG"
      },
      "outputs": [],
      "source": [
        "def conv_block(X,f,d=0.1,group=1):\n",
        "  c = tf.keras.layers.Conv2D(f[0], (2, 2), activation='relu', kernel_initializer='he_normal', padding='same',groups=group)(X)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  c = tf.keras.layers.Conv2D(f[1], (2, 2), kernel_initializer='he_normal', padding='same', groups=group)(c)\n",
        "  c = tf.keras.layers.BatchNormalization(axis=3)(c)\n",
        "  c = tf.keras.layers.Dropout(d)(c)\n",
        "  s = tf.keras.layers.Conv2D(f[1], (2, 2), kernel_initializer='he_normal', padding='same')(X)\n",
        "  s = tf.keras.layers.BatchNormalization(axis=3)(s)\n",
        "  c = tf.keras.layers.Add()([c,s])\n",
        "  c = tf.keras.layers.ReLU()(c)\n",
        "  return c,s"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNWwE-DOcxpY"
      },
      "source": [
        "### Max pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJgrBPxjiV-c",
        "outputId": "4350e5fc-4eb3-4ab4-df24-23bebf5e6c3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_12 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_11 (Lambda)             (None, 128, 128, 1)  0           ['input_12[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_282 (Conv2D)            (None, 128, 128, 16  80          ['lambda_11[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_273 (Batch  (None, 128, 128, 16  64         ['conv2d_282[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_91 (Dropout)           (None, 128, 128, 16  0           ['batch_normalization_273[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_283 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_91[0][0]']             \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_274 (Batch  (None, 128, 128, 16  64         ['conv2d_283[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_284 (Conv2D)            (None, 128, 128, 16  80          ['lambda_11[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_92 (Dropout)           (None, 128, 128, 16  0           ['batch_normalization_274[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_275 (Batch  (None, 128, 128, 16  64         ['conv2d_284[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_91 (Add)                   (None, 128, 128, 16  0           ['dropout_92[0][0]',             \n",
            "                                )                                 'batch_normalization_275[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_91 (ReLU)                (None, 128, 128, 16  0           ['add_91[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_57 (MaxPooling2D  (None, 64, 64, 16)  0           ['re_lu_91[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_285 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_57[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_276 (Batch  (None, 64, 64, 32)  128         ['conv2d_285[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_93 (Dropout)           (None, 64, 64, 32)   0           ['batch_normalization_276[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_286 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_93[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_277 (Batch  (None, 64, 64, 32)  128         ['conv2d_286[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_287 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_57[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_94 (Dropout)           (None, 64, 64, 32)   0           ['batch_normalization_277[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_278 (Batch  (None, 64, 64, 32)  128         ['conv2d_287[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_92 (Add)                   (None, 64, 64, 32)   0           ['dropout_94[0][0]',             \n",
            "                                                                  'batch_normalization_278[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_92 (ReLU)                (None, 64, 64, 32)   0           ['add_92[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_58 (MaxPooling2D  (None, 32, 32, 32)  0           ['re_lu_92[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_288 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_58[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_279 (Batch  (None, 32, 32, 64)  256         ['conv2d_288[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_95 (Dropout)           (None, 32, 32, 64)   0           ['batch_normalization_279[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_289 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_95[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_280 (Batch  (None, 32, 32, 64)  256         ['conv2d_289[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_290 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_58[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_96 (Dropout)           (None, 32, 32, 64)   0           ['batch_normalization_280[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_281 (Batch  (None, 32, 32, 64)  256         ['conv2d_290[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_93 (Add)                   (None, 32, 32, 64)   0           ['dropout_96[0][0]',             \n",
            "                                                                  'batch_normalization_281[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_93 (ReLU)                (None, 32, 32, 64)   0           ['add_93[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_59 (MaxPooling2D  (None, 16, 16, 64)  0           ['re_lu_93[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_291 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_59[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_282 (Batch  (None, 16, 16, 128)  512        ['conv2d_291[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_97 (Dropout)           (None, 16, 16, 128)  0           ['batch_normalization_282[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_292 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_97[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_283 (Batch  (None, 16, 16, 128)  512        ['conv2d_292[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_293 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_59[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_98 (Dropout)           (None, 16, 16, 128)  0           ['batch_normalization_283[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_284 (Batch  (None, 16, 16, 128)  512        ['conv2d_293[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_94 (Add)                   (None, 16, 16, 128)  0           ['dropout_98[0][0]',             \n",
            "                                                                  'batch_normalization_284[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_94 (ReLU)                (None, 16, 16, 128)  0           ['add_94[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_60 (MaxPooling2D  (None, 8, 8, 128)   0           ['re_lu_94[0][0]']               \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_294 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_60[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_285 (Batch  (None, 8, 8, 256)   1024        ['conv2d_294[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_99 (Dropout)           (None, 8, 8, 256)    0           ['batch_normalization_285[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_295 (Conv2D)            (None, 8, 8, 256)    262400      ['dropout_99[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization_286 (Batch  (None, 8, 8, 256)   1024        ['conv2d_295[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_296 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_60[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_100 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_286[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_287 (Batch  (None, 8, 8, 256)   1024        ['conv2d_296[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_95 (Add)                   (None, 8, 8, 256)    0           ['dropout_100[0][0]',            \n",
            "                                                                  'batch_normalization_287[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_95 (ReLU)                (None, 8, 8, 256)    0           ['add_95[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_61 (MaxPooling2D  (None, 2, 2, 256)   0           ['batch_normalization_287[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_36 (Conv2DTra  (None, 16, 16, 128)  131200     ['re_lu_95[0][0]']               \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_37 (Multi  (None, 16, 16, 128)  74144      ['batch_normalization_284[0][0]',\n",
            " HeadAttention)                                                   'max_pooling2d_61[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_36[0][0]',    \n",
            "                                                                  'multi_head_attention_37[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_297 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_36[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_288 (Batch  (None, 16, 16, 128)  512        ['conv2d_297[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_101 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_288[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_298 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_101[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_289 (Batch  (None, 16, 16, 128)  512        ['conv2d_298[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_299 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_36[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_102 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_289[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_290 (Batch  (None, 16, 16, 128)  512        ['conv2d_299[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_96 (Add)                   (None, 16, 16, 128)  0           ['dropout_102[0][0]',            \n",
            "                                                                  'batch_normalization_290[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_96 (ReLU)                (None, 16, 16, 128)  0           ['add_96[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_62 (MaxPooling2D  (None, 4, 4, 128)   0           ['batch_normalization_290[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_37 (Conv2DTra  (None, 32, 32, 64)  32832       ['re_lu_96[0][0]']               \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_38 (Multi  (None, 32, 32, 64)  37216       ['batch_normalization_281[0][0]',\n",
            " HeadAttention)                                                   'max_pooling2d_62[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_37[0][0]',    \n",
            "                                                                  'multi_head_attention_38[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_300 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_37[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_291 (Batch  (None, 32, 32, 64)  256         ['conv2d_300[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_103 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_291[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_301 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_103[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_292 (Batch  (None, 32, 32, 64)  256         ['conv2d_301[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_302 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_37[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_104 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_292[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_293 (Batch  (None, 32, 32, 64)  256         ['conv2d_302[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_97 (Add)                   (None, 32, 32, 64)   0           ['dropout_104[0][0]',            \n",
            "                                                                  'batch_normalization_293[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_97 (ReLU)                (None, 32, 32, 64)   0           ['add_97[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_63 (MaxPooling2D  (None, 8, 8, 64)    0           ['batch_normalization_293[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_38 (Conv2DTra  (None, 64, 64, 32)  8224        ['re_lu_97[0][0]']               \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_39 (Multi  (None, 64, 64, 32)  18752       ['batch_normalization_278[0][0]',\n",
            " HeadAttention)                                                   'max_pooling2d_63[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_38[0][0]',    \n",
            "                                                                  'multi_head_attention_39[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_303 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_294 (Batch  (None, 64, 64, 32)  128         ['conv2d_303[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_105 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_294[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_304 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_105[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_295 (Batch  (None, 64, 64, 32)  128         ['conv2d_304[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_305 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_38[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_106 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_295[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_296 (Batch  (None, 64, 64, 32)  128         ['conv2d_305[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_98 (Add)                   (None, 64, 64, 32)   0           ['dropout_106[0][0]',            \n",
            "                                                                  'batch_normalization_296[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_98 (ReLU)                (None, 64, 64, 32)   0           ['add_98[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling2d_64 (MaxPooling2D  (None, 16, 16, 32)  0           ['batch_normalization_296[0][0]']\n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_transpose_39 (Conv2DTra  (None, 128, 128, 16  2064       ['re_lu_98[0][0]']               \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_40 (Multi  (None, 128, 128, 16  9520       ['batch_normalization_275[0][0]',\n",
            " HeadAttention)                 )                                 'max_pooling2d_64[0][0]']       \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_39[0][0]',    \n",
            "                                )                                 'multi_head_attention_40[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_306 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_39[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_297 (Batch  (None, 128, 128, 16  64         ['conv2d_306[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_107 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_297[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_307 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_107[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_298 (Batch  (None, 128, 128, 16  64         ['conv2d_307[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_308 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_39[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_108 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_298[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_299 (Batch  (None, 128, 128, 16  64         ['conv2d_308[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_99 (Add)                   (None, 128, 128, 16  0           ['dropout_108[0][0]',            \n",
            "                                )                                 'batch_normalization_299[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_99 (ReLU)                (None, 128, 128, 16  0           ['add_99[0][0]']                 \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_309 (Conv2D)            (None, 128, 128, 1)  17          ['re_lu_99[0][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,457,681\n",
            "Trainable params: 1,453,265\n",
            "Non-trainable params: 4,416\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "nheads=32\n",
        "\n",
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "#s= inputs\n",
        "#Contraction path\n",
        "c1,z1 = conv_block(s,[16,16])\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "c2,z2 = conv_block(p1,[32,32])\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "#Expansive path \n",
        "z5=tf.keras.layers.MaxPooling2D((4, 4))(z5)\n",
        "m1= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z4,z5)\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "z6=tf.keras.layers.MaxPooling2D((4, 4))(z6)\n",
        "m2= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z3,z6)\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "z7=tf.keras.layers.MaxPooling2D((4, 4))(z7)\n",
        "m3= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z2,z7) \n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "z8=tf.keras.layers.MaxPooling2D((4, 4))(z8)\n",
        "m4= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z1,z8) \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "c9,_ = conv_block(u9,[16,16])\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgCsYVONdWxZ"
      },
      "source": [
        "### Average pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Q_AsdR1dWxo",
        "outputId": "2d3eae77-fc7c-46be-a604-4a20db39ebf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_15\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)          [(None, 128, 128, 1  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " lambda_12 (Lambda)             (None, 128, 128, 1)  0           ['input_13[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_310 (Conv2D)            (None, 128, 128, 16  80          ['lambda_12[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_300 (Batch  (None, 128, 128, 16  64         ['conv2d_310[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_109 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_300[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_311 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_109[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_301 (Batch  (None, 128, 128, 16  64         ['conv2d_311[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_312 (Conv2D)            (None, 128, 128, 16  80          ['lambda_12[0][0]']              \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_110 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_301[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_302 (Batch  (None, 128, 128, 16  64         ['conv2d_312[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_100 (Add)                  (None, 128, 128, 16  0           ['dropout_110[0][0]',            \n",
            "                                )                                 'batch_normalization_302[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_100 (ReLU)               (None, 128, 128, 16  0           ['add_100[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " max_pooling2d_65 (MaxPooling2D  (None, 64, 64, 16)  0           ['re_lu_100[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_313 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_65[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_303 (Batch  (None, 64, 64, 32)  128         ['conv2d_313[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_111 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_303[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_314 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_111[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_304 (Batch  (None, 64, 64, 32)  128         ['conv2d_314[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_315 (Conv2D)            (None, 64, 64, 32)   2080        ['max_pooling2d_65[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_112 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_304[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_305 (Batch  (None, 64, 64, 32)  128         ['conv2d_315[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_101 (Add)                  (None, 64, 64, 32)   0           ['dropout_112[0][0]',            \n",
            "                                                                  'batch_normalization_305[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_101 (ReLU)               (None, 64, 64, 32)   0           ['add_101[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_66 (MaxPooling2D  (None, 32, 32, 32)  0           ['re_lu_101[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_316 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_66[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_306 (Batch  (None, 32, 32, 64)  256         ['conv2d_316[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_113 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_306[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_317 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_113[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_307 (Batch  (None, 32, 32, 64)  256         ['conv2d_317[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_318 (Conv2D)            (None, 32, 32, 64)   8256        ['max_pooling2d_66[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_114 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_307[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_308 (Batch  (None, 32, 32, 64)  256         ['conv2d_318[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_102 (Add)                  (None, 32, 32, 64)   0           ['dropout_114[0][0]',            \n",
            "                                                                  'batch_normalization_308[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_102 (ReLU)               (None, 32, 32, 64)   0           ['add_102[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_67 (MaxPooling2D  (None, 16, 16, 64)  0           ['re_lu_102[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_319 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_67[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_309 (Batch  (None, 16, 16, 128)  512        ['conv2d_319[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_115 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_309[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_320 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_115[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_310 (Batch  (None, 16, 16, 128)  512        ['conv2d_320[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_321 (Conv2D)            (None, 16, 16, 128)  32896       ['max_pooling2d_67[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_116 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_310[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_311 (Batch  (None, 16, 16, 128)  512        ['conv2d_321[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_103 (Add)                  (None, 16, 16, 128)  0           ['dropout_116[0][0]',            \n",
            "                                                                  'batch_normalization_311[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_103 (ReLU)               (None, 16, 16, 128)  0           ['add_103[0][0]']                \n",
            "                                                                                                  \n",
            " max_pooling2d_68 (MaxPooling2D  (None, 8, 8, 128)   0           ['re_lu_103[0][0]']              \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " conv2d_322 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_68[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_312 (Batch  (None, 8, 8, 256)   1024        ['conv2d_322[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_117 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_312[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_323 (Conv2D)            (None, 8, 8, 256)    262400      ['dropout_117[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_313 (Batch  (None, 8, 8, 256)   1024        ['conv2d_323[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_324 (Conv2D)            (None, 8, 8, 256)    131328      ['max_pooling2d_68[0][0]']       \n",
            "                                                                                                  \n",
            " dropout_118 (Dropout)          (None, 8, 8, 256)    0           ['batch_normalization_313[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_314 (Batch  (None, 8, 8, 256)   1024        ['conv2d_324[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_104 (Add)                  (None, 8, 8, 256)    0           ['dropout_118[0][0]',            \n",
            "                                                                  'batch_normalization_314[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_104 (ReLU)               (None, 8, 8, 256)    0           ['add_104[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_20 (AverageP  (None, 3, 3, 256)   0           ['batch_normalization_314[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_40 (Conv2DTra  (None, 16, 16, 128)  131200     ['re_lu_104[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_41 (Multi  (None, 16, 16, 128)  37136      ['batch_normalization_311[0][0]',\n",
            " HeadAttention)                                                   'average_pooling2d_20[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 16, 16, 256)  0           ['conv2d_transpose_40[0][0]',    \n",
            "                                                                  'multi_head_attention_41[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_325 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_315 (Batch  (None, 16, 16, 128)  512        ['conv2d_325[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_119 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_315[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_326 (Conv2D)            (None, 16, 16, 128)  65664       ['dropout_119[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_316 (Batch  (None, 16, 16, 128)  512        ['conv2d_326[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_327 (Conv2D)            (None, 16, 16, 128)  131200      ['concatenate_40[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_120 (Dropout)          (None, 16, 16, 128)  0           ['batch_normalization_316[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_317 (Batch  (None, 16, 16, 128)  512        ['conv2d_327[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_105 (Add)                  (None, 16, 16, 128)  0           ['dropout_120[0][0]',            \n",
            "                                                                  'batch_normalization_317[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_105 (ReLU)               (None, 16, 16, 128)  0           ['add_105[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_21 (AverageP  (None, 5, 5, 128)   0           ['batch_normalization_317[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_41 (Conv2DTra  (None, 32, 32, 64)  32832       ['re_lu_105[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_42 (Multi  (None, 32, 32, 64)  18640       ['batch_normalization_308[0][0]',\n",
            " HeadAttention)                                                   'average_pooling2d_21[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 32, 32, 128)  0           ['conv2d_transpose_41[0][0]',    \n",
            "                                                                  'multi_head_attention_42[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_328 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_318 (Batch  (None, 32, 32, 64)  256         ['conv2d_328[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_121 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_318[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_329 (Conv2D)            (None, 32, 32, 64)   16448       ['dropout_121[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_319 (Batch  (None, 32, 32, 64)  256         ['conv2d_329[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_330 (Conv2D)            (None, 32, 32, 64)   32832       ['concatenate_41[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_122 (Dropout)          (None, 32, 32, 64)   0           ['batch_normalization_319[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_320 (Batch  (None, 32, 32, 64)  256         ['conv2d_330[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_106 (Add)                  (None, 32, 32, 64)   0           ['dropout_122[0][0]',            \n",
            "                                                                  'batch_normalization_320[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_106 (ReLU)               (None, 32, 32, 64)   0           ['add_106[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_22 (AverageP  (None, 11, 11, 64)  0           ['batch_normalization_320[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_42 (Conv2DTra  (None, 64, 64, 32)  8224        ['re_lu_106[0][0]']              \n",
            " nspose)                                                                                          \n",
            "                                                                                                  \n",
            " multi_head_attention_43 (Multi  (None, 64, 64, 32)  9392        ['batch_normalization_305[0][0]',\n",
            " HeadAttention)                                                   'average_pooling2d_22[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 64, 64, 64)   0           ['conv2d_transpose_42[0][0]',    \n",
            "                                                                  'multi_head_attention_43[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_331 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_42[0][0]']         \n",
            "                                                                                                  \n",
            " batch_normalization_321 (Batch  (None, 64, 64, 32)  128         ['conv2d_331[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " dropout_123 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_321[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_332 (Conv2D)            (None, 64, 64, 32)   4128        ['dropout_123[0][0]']            \n",
            "                                                                                                  \n",
            " batch_normalization_322 (Batch  (None, 64, 64, 32)  128         ['conv2d_332[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " conv2d_333 (Conv2D)            (None, 64, 64, 32)   8224        ['concatenate_42[0][0]']         \n",
            "                                                                                                  \n",
            " dropout_124 (Dropout)          (None, 64, 64, 32)   0           ['batch_normalization_322[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_323 (Batch  (None, 64, 64, 32)  128         ['conv2d_333[0][0]']             \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " add_107 (Add)                  (None, 64, 64, 32)   0           ['dropout_124[0][0]',            \n",
            "                                                                  'batch_normalization_323[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_107 (ReLU)               (None, 64, 64, 32)   0           ['add_107[0][0]']                \n",
            "                                                                                                  \n",
            " average_pooling2d_23 (AverageP  (None, 13, 13, 32)  0           ['batch_normalization_323[0][0]']\n",
            " ooling2D)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_transpose_43 (Conv2DTra  (None, 128, 128, 16  2064       ['re_lu_107[0][0]']              \n",
            " nspose)                        )                                                                 \n",
            "                                                                                                  \n",
            " multi_head_attention_44 (Multi  (None, 128, 128, 16  4768       ['batch_normalization_302[0][0]',\n",
            " HeadAttention)                 )                                 'average_pooling2d_23[0][0]']   \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 128, 128, 32  0           ['conv2d_transpose_43[0][0]',    \n",
            "                                )                                 'multi_head_attention_44[0][0]']\n",
            "                                                                                                  \n",
            " conv2d_334 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_43[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_324 (Batch  (None, 128, 128, 16  64         ['conv2d_334[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " dropout_125 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_324[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_335 (Conv2D)            (None, 128, 128, 16  1040        ['dropout_125[0][0]']            \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_325 (Batch  (None, 128, 128, 16  64         ['conv2d_335[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_336 (Conv2D)            (None, 128, 128, 16  2064        ['concatenate_43[0][0]']         \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " dropout_126 (Dropout)          (None, 128, 128, 16  0           ['batch_normalization_325[0][0]']\n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_326 (Batch  (None, 128, 128, 16  64         ['conv2d_336[0][0]']             \n",
            " Normalization)                 )                                                                 \n",
            "                                                                                                  \n",
            " add_108 (Add)                  (None, 128, 128, 16  0           ['dropout_126[0][0]',            \n",
            "                                )                                 'batch_normalization_326[0][0]']\n",
            "                                                                                                  \n",
            " re_lu_108 (ReLU)               (None, 128, 128, 16  0           ['add_108[0][0]']                \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv2d_337 (Conv2D)            (None, 128, 128, 1)  17          ['re_lu_108[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,387,985\n",
            "Trainable params: 1,383,569\n",
            "Non-trainable params: 4,416\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 1\n",
        "nheads=16\n",
        "\n",
        "#Build the model\n",
        "inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)\n",
        "#s= inputs\n",
        "#Contraction path\n",
        "c1,z1 = conv_block(s,[16,16])\n",
        "p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n",
        "\n",
        "\n",
        "c2,z2 = conv_block(p1,[32,32])\n",
        "p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n",
        "\n",
        "c3,z3 = conv_block(p2,[64,64],0.2)\n",
        "p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n",
        " \n",
        "c4,z4 = conv_block(p3,[128,128],0.2)\n",
        "p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n",
        " \n",
        "c5,z5 = conv_block(p4,[256,256],0.3)\n",
        "\n",
        "#Expansive path \n",
        "z5=tf.keras.layers.AveragePooling2D(strides=(3,3))(z5)\n",
        "m1= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z4,z5)\n",
        "u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, m1])\n",
        "c6,z6 = conv_block(u6,[128,128],0.2)\n",
        "\n",
        "z6=tf.keras.layers.AveragePooling2D(strides=(3,3))(z6)\n",
        "m2= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z3,z6)\n",
        "u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, m2])\n",
        "c7,z7 = conv_block(u7,[64,64],0.2)\n",
        "\n",
        "z7=tf.keras.layers.AveragePooling2D(strides=(3,3))(z7)\n",
        "m3= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z2,z7) \n",
        "u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, m3])\n",
        "c8,z8 = conv_block(u8,[32,32])\n",
        "\n",
        "z8=tf.keras.layers.AveragePooling2D(strides=(5,5))(z8)\n",
        "m4= tf.keras.layers.MultiHeadAttention(num_heads=nheads, key_dim=3, attention_axes=(1, 2))(z1,z8) \n",
        "u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, m4], axis=3)\n",
        "c9,_ = conv_block(u9,[16,16])\n",
        " \n",
        "outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(c9)\n",
        " \n",
        "model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JFyLMRezeWO"
      },
      "source": [
        "##Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCB8atJ1zda4",
        "outputId": "cd552d5d-14f3-45de-cfcf-b065dfa58d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "198/198 [==============================] - 57s 286ms/step - loss: 0.0236 - accuracy: 0.9882 - DiceMetric: 0.6715 - val_loss: 0.0234 - val_accuracy: 0.9871 - val_DiceMetric: 0.4584\n",
            "Epoch 2/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0170 - accuracy: 0.9885 - DiceMetric: 0.7510 - val_loss: 0.0170 - val_accuracy: 0.9875 - val_DiceMetric: 0.6516\n",
            "Epoch 3/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0146 - accuracy: 0.9886 - DiceMetric: 0.7812 - val_loss: 0.0148 - val_accuracy: 0.9872 - val_DiceMetric: 0.7392\n",
            "Epoch 4/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0152 - accuracy: 0.9883 - DiceMetric: 0.7446 - val_loss: 0.0146 - val_accuracy: 0.9874 - val_DiceMetric: 0.7117\n",
            "Epoch 5/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0119 - accuracy: 0.9888 - DiceMetric: 0.8197 - val_loss: 0.0139 - val_accuracy: 0.9870 - val_DiceMetric: 0.7869\n",
            "Epoch 6/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0109 - accuracy: 0.9889 - DiceMetric: 0.8340 - val_loss: 0.0125 - val_accuracy: 0.9878 - val_DiceMetric: 0.7536\n",
            "Epoch 7/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0106 - accuracy: 0.9889 - DiceMetric: 0.8347 - val_loss: 0.0130 - val_accuracy: 0.9877 - val_DiceMetric: 0.7379\n",
            "Epoch 8/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0097 - accuracy: 0.9891 - DiceMetric: 0.8455 - val_loss: 0.0111 - val_accuracy: 0.9876 - val_DiceMetric: 0.8133\n",
            "Epoch 9/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0097 - accuracy: 0.9890 - DiceMetric: 0.8533 - val_loss: 0.0117 - val_accuracy: 0.9879 - val_DiceMetric: 0.7835\n",
            "Epoch 10/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0095 - accuracy: 0.9891 - DiceMetric: 0.8590 - val_loss: 0.0162 - val_accuracy: 0.9859 - val_DiceMetric: 0.7496\n",
            "Epoch 11/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0121 - accuracy: 0.9887 - DiceMetric: 0.7897 - val_loss: 0.0114 - val_accuracy: 0.9876 - val_DiceMetric: 0.7902\n",
            "Epoch 12/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0096 - accuracy: 0.9890 - DiceMetric: 0.8501 - val_loss: 0.0104 - val_accuracy: 0.9878 - val_DiceMetric: 0.8062\n",
            "Epoch 13/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0089 - accuracy: 0.9891 - DiceMetric: 0.8596 - val_loss: 0.0103 - val_accuracy: 0.9880 - val_DiceMetric: 0.8095\n",
            "Epoch 14/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0087 - accuracy: 0.9891 - DiceMetric: 0.8660 - val_loss: 0.0104 - val_accuracy: 0.9876 - val_DiceMetric: 0.8182\n",
            "Epoch 15/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0083 - accuracy: 0.9892 - DiceMetric: 0.8757 - val_loss: 0.0102 - val_accuracy: 0.9880 - val_DiceMetric: 0.7990\n",
            "Epoch 16/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8794 - val_loss: 0.0095 - val_accuracy: 0.9880 - val_DiceMetric: 0.8285\n",
            "Epoch 17/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0081 - accuracy: 0.9892 - DiceMetric: 0.8799 - val_loss: 0.0093 - val_accuracy: 0.9880 - val_DiceMetric: 0.8336\n",
            "Epoch 18/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0078 - accuracy: 0.9893 - DiceMetric: 0.8846 - val_loss: 0.0091 - val_accuracy: 0.9879 - val_DiceMetric: 0.8380\n",
            "Epoch 19/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0077 - accuracy: 0.9892 - DiceMetric: 0.8870 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.8364\n",
            "Epoch 20/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0074 - accuracy: 0.9893 - DiceMetric: 0.8950 - val_loss: 0.0102 - val_accuracy: 0.9880 - val_DiceMetric: 0.8096\n",
            "Epoch 21/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.8819 - val_loss: 0.0094 - val_accuracy: 0.9877 - val_DiceMetric: 0.8475\n",
            "Epoch 22/25\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0076 - accuracy: 0.9893 - DiceMetric: 0.8897 - val_loss: 0.0092 - val_accuracy: 0.9880 - val_DiceMetric: 0.8337\n",
            "Epoch 23/25\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9040 - val_loss: 0.0093 - val_accuracy: 0.9876 - val_DiceMetric: 0.8464\n",
            "Epoch 24/25\n",
            "198/198 [==============================] - 57s 286ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.8959 - val_loss: 0.0091 - val_accuracy: 0.9879 - val_DiceMetric: 0.8354\n",
            "Epoch 25/25\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.8977 - val_loss: 0.0092 - val_accuracy: 0.9881 - val_DiceMetric: 0.8337\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f441e2e77d0>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train,Y_train,batch_size=10,epochs=25,validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,Y_train,batch_size=10,epochs=200,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvbQrIDSWZih",
        "outputId": "7220a147-22a3-4022-ba88-b4e5ac749b84"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "198/198 [==============================] - 58s 267ms/step - loss: 0.1938 - accuracy: 0.9530 - DiceMetric: 0.0627 - val_loss: 0.0792 - val_accuracy: 0.9855 - val_DiceMetric: 0.0761\n",
            "Epoch 2/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0593 - accuracy: 0.9870 - DiceMetric: 0.0651 - val_loss: 0.0601 - val_accuracy: 0.9855 - val_DiceMetric: 0.0582\n",
            "Epoch 3/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0438 - accuracy: 0.9870 - DiceMetric: 0.1716 - val_loss: 0.0495 - val_accuracy: 0.9855 - val_DiceMetric: 0.3111\n",
            "Epoch 4/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0341 - accuracy: 0.9869 - DiceMetric: 0.2973 - val_loss: 0.0444 - val_accuracy: 0.9855 - val_DiceMetric: 0.1117\n",
            "Epoch 5/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0272 - accuracy: 0.9869 - DiceMetric: 0.4443 - val_loss: 0.0233 - val_accuracy: 0.9861 - val_DiceMetric: 0.5072\n",
            "Epoch 6/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0211 - accuracy: 0.9875 - DiceMetric: 0.5811 - val_loss: 0.0283 - val_accuracy: 0.9858 - val_DiceMetric: 0.2875\n",
            "Epoch 7/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0181 - accuracy: 0.9879 - DiceMetric: 0.6610 - val_loss: 0.0176 - val_accuracy: 0.9873 - val_DiceMetric: 0.6512\n",
            "Epoch 8/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0154 - accuracy: 0.9883 - DiceMetric: 0.7181 - val_loss: 0.0178 - val_accuracy: 0.9873 - val_DiceMetric: 0.6074\n",
            "Epoch 9/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0132 - accuracy: 0.9886 - DiceMetric: 0.7729 - val_loss: 0.0147 - val_accuracy: 0.9877 - val_DiceMetric: 0.7509\n",
            "Epoch 10/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0120 - accuracy: 0.9887 - DiceMetric: 0.8046 - val_loss: 0.0125 - val_accuracy: 0.9877 - val_DiceMetric: 0.8164\n",
            "Epoch 11/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0117 - accuracy: 0.9887 - DiceMetric: 0.8029 - val_loss: 0.0134 - val_accuracy: 0.9877 - val_DiceMetric: 0.7592\n",
            "Epoch 12/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0114 - accuracy: 0.9888 - DiceMetric: 0.8056 - val_loss: 0.0131 - val_accuracy: 0.9877 - val_DiceMetric: 0.7602\n",
            "Epoch 13/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0109 - accuracy: 0.9888 - DiceMetric: 0.8205 - val_loss: 0.0120 - val_accuracy: 0.9877 - val_DiceMetric: 0.8199\n",
            "Epoch 14/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0103 - accuracy: 0.9889 - DiceMetric: 0.8335 - val_loss: 0.0116 - val_accuracy: 0.9875 - val_DiceMetric: 0.8349\n",
            "Epoch 15/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0099 - accuracy: 0.9890 - DiceMetric: 0.8441 - val_loss: 0.0146 - val_accuracy: 0.9878 - val_DiceMetric: 0.7169\n",
            "Epoch 16/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0098 - accuracy: 0.9890 - DiceMetric: 0.8415 - val_loss: 0.0111 - val_accuracy: 0.9880 - val_DiceMetric: 0.8271\n",
            "Epoch 17/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0092 - accuracy: 0.9891 - DiceMetric: 0.8595 - val_loss: 0.0104 - val_accuracy: 0.9878 - val_DiceMetric: 0.8583\n",
            "Epoch 18/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0091 - accuracy: 0.9891 - DiceMetric: 0.8584 - val_loss: 0.0101 - val_accuracy: 0.9878 - val_DiceMetric: 0.8665\n",
            "Epoch 19/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0090 - accuracy: 0.9891 - DiceMetric: 0.8531 - val_loss: 0.0102 - val_accuracy: 0.9879 - val_DiceMetric: 0.8618\n",
            "Epoch 20/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0087 - accuracy: 0.9891 - DiceMetric: 0.8651 - val_loss: 0.0098 - val_accuracy: 0.9879 - val_DiceMetric: 0.8687\n",
            "Epoch 21/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0085 - accuracy: 0.9892 - DiceMetric: 0.8688 - val_loss: 0.0096 - val_accuracy: 0.9879 - val_DiceMetric: 0.8775\n",
            "Epoch 22/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0081 - accuracy: 0.9892 - DiceMetric: 0.8797 - val_loss: 0.0122 - val_accuracy: 0.9879 - val_DiceMetric: 0.7823\n",
            "Epoch 23/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8816 - val_loss: 0.0094 - val_accuracy: 0.9877 - val_DiceMetric: 0.8860\n",
            "Epoch 24/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0079 - accuracy: 0.9892 - DiceMetric: 0.8820 - val_loss: 0.0116 - val_accuracy: 0.9868 - val_DiceMetric: 0.8534\n",
            "Epoch 25/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8756 - val_loss: 0.0099 - val_accuracy: 0.9881 - val_DiceMetric: 0.8540\n",
            "Epoch 26/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8818 - val_loss: 0.0094 - val_accuracy: 0.9878 - val_DiceMetric: 0.8856\n",
            "Epoch 27/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0079 - accuracy: 0.9892 - DiceMetric: 0.8845 - val_loss: 0.0103 - val_accuracy: 0.9881 - val_DiceMetric: 0.8560\n",
            "Epoch 28/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0074 - accuracy: 0.9893 - DiceMetric: 0.8967 - val_loss: 0.0097 - val_accuracy: 0.9880 - val_DiceMetric: 0.8849\n",
            "Epoch 29/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0072 - accuracy: 0.9893 - DiceMetric: 0.9035 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.8950\n",
            "Epoch 30/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0073 - accuracy: 0.9893 - DiceMetric: 0.8968 - val_loss: 0.0100 - val_accuracy: 0.9880 - val_DiceMetric: 0.8599\n",
            "Epoch 31/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0075 - accuracy: 0.9893 - DiceMetric: 0.8907 - val_loss: 0.0093 - val_accuracy: 0.9881 - val_DiceMetric: 0.8656\n",
            "Epoch 32/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.8972 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.8912\n",
            "Epoch 33/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0069 - accuracy: 0.9894 - DiceMetric: 0.9065 - val_loss: 0.0100 - val_accuracy: 0.9881 - val_DiceMetric: 0.8656\n",
            "Epoch 34/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0070 - accuracy: 0.9894 - DiceMetric: 0.9075 - val_loss: 0.0084 - val_accuracy: 0.9880 - val_DiceMetric: 0.9021\n",
            "Epoch 35/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9076 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9009\n",
            "Epoch 36/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9076 - val_loss: 0.0082 - val_accuracy: 0.9881 - val_DiceMetric: 0.9014\n",
            "Epoch 37/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0067 - accuracy: 0.9894 - DiceMetric: 0.9049 - val_loss: 0.0087 - val_accuracy: 0.9880 - val_DiceMetric: 0.9002\n",
            "Epoch 38/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9110 - val_loss: 0.0081 - val_accuracy: 0.9882 - val_DiceMetric: 0.9097\n",
            "Epoch 39/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9128 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.8800\n",
            "Epoch 40/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9178 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.8983\n",
            "Epoch 41/200\n",
            "198/198 [==============================] - 57s 287ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9192 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9002\n",
            "Epoch 42/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0069 - accuracy: 0.9894 - DiceMetric: 0.9081 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.8918\n",
            "Epoch 43/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0062 - accuracy: 0.9894 - DiceMetric: 0.9170 - val_loss: 0.0097 - val_accuracy: 0.9882 - val_DiceMetric: 0.8772\n",
            "Epoch 44/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0064 - accuracy: 0.9894 - DiceMetric: 0.9109 - val_loss: 0.0108 - val_accuracy: 0.9873 - val_DiceMetric: 0.8688\n",
            "Epoch 45/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0117 - accuracy: 0.9888 - DiceMetric: 0.7980 - val_loss: 0.0098 - val_accuracy: 0.9879 - val_DiceMetric: 0.8758\n",
            "Epoch 46/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0078 - accuracy: 0.9892 - DiceMetric: 0.8888 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.8826\n",
            "Epoch 47/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0071 - accuracy: 0.9893 - DiceMetric: 0.9003 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.8941\n",
            "Epoch 48/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0067 - accuracy: 0.9894 - DiceMetric: 0.9081 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.8869\n",
            "Epoch 49/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9058 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9056\n",
            "Epoch 50/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0062 - accuracy: 0.9895 - DiceMetric: 0.9176 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9051\n",
            "Epoch 51/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9189 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9064\n",
            "Epoch 52/200\n",
            "198/198 [==============================] - 52s 262ms/step - loss: 0.0061 - accuracy: 0.9895 - DiceMetric: 0.9104 - val_loss: 0.0081 - val_accuracy: 0.9882 - val_DiceMetric: 0.9031\n",
            "Epoch 53/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0060 - accuracy: 0.9895 - DiceMetric: 0.9264 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9078\n",
            "Epoch 54/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9234 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.8971\n",
            "Epoch 55/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9302 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9136\n",
            "Epoch 56/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9241 - val_loss: 0.0078 - val_accuracy: 0.9882 - val_DiceMetric: 0.9153\n",
            "Epoch 57/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9306 - val_loss: 0.0093 - val_accuracy: 0.9879 - val_DiceMetric: 0.8894\n",
            "Epoch 58/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0066 - accuracy: 0.9894 - DiceMetric: 0.9086 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.8974\n",
            "Epoch 59/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9266 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9152\n",
            "Epoch 60/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9286 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9064\n",
            "Epoch 61/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0058 - accuracy: 0.9895 - DiceMetric: 0.9259 - val_loss: 0.0078 - val_accuracy: 0.9881 - val_DiceMetric: 0.9159\n",
            "Epoch 62/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9276 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9042\n",
            "Epoch 63/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9346 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9120\n",
            "Epoch 64/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9354 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9165\n",
            "Epoch 65/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9376 - val_loss: 0.0076 - val_accuracy: 0.9882 - val_DiceMetric: 0.9199\n",
            "Epoch 66/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9383 - val_loss: 0.0077 - val_accuracy: 0.9882 - val_DiceMetric: 0.9204\n",
            "Epoch 67/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9339 - val_loss: 0.0083 - val_accuracy: 0.9880 - val_DiceMetric: 0.9099\n",
            "Epoch 68/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0080 - accuracy: 0.9892 - DiceMetric: 0.8838 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9008\n",
            "Epoch 69/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0059 - accuracy: 0.9895 - DiceMetric: 0.9275 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9144\n",
            "Epoch 70/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0057 - accuracy: 0.9895 - DiceMetric: 0.9258 - val_loss: 0.0077 - val_accuracy: 0.9882 - val_DiceMetric: 0.9174\n",
            "Epoch 71/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0055 - accuracy: 0.9895 - DiceMetric: 0.9377 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9144\n",
            "Epoch 72/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9387 - val_loss: 0.0077 - val_accuracy: 0.9882 - val_DiceMetric: 0.9193\n",
            "Epoch 73/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9424 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9186\n",
            "Epoch 74/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0056 - accuracy: 0.9895 - DiceMetric: 0.9352 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9154\n",
            "Epoch 75/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9346 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9168\n",
            "Epoch 76/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9448 - val_loss: 0.0079 - val_accuracy: 0.9881 - val_DiceMetric: 0.9202\n",
            "Epoch 77/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9392 - val_loss: 0.0078 - val_accuracy: 0.9881 - val_DiceMetric: 0.9182\n",
            "Epoch 78/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9407 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9199\n",
            "Epoch 79/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9430 - val_loss: 0.0088 - val_accuracy: 0.9878 - val_DiceMetric: 0.9080\n",
            "Epoch 80/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0065 - accuracy: 0.9894 - DiceMetric: 0.9122 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9123\n",
            "Epoch 81/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0054 - accuracy: 0.9895 - DiceMetric: 0.9399 - val_loss: 0.0079 - val_accuracy: 0.9881 - val_DiceMetric: 0.9184\n",
            "Epoch 82/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9392 - val_loss: 0.0083 - val_accuracy: 0.9883 - val_DiceMetric: 0.9079\n",
            "Epoch 83/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9471 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9223\n",
            "Epoch 84/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9460 - val_loss: 0.0079 - val_accuracy: 0.9881 - val_DiceMetric: 0.9184\n",
            "Epoch 85/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9445 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9210\n",
            "Epoch 86/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9436 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9203\n",
            "Epoch 87/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9442 - val_loss: 0.0082 - val_accuracy: 0.9883 - val_DiceMetric: 0.9156\n",
            "Epoch 88/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0050 - accuracy: 0.9896 - DiceMetric: 0.9483 - val_loss: 0.0082 - val_accuracy: 0.9879 - val_DiceMetric: 0.9167\n",
            "Epoch 89/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0068 - accuracy: 0.9894 - DiceMetric: 0.9063 - val_loss: 0.0077 - val_accuracy: 0.9881 - val_DiceMetric: 0.9194\n",
            "Epoch 90/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0053 - accuracy: 0.9896 - DiceMetric: 0.9388 - val_loss: 0.0085 - val_accuracy: 0.9883 - val_DiceMetric: 0.9107\n",
            "Epoch 91/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0051 - accuracy: 0.9896 - DiceMetric: 0.9381 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9211\n",
            "Epoch 92/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9492 - val_loss: 0.0078 - val_accuracy: 0.9881 - val_DiceMetric: 0.9220\n",
            "Epoch 93/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9462 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9232\n",
            "Epoch 94/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9453 - val_loss: 0.0079 - val_accuracy: 0.9882 - val_DiceMetric: 0.9224\n",
            "Epoch 95/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9412 - val_loss: 0.0078 - val_accuracy: 0.9882 - val_DiceMetric: 0.9214\n",
            "Epoch 96/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9466 - val_loss: 0.0081 - val_accuracy: 0.9882 - val_DiceMetric: 0.9219\n",
            "Epoch 97/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9535 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9203\n",
            "Epoch 98/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0048 - accuracy: 0.9896 - DiceMetric: 0.9479 - val_loss: 0.0089 - val_accuracy: 0.9883 - val_DiceMetric: 0.9151\n",
            "Epoch 99/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0052 - accuracy: 0.9896 - DiceMetric: 0.9403 - val_loss: 0.0080 - val_accuracy: 0.9882 - val_DiceMetric: 0.9245\n",
            "Epoch 100/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9404 - val_loss: 0.0078 - val_accuracy: 0.9881 - val_DiceMetric: 0.9251\n",
            "Epoch 101/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9539 - val_loss: 0.0079 - val_accuracy: 0.9883 - val_DiceMetric: 0.9210\n",
            "Epoch 102/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9538 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9234\n",
            "Epoch 103/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9538 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9218\n",
            "Epoch 104/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9560 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9213\n",
            "Epoch 105/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9482 - val_loss: 0.0089 - val_accuracy: 0.9883 - val_DiceMetric: 0.9100\n",
            "Epoch 106/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9558 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9240\n",
            "Epoch 107/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9565 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9222\n",
            "Epoch 108/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9544 - val_loss: 0.0085 - val_accuracy: 0.9883 - val_DiceMetric: 0.9175\n",
            "Epoch 109/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9461 - val_loss: 0.0088 - val_accuracy: 0.9883 - val_DiceMetric: 0.9178\n",
            "Epoch 110/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9563 - val_loss: 0.0083 - val_accuracy: 0.9881 - val_DiceMetric: 0.9233\n",
            "Epoch 111/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0046 - accuracy: 0.9896 - DiceMetric: 0.9503 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9208\n",
            "Epoch 112/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9535 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9239\n",
            "Epoch 113/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9484 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9180\n",
            "Epoch 114/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9589 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9257\n",
            "Epoch 115/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9592 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9188\n",
            "Epoch 116/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0047 - accuracy: 0.9896 - DiceMetric: 0.9552 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.9240\n",
            "Epoch 117/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9559 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9198\n",
            "Epoch 118/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9611 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9220\n",
            "Epoch 119/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9605 - val_loss: 0.0081 - val_accuracy: 0.9883 - val_DiceMetric: 0.9235\n",
            "Epoch 120/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9584 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9250\n",
            "Epoch 121/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9617 - val_loss: 0.0082 - val_accuracy: 0.9882 - val_DiceMetric: 0.9259\n",
            "Epoch 122/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9629 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9254\n",
            "Epoch 123/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9614 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9223\n",
            "Epoch 124/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9630 - val_loss: 0.0084 - val_accuracy: 0.9882 - val_DiceMetric: 0.9238\n",
            "Epoch 125/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9614 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9229\n",
            "Epoch 126/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0044 - accuracy: 0.9896 - DiceMetric: 0.9575 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9222\n",
            "Epoch 127/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9624 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9186\n",
            "Epoch 128/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9635 - val_loss: 0.0084 - val_accuracy: 0.9881 - val_DiceMetric: 0.9238\n",
            "Epoch 129/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9541 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9259\n",
            "Epoch 130/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9593 - val_loss: 0.0095 - val_accuracy: 0.9883 - val_DiceMetric: 0.9139\n",
            "Epoch 131/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9590 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9217\n",
            "Epoch 132/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9664 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9237\n",
            "Epoch 133/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9673 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9258\n",
            "Epoch 134/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9671 - val_loss: 0.0089 - val_accuracy: 0.9883 - val_DiceMetric: 0.9231\n",
            "Epoch 135/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9627 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9263\n",
            "Epoch 136/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9674 - val_loss: 0.0093 - val_accuracy: 0.9883 - val_DiceMetric: 0.9188\n",
            "Epoch 137/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9677 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9252\n",
            "Epoch 138/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9621 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9254\n",
            "Epoch 139/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0042 - accuracy: 0.9896 - DiceMetric: 0.9661 - val_loss: 0.0094 - val_accuracy: 0.9883 - val_DiceMetric: 0.9207\n",
            "Epoch 140/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9633 - val_loss: 0.0086 - val_accuracy: 0.9881 - val_DiceMetric: 0.9238\n",
            "Epoch 141/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0041 - accuracy: 0.9897 - DiceMetric: 0.9618 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9235\n",
            "Epoch 142/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0041 - accuracy: 0.9896 - DiceMetric: 0.9625 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9231\n",
            "Epoch 143/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9649 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9272\n",
            "Epoch 144/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9604 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9248\n",
            "Epoch 145/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0040 - accuracy: 0.9896 - DiceMetric: 0.9699 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9230\n",
            "Epoch 146/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9708 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9250\n",
            "Epoch 147/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9658 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9226\n",
            "Epoch 148/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9696 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9241\n",
            "Epoch 149/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0043 - accuracy: 0.9896 - DiceMetric: 0.9586 - val_loss: 0.0085 - val_accuracy: 0.9882 - val_DiceMetric: 0.9213\n",
            "Epoch 150/200\n",
            "198/198 [==============================] - 57s 288ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9699 - val_loss: 0.0087 - val_accuracy: 0.9882 - val_DiceMetric: 0.9262\n",
            "Epoch 151/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9705 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9240\n",
            "Epoch 152/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9652 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9258\n",
            "Epoch 153/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9724 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9243\n",
            "Epoch 154/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9678 - val_loss: 0.0094 - val_accuracy: 0.9882 - val_DiceMetric: 0.9245\n",
            "Epoch 155/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9727 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9240\n",
            "Epoch 156/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9655 - val_loss: 0.0089 - val_accuracy: 0.9882 - val_DiceMetric: 0.9248\n",
            "Epoch 157/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9720 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9243\n",
            "Epoch 158/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9687 - val_loss: 0.0096 - val_accuracy: 0.9882 - val_DiceMetric: 0.9245\n",
            "Epoch 159/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9727 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9258\n",
            "Epoch 160/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9613 - val_loss: 0.0094 - val_accuracy: 0.9880 - val_DiceMetric: 0.9178\n",
            "Epoch 161/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0045 - accuracy: 0.9896 - DiceMetric: 0.9599 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9233\n",
            "Epoch 162/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9716 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9251\n",
            "Epoch 163/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9735 - val_loss: 0.0103 - val_accuracy: 0.9883 - val_DiceMetric: 0.9118\n",
            "Epoch 164/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9701 - val_loss: 0.0088 - val_accuracy: 0.9882 - val_DiceMetric: 0.9252\n",
            "Epoch 165/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9736 - val_loss: 0.0095 - val_accuracy: 0.9881 - val_DiceMetric: 0.9212\n",
            "Epoch 166/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0049 - accuracy: 0.9896 - DiceMetric: 0.9462 - val_loss: 0.0083 - val_accuracy: 0.9882 - val_DiceMetric: 0.9252\n",
            "Epoch 167/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0040 - accuracy: 0.9897 - DiceMetric: 0.9666 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9244\n",
            "Epoch 168/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9672 - val_loss: 0.0086 - val_accuracy: 0.9882 - val_DiceMetric: 0.9261\n",
            "Epoch 169/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0092 - val_accuracy: 0.9883 - val_DiceMetric: 0.9234\n",
            "Epoch 170/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9749 - val_loss: 0.0091 - val_accuracy: 0.9882 - val_DiceMetric: 0.9250\n",
            "Epoch 171/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9747 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9250\n",
            "Epoch 172/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9715 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9240\n",
            "Epoch 173/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9716 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9244\n",
            "Epoch 174/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9766 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9245\n",
            "Epoch 175/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9761 - val_loss: 0.0095 - val_accuracy: 0.9882 - val_DiceMetric: 0.9227\n",
            "Epoch 176/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9769 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9232\n",
            "Epoch 177/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9897 - DiceMetric: 0.9673 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9244\n",
            "Epoch 178/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9712 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9249\n",
            "Epoch 179/200\n",
            "198/198 [==============================] - 52s 263ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9734 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9249\n",
            "Epoch 180/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9719 - val_loss: 0.0095 - val_accuracy: 0.9882 - val_DiceMetric: 0.9217\n",
            "Epoch 181/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9765 - val_loss: 0.0096 - val_accuracy: 0.9882 - val_DiceMetric: 0.9221\n",
            "Epoch 182/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0039 - accuracy: 0.9896 - DiceMetric: 0.9687 - val_loss: 0.0089 - val_accuracy: 0.9883 - val_DiceMetric: 0.9209\n",
            "Epoch 183/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0038 - accuracy: 0.9897 - DiceMetric: 0.9742 - val_loss: 0.0091 - val_accuracy: 0.9881 - val_DiceMetric: 0.9241\n",
            "Epoch 184/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9746 - val_loss: 0.0090 - val_accuracy: 0.9881 - val_DiceMetric: 0.9246\n",
            "Epoch 185/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9770 - val_loss: 0.0094 - val_accuracy: 0.9882 - val_DiceMetric: 0.9232\n",
            "Epoch 186/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0037 - accuracy: 0.9897 - DiceMetric: 0.9718 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9246\n",
            "Epoch 187/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9683 - val_loss: 0.0095 - val_accuracy: 0.9882 - val_DiceMetric: 0.9250\n",
            "Epoch 188/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9783 - val_loss: 0.0090 - val_accuracy: 0.9882 - val_DiceMetric: 0.9249\n",
            "Epoch 189/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9722 - val_loss: 0.0094 - val_accuracy: 0.9882 - val_DiceMetric: 0.9240\n",
            "Epoch 190/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9731 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9247\n",
            "Epoch 191/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9781 - val_loss: 0.0094 - val_accuracy: 0.9881 - val_DiceMetric: 0.9240\n",
            "Epoch 192/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9777 - val_loss: 0.0097 - val_accuracy: 0.9882 - val_DiceMetric: 0.9224\n",
            "Epoch 193/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9737 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9245\n",
            "Epoch 194/200\n",
            "198/198 [==============================] - 57s 289ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9745 - val_loss: 0.0092 - val_accuracy: 0.9882 - val_DiceMetric: 0.9239\n",
            "Epoch 195/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9793 - val_loss: 0.0097 - val_accuracy: 0.9882 - val_DiceMetric: 0.9230\n",
            "Epoch 196/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9741 - val_loss: 0.0095 - val_accuracy: 0.9882 - val_DiceMetric: 0.9233\n",
            "Epoch 197/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9736 - val_loss: 0.0093 - val_accuracy: 0.9882 - val_DiceMetric: 0.9243\n",
            "Epoch 198/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9750 - val_loss: 0.0102 - val_accuracy: 0.9882 - val_DiceMetric: 0.9216\n",
            "Epoch 199/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9724 - val_loss: 0.0094 - val_accuracy: 0.9882 - val_DiceMetric: 0.9244\n",
            "Epoch 200/200\n",
            "198/198 [==============================] - 52s 264ms/step - loss: 0.0036 - accuracy: 0.9897 - DiceMetric: 0.9757 - val_loss: 0.0097 - val_accuracy: 0.9883 - val_DiceMetric: 0.9222\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f442d3c15d0>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "b0yz227RzmyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d192a323-75a5-40fd-e74d-b50699ba7b15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 0: loss of 0.007697664201259613; accuracy of 98.96839261054993% DiceMetric of 92.16498136520386%\n",
            "['loss', 'accuracy', 'DiceMetric']\n"
          ]
        }
      ],
      "source": [
        "scores= model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f'Score for fold {0}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oud92q0zgO7x"
      },
      "outputs": [],
      "source": [
        "print(model.metrics_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFP-3gQ9BO8f"
      },
      "source": [
        "##Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "5P0-RG0wjzA6"
      },
      "outputs": [],
      "source": [
        "Ypred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "id": "J1ra2EoVycvl",
        "outputId": "9e674a8b-f2f3-4a16-d49a-f5589fb9564c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(np.reshape(Ypred[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()\n",
        "plt.imshow(np.reshape(Y_test[160],(128,128)), cmap=plt.cm.gray)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikB_eI_ZmVtq"
      },
      "outputs": [],
      "source": [
        "def dice(true_mask, pred_mask):\n",
        "    \"\"\"\n",
        "        Computes the Dice coefficient.\n",
        "        Args:\n",
        "            true_mask : Array of arbitrary shape.\n",
        "            pred_mask : Array with the same shape than true_mask.  \n",
        "        \n",
        "        Returns:\n",
        "            A scalar representing the Dice coefficient between the two segmentations. \n",
        "        \n",
        "    \"\"\"\n",
        "    non_seg_score=1.0\n",
        "    if type(pred_mask) != np.ndarray:\n",
        "      t = torch.Tensor([0.5])\n",
        "      pred_mask=(pred_mask > t)\n",
        "    else:\n",
        "      pred_mask[pred_mask>=0.5]=1\n",
        "      pred_mask[pred_mask<0.5]=0\n",
        "\n",
        "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
        "    im_sum = true_mask.sum() + pred_mask.sum()\n",
        "    if im_sum == 0:\n",
        "        return non_seg_score\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(true_mask, pred_mask)\n",
        "    return 2. * intersection.sum() / im_sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beBVFzRQmXI-",
        "outputId": "51ce2c6f-6865-46b0-fa59-3128c9f76d8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9357399154707194"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "diceaux=dice(Y_test[160],Ypred[160])\n",
        "diceaux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2Sj8PPuJBv"
      },
      "source": [
        "## Model Fit Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fh7g3mmhuNPV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import absolute\n",
        "from numpy import sqrt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kj7ESdJAuOaW"
      },
      "outputs": [],
      "source": [
        "cv = KFold(n_splits=5, random_state=1, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erWVgdGfhXKu"
      },
      "source": [
        "25 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7gaF3LTvOU-",
        "outputId": "9618694e-c8fb-4139-9afc-63238c7bd3bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score for fold 1: loss of 0.010589624755084515; accuracy of 98.96852374076843% DiceMetric of 90.11987447738647%\n",
            "Score for fold 2: loss of 0.017074687406420708; accuracy of 98.8399863243103% DiceMetric of 88.72661590576172%\n",
            "Score for fold 3: loss of 0.019004367291927338; accuracy of 98.89352321624756% DiceMetric of 85.86874604225159%\n",
            "Score for fold 4: loss of 0.021750498563051224; accuracy of 98.8770842552185% DiceMetric of 87.11013793945312%\n",
            "Score for fold 5: loss of 0.02564629353582859; accuracy of 98.92171621322632% DiceMetric of 84.32435989379883%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=25,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOWim3VxhZsj"
      },
      "source": [
        "50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN7om-UlhbP9",
        "outputId": "7142d030-2897-49ad-b206-aea1d08d28a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score for fold 1: loss of 0.016142617911100388; accuracy of 98.95281195640564% DiceMetric of 86.89823746681213%\n",
            "Score for fold 2: loss of 0.02502174861729145; accuracy of 98.81399273872375% DiceMetric of 86.29720211029053%\n",
            "Score for fold 3: loss of 0.029437260702252388; accuracy of 98.87399077415466% DiceMetric of 82.6685905456543%\n",
            "Score for fold 4: loss of 0.027194472029805183; accuracy of 98.87163639068604% DiceMetric of 86.16846203804016%\n",
            "Score for fold 5: loss of 0.029874801635742188; accuracy of 98.91415238380432% DiceMetric of 83.36634039878845%\n"
          ]
        }
      ],
      "source": [
        "VALIDATION_ACCURACY = []\n",
        "VALIDAITON_LOSS = []\n",
        "nfold=1\n",
        "for train_index, val_index in cv.split(X,Y):\n",
        "  model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy',DiceMetric])\n",
        "\n",
        "  X_fold=X[train_index,:,:]\n",
        "  Y_fold=Y[train_index,:,:]\n",
        "  \n",
        "  model.fit(X_fold,Y_fold,batch_size=16,epochs=50,validation_split=0.2,verbose=0)\n",
        "\n",
        "  Xtest_fold=X[val_index,:,:]\n",
        "  Ytest_fold=Y[val_index,:,:]\n",
        "  scores= model.evaluate(Xtest_fold, Ytest_fold, verbose=0)\n",
        "  print(f'Score for fold {nfold}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}% {model.metrics_names[2]} of {scores[2]*100}%')\n",
        "  nfold+=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Attention Resnet Multihead.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM5W9DEupyq8ReK6zm2AI9h",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}